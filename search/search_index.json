{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reefer Anomaly Detection Solution This project aims to demonstrate how to perform real time analytics on data streams using an anomaly detection scoring service to assess Reefer container maintenance needs. Most of machine learning and analytics are done on data at rest and data warehouse. In this repository we are presenting an approach to use both data at rest and in motion within kafka. Note This project is part of the reference implementation solution to demonstrate the IBM event driven reference architecture and represents one use case for the IBM Data, AI and Analytics reference architecture but it also presents how to combine different IBM cloud paks to build the solution: Cloud pak for data, Cloud pak for automation, Cloud pak for integration and Cloud pak for application. The implementation is using two different approaches: one using open sources mostly from Apache projects one using IBM Cloud Pak products As we will detail in next section , there are five components in this solution that make the end to end anomaly detection solution: a Reefer simulator (we do not have such Reefer containers in our stock yet), a container microservice to manage reefer container as entity, an analytics scoring agent combined with a deployed model as a service and a business process. Figure 1: The solution Components The open source version integrate the model and the agent, so components 4 and 5 are integrated into the same process. Problem statements The Reefer container is an IoT device, which emits container telemetries every 3 minutes via the MQTT protocol. Figure 2: A Reefer or Refrigerator container as IoT We want to detect sensor anomaly and trigger a field engineer dispatch job to perform maintenance when the Reefer reaches an harbor. The Reefer container carries fresh product over seas. The telemetries are kept in the event backbone (kafka) for 20 days, an average vessel travel duration. And the telemetries are also persisted for longer time period in a document database (Cassandra or Mongo) or an object storage like Ceph . Going into this content you will learn the following: how to apply anomaly detection on telemetry data using Waston Studio or Jupiter notebook and python library how to deploy the model as agent or web service using Watson ML or Kubeflow how to integrate kafka events into Pandas dataset for build test and training sets. how to develop microservice in python for scoring telemetry using Appsody how to use the different IBM cloud paks to develop and deploy the solution. Some sensors may act badly. For example the co2 sensor telemetry plotted over time shows some sporadic behavior: Figure 3: A Co2 sensor acting strangely The goal is to identify in real time such behavior. When anomaly is detected, a new event is posted to the containers Kafka topic so the Reefer container manager microservice can apply the expected business logic. A Cloud Pak Approach To develop the solution we can use IBM Cloud Pak products as presented in the diagram below using a three layer architecture with Openshift for the deployment infrastructure. Figure 4: Cloud Paks and solution components We recommend to follow the following tutorial for understanding how the solution was built. An open source based solution We also look at an open source version of this solution using an approach close to opendatahub.io proposed architecture, as illustrated in the following diagram: Figure 5: Open source components and the solution components We are simplifying the Data ingestion layer, where normally the Reefer containers are sending telemetry via the MQTT protocol, so the data ingestion may leverage a product like Apache Nifi to transform the telemetry messages to kafka events. Apache Kafka is used as the event backbone and event sourcing so microservices, deployed on Openshift, can consume and publish messages. For persistence reason, we may leverage big data type of storage like Apache Cassandra or mongodb to persist the container's telemetries over a longer time period. This datasource is used by the Data Scientists to do its data preparation and build training and test sets and select the best model. We also illustrate how to connect to Kafka topic as data source, from a Jupyter notebook to also use data from Kafka to build training set and test set. Data scientists can run Jupyter lab on OpenShift and build a model to be deployed as python microservice, consumer of Reefer telemetry events. If you want to read more on how to build and run the solution with open source stack, see this note . MVP component view For a minimum viable demonstration the runtime components looks like in the figure below: Figure 6: The solution components A web app, deployed on Openshift, is running a simulator to simulate the generation of Reefer container telemetries while the container is at sea or during end to end transportation. The app exposes a simple POST operation with a control object to control the simulation. Here is an example of such control.json object { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } The simulation can be done on o2sensor, co2sensor or power. A curl script does the HTTP POST request of this json object. See this paragraph. The telemetry events are sent to the reeferTelemetries topic in Kafka. They are defined as Avro schema. The predictive scoring is a consumer of such events, read one event at a time and call the model internally, then sends a new event when maintenance is required. See the note for details. The maintenance requirement is an event in the containers topic. The 6 th component of the solution, is the container microservice which was defined in the EDA reference implementation. The maintenance engineer intervention process is modeled in BPM, deploy on public cloud and the process application is exposed as API. The container identifier and the telemetry record is sent as input to the process. Project approach The solution covers different development areas and practices: event driven microservice business process cloud native microservices DevOps data ops and machine learning model development. The software life cycle integrate the three main tracks of DevOps, DataOps and MLOps as presented in this methodology article . DevOps As all the services developed in this solution are event-driven we do not need to rephrase how we used event storming and applied the different patterns for each microservices. You can read the following articles to learn more: Event storming workshop From analysis to microservice using domain driven design Apply DDD to order microservice Apply DDD to the container management microservice For the adoption of Appsody and CI/CD based on tektron, we present how we use Cloud Pak for application in the development of the scoring microservice in this note . To understand how to deploy each service on openshift see this detailed note . DataOps Collect data with open source approach . Collect data with cloud pak for data . Model Ops To use Jupyter, Sparks and kubeflow see this note Further Readings Data AI reference architecture Romeo Kienzler anomaly detection article 1 Romeo Kienzler anomaly detection - Generating data for anomaly detection Romeo Kienzler anomaly detection with Deeplearning4j Romeo Kienzler anomaly detection using Apache SystemML Romeo Kienzler anomaly detection using Tensoflow and Keras","title":"What you will learn"},{"location":"#reefer-anomaly-detection-solution","text":"This project aims to demonstrate how to perform real time analytics on data streams using an anomaly detection scoring service to assess Reefer container maintenance needs. Most of machine learning and analytics are done on data at rest and data warehouse. In this repository we are presenting an approach to use both data at rest and in motion within kafka. Note This project is part of the reference implementation solution to demonstrate the IBM event driven reference architecture and represents one use case for the IBM Data, AI and Analytics reference architecture but it also presents how to combine different IBM cloud paks to build the solution: Cloud pak for data, Cloud pak for automation, Cloud pak for integration and Cloud pak for application. The implementation is using two different approaches: one using open sources mostly from Apache projects one using IBM Cloud Pak products As we will detail in next section , there are five components in this solution that make the end to end anomaly detection solution: a Reefer simulator (we do not have such Reefer containers in our stock yet), a container microservice to manage reefer container as entity, an analytics scoring agent combined with a deployed model as a service and a business process. Figure 1: The solution Components The open source version integrate the model and the agent, so components 4 and 5 are integrated into the same process.","title":"Reefer Anomaly Detection Solution"},{"location":"#problem-statements","text":"The Reefer container is an IoT device, which emits container telemetries every 3 minutes via the MQTT protocol. Figure 2: A Reefer or Refrigerator container as IoT We want to detect sensor anomaly and trigger a field engineer dispatch job to perform maintenance when the Reefer reaches an harbor. The Reefer container carries fresh product over seas. The telemetries are kept in the event backbone (kafka) for 20 days, an average vessel travel duration. And the telemetries are also persisted for longer time period in a document database (Cassandra or Mongo) or an object storage like Ceph . Going into this content you will learn the following: how to apply anomaly detection on telemetry data using Waston Studio or Jupiter notebook and python library how to deploy the model as agent or web service using Watson ML or Kubeflow how to integrate kafka events into Pandas dataset for build test and training sets. how to develop microservice in python for scoring telemetry using Appsody how to use the different IBM cloud paks to develop and deploy the solution. Some sensors may act badly. For example the co2 sensor telemetry plotted over time shows some sporadic behavior: Figure 3: A Co2 sensor acting strangely The goal is to identify in real time such behavior. When anomaly is detected, a new event is posted to the containers Kafka topic so the Reefer container manager microservice can apply the expected business logic.","title":"Problem statements"},{"location":"#a-cloud-pak-approach","text":"To develop the solution we can use IBM Cloud Pak products as presented in the diagram below using a three layer architecture with Openshift for the deployment infrastructure. Figure 4: Cloud Paks and solution components We recommend to follow the following tutorial for understanding how the solution was built.","title":"A Cloud Pak Approach"},{"location":"#an-open-source-based-solution","text":"We also look at an open source version of this solution using an approach close to opendatahub.io proposed architecture, as illustrated in the following diagram: Figure 5: Open source components and the solution components We are simplifying the Data ingestion layer, where normally the Reefer containers are sending telemetry via the MQTT protocol, so the data ingestion may leverage a product like Apache Nifi to transform the telemetry messages to kafka events. Apache Kafka is used as the event backbone and event sourcing so microservices, deployed on Openshift, can consume and publish messages. For persistence reason, we may leverage big data type of storage like Apache Cassandra or mongodb to persist the container's telemetries over a longer time period. This datasource is used by the Data Scientists to do its data preparation and build training and test sets and select the best model. We also illustrate how to connect to Kafka topic as data source, from a Jupyter notebook to also use data from Kafka to build training set and test set. Data scientists can run Jupyter lab on OpenShift and build a model to be deployed as python microservice, consumer of Reefer telemetry events. If you want to read more on how to build and run the solution with open source stack, see this note .","title":"An open source based solution"},{"location":"#mvp-component-view","text":"For a minimum viable demonstration the runtime components looks like in the figure below: Figure 6: The solution components A web app, deployed on Openshift, is running a simulator to simulate the generation of Reefer container telemetries while the container is at sea or during end to end transportation. The app exposes a simple POST operation with a control object to control the simulation. Here is an example of such control.json object { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } The simulation can be done on o2sensor, co2sensor or power. A curl script does the HTTP POST request of this json object. See this paragraph. The telemetry events are sent to the reeferTelemetries topic in Kafka. They are defined as Avro schema. The predictive scoring is a consumer of such events, read one event at a time and call the model internally, then sends a new event when maintenance is required. See the note for details. The maintenance requirement is an event in the containers topic. The 6 th component of the solution, is the container microservice which was defined in the EDA reference implementation. The maintenance engineer intervention process is modeled in BPM, deploy on public cloud and the process application is exposed as API. The container identifier and the telemetry record is sent as input to the process.","title":"MVP component view"},{"location":"#project-approach","text":"The solution covers different development areas and practices: event driven microservice business process cloud native microservices DevOps data ops and machine learning model development. The software life cycle integrate the three main tracks of DevOps, DataOps and MLOps as presented in this methodology article .","title":"Project approach"},{"location":"#devops","text":"As all the services developed in this solution are event-driven we do not need to rephrase how we used event storming and applied the different patterns for each microservices. You can read the following articles to learn more: Event storming workshop From analysis to microservice using domain driven design Apply DDD to order microservice Apply DDD to the container management microservice For the adoption of Appsody and CI/CD based on tektron, we present how we use Cloud Pak for application in the development of the scoring microservice in this note . To understand how to deploy each service on openshift see this detailed note .","title":"DevOps"},{"location":"#dataops","text":"Collect data with open source approach . Collect data with cloud pak for data .","title":"DataOps"},{"location":"#model-ops","text":"To use Jupyter, Sparks and kubeflow see this note","title":"Model Ops"},{"location":"#further-readings","text":"Data AI reference architecture Romeo Kienzler anomaly detection article 1 Romeo Kienzler anomaly detection - Generating data for anomaly detection Romeo Kienzler anomaly detection with Deeplearning4j Romeo Kienzler anomaly detection using Apache SystemML Romeo Kienzler anomaly detection using Tensoflow and Keras","title":"Further Readings"},{"location":"compendium/","text":"Compendium Cloud Pak For Integration Event Streams product documentation Event Driven reference architecture Cloud Pak for Application IBM Cloud Pak for Applications Kabanero architecture and workflows Cloud Pak for Data Data AI reference architecture Cloud Pak for Data product documentation Install CP4D on Openshift Getting stared with Cloud Pak for data Explore Knowledge Catalog's Data Governance Capabilities Model deployment with Watson Machine Learning Cloud Pak for Automation Garage solution engineering - Denim compute Digital Business Automation Architecture Center - streamlines business operations DBA - Garage team's cookbook Openshift Ansible CI/CD Gitops Our Git action to update gitops Kabanero Appsody Git action Tekton - kubernetes native pipeline tool Anomaly detection Romeo Kienzler anomaly detection article 1 Romeo Kienzler anomaly detection article 2 Romeo Kienzler anomaly detection article 3","title":"Compendium"},{"location":"compendium/#compendium","text":"","title":"Compendium"},{"location":"compendium/#cloud-pak-for-integration","text":"Event Streams product documentation Event Driven reference architecture","title":"Cloud Pak For Integration"},{"location":"compendium/#cloud-pak-for-application","text":"IBM Cloud Pak for Applications Kabanero architecture and workflows","title":"Cloud Pak for Application"},{"location":"compendium/#cloud-pak-for-data","text":"Data AI reference architecture Cloud Pak for Data product documentation Install CP4D on Openshift Getting stared with Cloud Pak for data Explore Knowledge Catalog's Data Governance Capabilities Model deployment with Watson Machine Learning","title":"Cloud Pak for Data"},{"location":"compendium/#cloud-pak-for-automation","text":"Garage solution engineering - Denim compute Digital Business Automation Architecture Center - streamlines business operations DBA - Garage team's cookbook","title":"Cloud Pak for Automation"},{"location":"compendium/#openshift","text":"Ansible","title":"Openshift"},{"location":"compendium/#cicd","text":"Gitops Our Git action to update gitops Kabanero Appsody Git action Tekton - kubernetes native pipeline tool","title":"CI/CD"},{"location":"compendium/#anomaly-detection","text":"Romeo Kienzler anomaly detection article 1 Romeo Kienzler anomaly detection article 2 Romeo Kienzler anomaly detection article 3","title":"Anomaly detection"},{"location":"cp-approach/","text":"IBM Cloud Pak Approach The goal of this approach is to use the following IBM Cloud Paks to develop and deploy the solution. The following diagram below, illustrates the features or components used in each cloud paks to support the implementation. The grey components are part of the solution implementation. As part of Cloud pak for Application , we are using Appsody to generate the code and the development build, test, deploy for the Reefer simulator done in Python / Flask, and for the scoring app that is a Java Microprofile 3.0 running on Liberty server. The microprofile app is also generated using Appsody and build and deploy. As part of the CI/CD tooling, we use public github (This repository !), with Github action and Tekton for the pipeline deployment. Cloud Pak for Application is deployed on Openshift 4.2 Cloud Pak for Integration is used to deploy IBM Event Streams, the IBM event backbone based on Apache Kafka. We wrote a lot of content and best practices around Kafka in the Event Driven Architecture (EDA) repository. In the EDA reference implementation solution we present other CP4I assets. Cloud Pak for Automation is used to define the Field engineer dispatching process, and we deliver the process App for that. Finally Cloud Pak for Data is used to support the AI ladder: collecting and govern the data, develop the model, and publish the model as a service. Preparing the different environments The numbered figure below, illustrates the step by step environment setup: 1- PostgreSQL Service The products database is defined in PostgreSQL service within IBM Cloud public. To provision the environment you can read this note , and then to populate the data use this note to see how we use this service and the python code in the simulator folder or the psql tool to create the product database. 2- MongoDB Service The long term persistence for the telemetry metrics we are using mongodb on IBM Cloud. This separate note goes into details on how we prepare the data and upload them to mongo. 3- Openshift 4.2 Cluster for CP4I and CP4App To install Openshift 4.2 cluster we recommend following Red Hat tutorials and our cookbook . 4- Deploy event streams and defines topics We are presenting a quick overview of deploying IBM Event Streams from the Cloud Pak for Integration in this note and how to configure the required Kafka topics with automation in this note . 5- Deploy Cloud Pak for Application As part of Cloud Pak for Application we are using Tekton, Appsody and the Kabanero deployment. The architecture and development workflows are presented in this figure below: The approach is to provide capabilities and extension so that lead developers or architects can define Stack and base code to reuse. See also the reference architecture article for application modernization . 6- Reefer simulator as Appsody Python We are presenting how we used the Appsody Python Stack as a base to implement the Reefer Simulator, combined with other Python development best practices in this note and we use the CI and CD practices for deployment. 7- Scoring microservice as Java Microprofile app We are detailing how to leverage the Liberty profile server and MicroProfile 3.0 with the new Reactive Messaging to integrate with Kafka in this note . 8- Data Virtualization The data management is done using Cloud Pak for data Data Virtualization capability, on a remote Mongo DB datasource. We used this approach, to illustrate how easy, it is to define virtual tables, join them to prepare a data set for machine learning work. The Telemetries are saved in a MongoDB instance provisioned on the IBM Cloud. You can read the detail in this note . 9- Anomaly detection notebook this note 10- Anomaly detection service 11- Cloud Pak for Automation Digital business automation (DBA) allows an organization to improve its operations by streamlining the way people participate in business processes and workflows, automate repeatable decisions, and provide business users with the ability to edit and change the business logic involved in these business processes. The implementation of the Engineer dispatching for the maintenance of the Reefer containers is documented in this note. For more information about CP for Automation read the Garage team's cookbook , and the 'Denim compute' to present a reference implementation for a Digital Business Automation solution. 12- Reefer Maintenance business process The reefer maintenance process is explained in this note .","title":"Cloud Pak approach"},{"location":"cp-approach/#ibm-cloud-pak-approach","text":"The goal of this approach is to use the following IBM Cloud Paks to develop and deploy the solution. The following diagram below, illustrates the features or components used in each cloud paks to support the implementation. The grey components are part of the solution implementation. As part of Cloud pak for Application , we are using Appsody to generate the code and the development build, test, deploy for the Reefer simulator done in Python / Flask, and for the scoring app that is a Java Microprofile 3.0 running on Liberty server. The microprofile app is also generated using Appsody and build and deploy. As part of the CI/CD tooling, we use public github (This repository !), with Github action and Tekton for the pipeline deployment. Cloud Pak for Application is deployed on Openshift 4.2 Cloud Pak for Integration is used to deploy IBM Event Streams, the IBM event backbone based on Apache Kafka. We wrote a lot of content and best practices around Kafka in the Event Driven Architecture (EDA) repository. In the EDA reference implementation solution we present other CP4I assets. Cloud Pak for Automation is used to define the Field engineer dispatching process, and we deliver the process App for that. Finally Cloud Pak for Data is used to support the AI ladder: collecting and govern the data, develop the model, and publish the model as a service.","title":"IBM Cloud Pak Approach"},{"location":"cp-approach/#preparing-the-different-environments","text":"The numbered figure below, illustrates the step by step environment setup:","title":"Preparing the different environments"},{"location":"cp-approach/#1-postgresql-service","text":"The products database is defined in PostgreSQL service within IBM Cloud public. To provision the environment you can read this note , and then to populate the data use this note to see how we use this service and the python code in the simulator folder or the psql tool to create the product database.","title":"1- PostgreSQL Service"},{"location":"cp-approach/#2-mongodb-service","text":"The long term persistence for the telemetry metrics we are using mongodb on IBM Cloud. This separate note goes into details on how we prepare the data and upload them to mongo.","title":"2- MongoDB Service"},{"location":"cp-approach/#3-openshift-42-cluster-for-cp4i-and-cp4app","text":"To install Openshift 4.2 cluster we recommend following Red Hat tutorials and our cookbook .","title":"3- Openshift 4.2 Cluster for CP4I and CP4App"},{"location":"cp-approach/#4-deploy-event-streams-and-defines-topics","text":"We are presenting a quick overview of deploying IBM Event Streams from the Cloud Pak for Integration in this note and how to configure the required Kafka topics with automation in this note .","title":"4- Deploy event streams and defines topics"},{"location":"cp-approach/#5-deploy-cloud-pak-for-application","text":"As part of Cloud Pak for Application we are using Tekton, Appsody and the Kabanero deployment. The architecture and development workflows are presented in this figure below: The approach is to provide capabilities and extension so that lead developers or architects can define Stack and base code to reuse. See also the reference architecture article for application modernization .","title":"5- Deploy Cloud Pak for Application"},{"location":"cp-approach/#6-reefer-simulator-as-appsody-python","text":"We are presenting how we used the Appsody Python Stack as a base to implement the Reefer Simulator, combined with other Python development best practices in this note and we use the CI and CD practices for deployment.","title":"6- Reefer simulator as Appsody Python"},{"location":"cp-approach/#7-scoring-microservice-as-java-microprofile-app","text":"We are detailing how to leverage the Liberty profile server and MicroProfile 3.0 with the new Reactive Messaging to integrate with Kafka in this note .","title":"7- Scoring microservice as Java Microprofile app"},{"location":"cp-approach/#8-data-virtualization","text":"The data management is done using Cloud Pak for data Data Virtualization capability, on a remote Mongo DB datasource. We used this approach, to illustrate how easy, it is to define virtual tables, join them to prepare a data set for machine learning work. The Telemetries are saved in a MongoDB instance provisioned on the IBM Cloud. You can read the detail in this note .","title":"8- Data Virtualization"},{"location":"cp-approach/#9-anomaly-detection-notebook","text":"this note","title":"9- Anomaly detection notebook"},{"location":"cp-approach/#10-anomaly-detection-service","text":"","title":"10- Anomaly detection service"},{"location":"cp-approach/#11-cloud-pak-for-automation","text":"Digital business automation (DBA) allows an organization to improve its operations by streamlining the way people participate in business processes and workflows, automate repeatable decisions, and provide business users with the ability to edit and change the business logic involved in these business processes. The implementation of the Engineer dispatching for the maintenance of the Reefer containers is documented in this note. For more information about CP for Automation read the Garage team's cookbook , and the 'Denim compute' to present a reference implementation for a Digital Business Automation solution.","title":"11- Cloud Pak for Automation"},{"location":"cp-approach/#12-reefer-maintenance-business-process","text":"The reefer maintenance process is explained in this note .","title":"12- Reefer Maintenance business process"},{"location":"oos-approach/","text":"Open Source Approach UNDER Construction ! Figure 1: The open source components Build the docker image for Jupyter notebook We are using a special version of conda to add the postgresql and kafka libraries for python so we can access postgresql or kafka from notebook. The Dockerfile may use a cert.pem file, which contains the postgres certificate so the notebook can connect to postgresql service with SSL connection. cd docker docker build -f docker-jupyter-tool -t ibmcase/jupyter . To run this jupyter server use the startJupyterServer.sh script: # refarch-reefer-ml project folder ./startJupyterServer.sh IBMCLOUD","title":"Open source approach"},{"location":"oos-approach/#open-source-approach","text":"UNDER Construction ! Figure 1: The open source components","title":"Open Source Approach"},{"location":"oos-approach/#build-the-docker-image-for-jupyter-notebook","text":"We are using a special version of conda to add the postgresql and kafka libraries for python so we can access postgresql or kafka from notebook. The Dockerfile may use a cert.pem file, which contains the postgres certificate so the notebook can connect to postgresql service with SSL connection. cd docker docker build -f docker-jupyter-tool -t ibmcase/jupyter . To run this jupyter server use the startJupyterServer.sh script: # refarch-reefer-ml project folder ./startJupyterServer.sh IBMCLOUD","title":"Build the docker image for Jupyter notebook"},{"location":"performance-tests/","text":"Performance tests We have to develop some performance tests for some client engagements. So reusing the reefer telemetry generator can help us to run performance test. The simulator is configured to run 2 Gunicorn processes with 4 threads each. So potentially running 8 requests in parallel. (See the code simulator/infrastructure/webappconfig.py )","title":"Performance tests"},{"location":"performance-tests/#performance-tests","text":"We have to develop some performance tests for some client engagements. So reusing the reefer telemetry generator can help us to run performance test. The simulator is configured to run 2 Gunicorn processes with 4 threads each. So potentially running 8 requests in parallel. (See the code simulator/infrastructure/webappconfig.py )","title":"Performance tests"},{"location":"workshop/","text":"Build the solution - Workshop Pre-requisites to build and run this solution Start by cloning this project using the command: git clone https://github.com/ibm-cloud-architecture/refarch-reefer-ml Repository structure The solution to implement includes the following components: The Reefer simulator (1) to send telemetry events or to create such data elements as CSV file. The code is under simulator folder. The curl call (2) is done in a script: scripts/sendSimulControl.sh The scoring agent (4) is in the scoring folder or if you are using IBM Cloud Pak solution it is a Java Microprofile applicating in the scoring-mp folder. The Reefer container service (6) is in a separate project, but we have defined a docker image, published in docker hub so we propose to deploy it on Openshift in this section The business process definition (7) is in the twx file under the bpm folder. Building a python development environment as docker image To avoid impacting our laptop environment (specially macbook which uses python a lot), we use a Dockerfile to get the basic of python 3.7.x and the python modules we need, like kafka, http requests, pandas, sklearn, pytest.... To build your python image with all the needed libraries, use the following commands: cd docker docker build -f docker-python-tools -t ibmcase/python . To use this python environment you can use the script: startPythonEnv.sh . When running with Event Stream and Postgres DB on the cloud use IBMCLOUD argument, if you use and on-premise Openshif cluster use the OCP argument. # refarch-reefer-ml project folder ./startPythonEnv.sh IBMCLOUD # or for Openshift on premise: ./startPythonEnv.sh OCP Set environment variables As part of the 12 factors practice , we externalize the end points configuration in environment variables. We are providing a script template ( scripts/setenv-tmp.sh ) to set those variables for your local development. Rename this file as setenv.sh . This file is git ignored, to do not share keys and passwords in public domain. The variables help the different code in the solution to access the Event Stream broker cluster and the Postgresql service running on IBM Cloud. Deploy Reefer Containermicroservice The Reefer container is a microservice we developed in the context of managing Reefer containers, as part of the end to end solution. We are reusing it in the context of this anomaly detection solution. When a container will be in maintenace mode, this microservice will call BPM to create a process instance. Consider it as a black box. Deploy on Openshift We use Openshift image deployment capability with the following command: oc new-app ibmcase/kcontainer-spring-container-ms oc expose svc/kcontainer-spring-container-ms","title":"Preparation"},{"location":"workshop/#build-the-solution-workshop","text":"","title":"Build the solution - Workshop"},{"location":"workshop/#pre-requisites-to-build-and-run-this-solution","text":"Start by cloning this project using the command: git clone https://github.com/ibm-cloud-architecture/refarch-reefer-ml","title":"Pre-requisites to build and run this solution"},{"location":"workshop/#repository-structure","text":"The solution to implement includes the following components: The Reefer simulator (1) to send telemetry events or to create such data elements as CSV file. The code is under simulator folder. The curl call (2) is done in a script: scripts/sendSimulControl.sh The scoring agent (4) is in the scoring folder or if you are using IBM Cloud Pak solution it is a Java Microprofile applicating in the scoring-mp folder. The Reefer container service (6) is in a separate project, but we have defined a docker image, published in docker hub so we propose to deploy it on Openshift in this section The business process definition (7) is in the twx file under the bpm folder.","title":"Repository structure"},{"location":"workshop/#building-a-python-development-environment-as-docker-image","text":"To avoid impacting our laptop environment (specially macbook which uses python a lot), we use a Dockerfile to get the basic of python 3.7.x and the python modules we need, like kafka, http requests, pandas, sklearn, pytest.... To build your python image with all the needed libraries, use the following commands: cd docker docker build -f docker-python-tools -t ibmcase/python . To use this python environment you can use the script: startPythonEnv.sh . When running with Event Stream and Postgres DB on the cloud use IBMCLOUD argument, if you use and on-premise Openshif cluster use the OCP argument. # refarch-reefer-ml project folder ./startPythonEnv.sh IBMCLOUD # or for Openshift on premise: ./startPythonEnv.sh OCP","title":"Building a python development environment as docker image"},{"location":"workshop/#set-environment-variables","text":"As part of the 12 factors practice , we externalize the end points configuration in environment variables. We are providing a script template ( scripts/setenv-tmp.sh ) to set those variables for your local development. Rename this file as setenv.sh . This file is git ignored, to do not share keys and passwords in public domain. The variables help the different code in the solution to access the Event Stream broker cluster and the Postgresql service running on IBM Cloud.","title":"Set environment variables"},{"location":"workshop/#deploy-reefer-containermicroservice","text":"The Reefer container is a microservice we developed in the context of managing Reefer containers, as part of the end to end solution. We are reusing it in the context of this anomaly detection solution. When a container will be in maintenace mode, this microservice will call BPM to create a process instance. Consider it as a black box.","title":"Deploy Reefer Containermicroservice"},{"location":"workshop/#deploy-on-openshift","text":"We use Openshift image deployment capability with the following command: oc new-app ibmcase/kcontainer-spring-container-ms oc expose svc/kcontainer-spring-container-ms","title":"Deploy on Openshift"},{"location":"analyze/oss-ml-dev/","text":"Defining the anomaly detection scoring model Predictive maintenance and anomaly detection are complex problems to address. We do not pretend to address those complex problems in this repository, as we focus in putting in place the end to end creation and deployment of the model. To review the problem of predictive maintenance read this article. If you want to contribute to build a better model, we are looking for contributors . To build the model and work on the data, we will use a local version of Jupyter notebook to load the logistic regression nodebook from the ml folder. We have two types of notebook Start a jupyter server using our docker image and a postgresql in IBM cloud. pwd ./startJupyterServer IBMCLOUD or LOCAL Then open a web browser to http://localhost:8888?token=<sometoken> go under work/ml folder. Open one of the model: the model_logistic_regression.ipynb to work on data set saved in the ml/data/telemetries.csv file. the model_logistic_regression-pg.ipynb to work on data saved in postgresql running on IBM Cloud. The notebooks include comments to explain how the model is done. We use logistic regression to build a binary classification (maintenance required or not), as the data are simulated, and the focus is not in the model building, but more on the end to end process. The notebook persists the trained model as a pickle file so it can be loaded by a python module or another notebook. For more information on using the Jupyter notebook, here is a product documentation . The co2 sensor plot over time shows the training data with some sporadic behavior: The confusion matrix shows very little false positive and false negative: Use the model in another notebook: We can use a second notebook to test the model with one telemetry record using the pickle serialized model. The notebook is named predictMaintenance.ipynb .","title":"Define the anomaly detection scoring model with OSS"},{"location":"analyze/oss-ml-dev/#defining-the-anomaly-detection-scoring-model","text":"Predictive maintenance and anomaly detection are complex problems to address. We do not pretend to address those complex problems in this repository, as we focus in putting in place the end to end creation and deployment of the model. To review the problem of predictive maintenance read this article. If you want to contribute to build a better model, we are looking for contributors . To build the model and work on the data, we will use a local version of Jupyter notebook to load the logistic regression nodebook from the ml folder. We have two types of notebook Start a jupyter server using our docker image and a postgresql in IBM cloud. pwd ./startJupyterServer IBMCLOUD or LOCAL Then open a web browser to http://localhost:8888?token=<sometoken> go under work/ml folder. Open one of the model: the model_logistic_regression.ipynb to work on data set saved in the ml/data/telemetries.csv file. the model_logistic_regression-pg.ipynb to work on data saved in postgresql running on IBM Cloud. The notebooks include comments to explain how the model is done. We use logistic regression to build a binary classification (maintenance required or not), as the data are simulated, and the focus is not in the model building, but more on the end to end process. The notebook persists the trained model as a pickle file so it can be loaded by a python module or another notebook. For more information on using the Jupyter notebook, here is a product documentation . The co2 sensor plot over time shows the training data with some sporadic behavior: The confusion matrix shows very little false positive and false negative: Use the model in another notebook: We can use a second notebook to test the model with one telemetry record using the pickle serialized model. The notebook is named predictMaintenance.ipynb .","title":"Defining the anomaly detection scoring model"},{"location":"analyze/predictive-maintenance/","text":"Reefer Container Predictive Maintenance In this section, we discuss how to build an analytic model using machine learning techniques from data coming from event store like kafka. We train the model with the help of historical data to predict whether maintenance is required for the reefer container at a certain point in time. You will learn how to simulate date for reefer, develop the predictive maintenance model, and integrate the model into an application. Introduction A reefer container is a refrigerated shipping container used to store or transport frozen or cold goods perishable items or goods that require temperature control. Reefers make an excellent, portable solution for short or long term storage and can be used to ship or truck goods over long distances as they can be plugged into the power station on ships or have it clipped on generators attached. Perishable products must be kept at a controlled temperature, from point of origin to delivery to retailer or pharmacy. The logistics industry refers to this as the \u201ccold chain\u201d and it encompasses both \u201creefers\u201d (refrigerated containers) as well as warehouses, distribution centers and the final storage or holding areas. Throughout this chain the risk of failure is ever-present, meaning there is always a possibility of cargo exceeding permissible or safe temperature levels, even if only briefly. For example, a truck might be stopped without power in desert heat, allowing temperatures in the reefer to rise. Then power is restored and the temperature in the container comes back down, but the product is damaged. When cargo with such as any of those items listed above are exposed to temperatures outside of prescribed limits it can be damaged. In some cases this is evident, such as with bananas, but in other situations, like the transport of vaccines, it may not be apparent that damage has occurred and the vaccine becomes ineffective. For some products, going over temperature, even only briefly, can reduce shelf life dramatically, incurring substantial costs when it cannot be sold. Organizations contracting to ship perishable products often specify the permissible temperature range. However, even if it is possible to show that product was exposed to conditions outside of those contracted, proving where it happened, and thus responsibility, can be much harder. Predictive maintenance problem statement If you want a good understanding of the problem space for predictive maintenance read, Yana Ageeva's article in toward data science . The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly From a methodology point of view the Data Scientist needs to address the following questions: What type of failure to consider and which one to predict? What kind of failure is happening? slow degradation or instantaneous failure? What could be the relation between a product characteristics and the failure? What kind of measure exist to assess the given characteristic? Which measurements correspond to good functioning and which ones correspond to failure? How often metrics are reported? What question the model should answer? What kind of output should the model give? How long in advance should the model be able to indicate that a failure will occur? What are the business impact to do not predict the failure? and predicting false negative failure? What is the expected accuracy? Reefer problem types There are multiple different potential issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions\" issue: Sensors in the refrigeration unit need to be calibrated and be continuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which leads to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be difficult to know if there is an issue or not. The other common potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. Blocked air passage: to keep the temperature homogenous inside the reefer. So the question we want to answer is: does the Reefer keep accurate temperature overtime between what is set versus what is measured? Modeling techniques The model uses the generated data from above scenarios: When the container's door is open for a longer time - this gives a false positive that maintainence is required. When sensors are malfunctioning, it records arbitrary readings. When the readings are normal. We have currently trained our model on 3000 datapoints from the three scenarios above. There are different modeling approach to tackle predictive maintenance: regression model classification to predict failure for a given time period classify anomalous behavior: classes are not known in advance. Normal operation is known. compute probability of failure over time Code execution The simulator continuosly generates container metrics, publishes it to Kafka and run the predictMaintainence.ipynb to predict if maintainence is sought at this point in time. Model description We are using Machine Learning supervised learning here. There are two types of supervised learning - 1) Classification: Predict a categorical response, 2) Regression: Predict a continuous response Linear regression Pros: 1) Fast 2) No tuning required 3) Highly interpretable 4) Well-understood Cons: 1) Unlikely to produce the best predictive accuracy 2) Presumes a linear relationship between the features and response 3) If the relationship is highly non-linear as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate Naive Bayes classification Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent. The classification is conducted by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively. Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator. Model evaluation We are using Root Mean Squared Error (RMSE) for evaluating the model performance. Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors. Classification does better here as the scenarion is more of a classification problem. References Understand Reefer container For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting. PREDICTION OF TEMPERATURE INSIDE A REFRIGERATED CONTAINER IN THE PRESENCE OF PERISHABLE GOODS Temperature Monitoring During Transportation, Storage and Processing of Perishable Products Understanding machine learning classifiers","title":"Reefer anomaly detection problem"},{"location":"analyze/predictive-maintenance/#reefer-container-predictive-maintenance","text":"In this section, we discuss how to build an analytic model using machine learning techniques from data coming from event store like kafka. We train the model with the help of historical data to predict whether maintenance is required for the reefer container at a certain point in time. You will learn how to simulate date for reefer, develop the predictive maintenance model, and integrate the model into an application.","title":"Reefer Container Predictive Maintenance"},{"location":"analyze/predictive-maintenance/#introduction","text":"A reefer container is a refrigerated shipping container used to store or transport frozen or cold goods perishable items or goods that require temperature control. Reefers make an excellent, portable solution for short or long term storage and can be used to ship or truck goods over long distances as they can be plugged into the power station on ships or have it clipped on generators attached. Perishable products must be kept at a controlled temperature, from point of origin to delivery to retailer or pharmacy. The logistics industry refers to this as the \u201ccold chain\u201d and it encompasses both \u201creefers\u201d (refrigerated containers) as well as warehouses, distribution centers and the final storage or holding areas. Throughout this chain the risk of failure is ever-present, meaning there is always a possibility of cargo exceeding permissible or safe temperature levels, even if only briefly. For example, a truck might be stopped without power in desert heat, allowing temperatures in the reefer to rise. Then power is restored and the temperature in the container comes back down, but the product is damaged. When cargo with such as any of those items listed above are exposed to temperatures outside of prescribed limits it can be damaged. In some cases this is evident, such as with bananas, but in other situations, like the transport of vaccines, it may not be apparent that damage has occurred and the vaccine becomes ineffective. For some products, going over temperature, even only briefly, can reduce shelf life dramatically, incurring substantial costs when it cannot be sold. Organizations contracting to ship perishable products often specify the permissible temperature range. However, even if it is possible to show that product was exposed to conditions outside of those contracted, proving where it happened, and thus responsibility, can be much harder.","title":"Introduction"},{"location":"analyze/predictive-maintenance/#predictive-maintenance-problem-statement","text":"If you want a good understanding of the problem space for predictive maintenance read, Yana Ageeva's article in toward data science . The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly From a methodology point of view the Data Scientist needs to address the following questions: What type of failure to consider and which one to predict? What kind of failure is happening? slow degradation or instantaneous failure? What could be the relation between a product characteristics and the failure? What kind of measure exist to assess the given characteristic? Which measurements correspond to good functioning and which ones correspond to failure? How often metrics are reported? What question the model should answer? What kind of output should the model give? How long in advance should the model be able to indicate that a failure will occur? What are the business impact to do not predict the failure? and predicting false negative failure? What is the expected accuracy?","title":"Predictive maintenance problem statement"},{"location":"analyze/predictive-maintenance/#reefer-problem-types","text":"There are multiple different potential issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions\" issue: Sensors in the refrigeration unit need to be calibrated and be continuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which leads to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be difficult to know if there is an issue or not. The other common potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. Blocked air passage: to keep the temperature homogenous inside the reefer. So the question we want to answer is: does the Reefer keep accurate temperature overtime between what is set versus what is measured?","title":"Reefer problem types"},{"location":"analyze/predictive-maintenance/#modeling-techniques","text":"The model uses the generated data from above scenarios: When the container's door is open for a longer time - this gives a false positive that maintainence is required. When sensors are malfunctioning, it records arbitrary readings. When the readings are normal. We have currently trained our model on 3000 datapoints from the three scenarios above. There are different modeling approach to tackle predictive maintenance: regression model classification to predict failure for a given time period classify anomalous behavior: classes are not known in advance. Normal operation is known. compute probability of failure over time","title":"Modeling techniques"},{"location":"analyze/predictive-maintenance/#code-execution","text":"The simulator continuosly generates container metrics, publishes it to Kafka and run the predictMaintainence.ipynb to predict if maintainence is sought at this point in time.","title":"Code execution"},{"location":"analyze/predictive-maintenance/#model-description","text":"We are using Machine Learning supervised learning here. There are two types of supervised learning - 1) Classification: Predict a categorical response, 2) Regression: Predict a continuous response","title":"Model description"},{"location":"analyze/predictive-maintenance/#linear-regression","text":"Pros: 1) Fast 2) No tuning required 3) Highly interpretable 4) Well-understood Cons: 1) Unlikely to produce the best predictive accuracy 2) Presumes a linear relationship between the features and response 3) If the relationship is highly non-linear as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate","title":"Linear regression"},{"location":"analyze/predictive-maintenance/#naive-bayes-classification","text":"Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent. The classification is conducted by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively. Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator.","title":"Naive Bayes classification"},{"location":"analyze/predictive-maintenance/#model-evaluation","text":"We are using Root Mean Squared Error (RMSE) for evaluating the model performance. Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors. Classification does better here as the scenarion is more of a classification problem.","title":"Model evaluation"},{"location":"analyze/predictive-maintenance/#references","text":"Understand Reefer container For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting. PREDICTION OF TEMPERATURE INSIDE A REFRIGERATED CONTAINER IN THE PRESENCE OF PERISHABLE GOODS Temperature Monitoring During Transportation, Storage and Processing of Perishable Products Understanding machine learning classifiers","title":"References"},{"location":"analyze/ws-ml-dev/","text":"Watson Studio: developing the predictive model Cloud Pak for data integrate a well known product called watson Studio. In this chapter we are using two approaches to develop the model, one using AutoAI and one use notebooks. AutoAI AutoAI uses data to automatically select the best supervised algorithm to determine the best classification or regression models with optimized hyper parameters. Notebook In this chapter, we are developing a notebook on the telemetries dataset. We have already a project, and defined a data set from the data collection step . In the project view, use Add to project and select the Notebook Add a cell to access pandas, and maplablib in the first cell, and then get the code for accessing the dataset, by using the top right button:","title":"Define the anomaly detection scoring model with Watson Studio"},{"location":"analyze/ws-ml-dev/#watson-studio-developing-the-predictive-model","text":"Cloud Pak for data integrate a well known product called watson Studio. In this chapter we are using two approaches to develop the model, one using AutoAI and one use notebooks.","title":"Watson Studio: developing the predictive model"},{"location":"analyze/ws-ml-dev/#autoai","text":"AutoAI uses data to automatically select the best supervised algorithm to determine the best classification or regression models with optimized hyper parameters.","title":"AutoAI"},{"location":"analyze/ws-ml-dev/#notebook","text":"In this chapter, we are developing a notebook on the telemetries dataset. We have already a project, and defined a data set from the data collection step . In the project view, use Add to project and select the Notebook Add a cell to access pandas, and maplablib in the first cell, and then get the code for accessing the dataset, by using the top right button:","title":"Notebook"},{"location":"bpm/readme/","text":"Maintenance field engineer dispatching business process Problem statement When the anomaly detection scoring service creates a maintenance record on the containers topic, the container microservice will dispatch a field engineer so that the engineer can go the reefer container if it was unloaded in the destination harbor. The process of scheduling an engineer and then completing the work can best be facilitated through a process based, structured workflow. We will be using IBM BPM on Cloud or Cloud Pak for Automation to best demonstrate the workflow. Before creating a ticket or while adding information to the ticket, the process may get the container estimated arrival time and destination harbor to do the dispatching. This means accessing an API on container microservice. Business Process Model This process demonstrates the flow of the task through the process workflow Craft Voyage Payload Script - This step assembles the payload for communicating to the reefer container microservice for status updates. We will craft it from the incoming payload. Set Machine To Maintenance Service Flow - This step creates a REST call to the reefer container to set the machine to Maintenance Mode Schedule Dispatch User Interface - This step gives the scheduler team an interface to look at the given maintenance request and reefer container with issues and manually assign a worker to provide physical support on it Work Order Completion User Interface - This step allows the worker to complete a simple form marking the request as fixed and updating comments relating to the effort Was Completed? Decision Gateway - This decision gateway determines whether to reassign the task to the worker to fill out on a later date (such as equipment not being available), or allows to finish the process Set Machine Off Maintenance Service Flow - This step initiates a REST call to process the changes, capacity, and updated information from the previous captured in the Work Order Completion step. Deploy TWX to BPM on Cloud The following simple video shows you how to deploy the exported 'twx' file to BPM on Cloud. The same can be done on Cloud Pak for automation. Demo of Workflow The following short video presents how to capture our CSRF Token for the Call to Kick off BPM/BAW We get the CSRF Token at first, and then find the Acronym such as via the Designer When we actually execute the request we need the CSRF Token, the Model, the Acronym, and the input data Here we can see the Model as the Process Name directly from Designer Our input data is the input variables in Name,Value pair format We then execute the request and get our 201 Success! In this step we open up process portal, claim, and see our Dispatch Scheduler UI. When we click \"Retrieve Current Location\" we query the container via ID to get updated container info if needed. This code also executes on Load. It is then the Scheduler's job to assign a worker, so we rely on a Worker to be selected from the dropdown We now see the Work Order Completion UI, the worker will pick up the task and work it. We allow for the latitude and logitude to be updated, the capacity to be modified, an audit log to capture comment history, and whether or not the work was able to be completed. Upon finishing we complete the Work Order and via BPM/BAW message the reefer container microservices with the new status Here we see the container updated with the value of 25 we put in as a worker","title":"Engineer dispatching with Business process with Cloud Pak for Automation"},{"location":"bpm/readme/#maintenance-field-engineer-dispatching-business-process","text":"","title":"Maintenance field engineer dispatching business process"},{"location":"bpm/readme/#problem-statement","text":"When the anomaly detection scoring service creates a maintenance record on the containers topic, the container microservice will dispatch a field engineer so that the engineer can go the reefer container if it was unloaded in the destination harbor. The process of scheduling an engineer and then completing the work can best be facilitated through a process based, structured workflow. We will be using IBM BPM on Cloud or Cloud Pak for Automation to best demonstrate the workflow. Before creating a ticket or while adding information to the ticket, the process may get the container estimated arrival time and destination harbor to do the dispatching. This means accessing an API on container microservice.","title":"Problem statement"},{"location":"bpm/readme/#business-process-model","text":"This process demonstrates the flow of the task through the process workflow Craft Voyage Payload Script - This step assembles the payload for communicating to the reefer container microservice for status updates. We will craft it from the incoming payload. Set Machine To Maintenance Service Flow - This step creates a REST call to the reefer container to set the machine to Maintenance Mode Schedule Dispatch User Interface - This step gives the scheduler team an interface to look at the given maintenance request and reefer container with issues and manually assign a worker to provide physical support on it Work Order Completion User Interface - This step allows the worker to complete a simple form marking the request as fixed and updating comments relating to the effort Was Completed? Decision Gateway - This decision gateway determines whether to reassign the task to the worker to fill out on a later date (such as equipment not being available), or allows to finish the process Set Machine Off Maintenance Service Flow - This step initiates a REST call to process the changes, capacity, and updated information from the previous captured in the Work Order Completion step.","title":"Business Process Model"},{"location":"bpm/readme/#deploy-twx-to-bpm-on-cloud","text":"The following simple video shows you how to deploy the exported 'twx' file to BPM on Cloud. The same can be done on Cloud Pak for automation.","title":"Deploy TWX to BPM on Cloud"},{"location":"bpm/readme/#demo-of-workflow","text":"The following short video presents how to capture our CSRF Token for the Call to Kick off BPM/BAW We get the CSRF Token at first, and then find the Acronym such as via the Designer When we actually execute the request we need the CSRF Token, the Model, the Acronym, and the input data Here we can see the Model as the Process Name directly from Designer Our input data is the input variables in Name,Value pair format We then execute the request and get our 201 Success! In this step we open up process portal, claim, and see our Dispatch Scheduler UI. When we click \"Retrieve Current Location\" we query the container via ID to get updated container info if needed. This code also executes on Load. It is then the Scheduler's job to assign a worker, so we rely on a Worker to be selected from the dropdown We now see the Work Order Completion UI, the worker will pick up the task and work it. We allow for the latitude and logitude to be updated, the capacity to be modified, an audit log to capture comment history, and whether or not the work was able to be completed. Upon finishing we complete the Work Order and via BPM/BAW message the reefer container microservices with the new status Here we see the container updated with the value of 25 we put in as a worker","title":"Demo of Workflow"},{"location":"collect/cp4d-collect-data/","text":"IBM Cloud Pak for Data: data collection To develop the anomaly predictive service we first need to access the data. We have two datasources in this example: the product information and the telemetries data coming from the different Reefer Containers. With the telemetries we should be able to assess anomaly. The Telemetries are saved to a noSQL database. We are using MongoDB on IBM Cloud. Using Mongo Compass , we can see one of telemetry document as saved into MongoDB. Figure 1: Mongo DB Compass: ibmcloud.telemetries collection It is important to note that the Json document has sensors document embedded. As we will see later they will be mapped to different tables in Cloud Pak Virtualization. As part of the data governance capability, a user with data engineer role can do the following tasks: Define one to many connections to the remote different data sources Create virtual assets to materialize tables and views from the different data sources Assign an asset to an exisint project or a data request (governance object to ask to access data) Define connection First we need to get the connection information for the MongoDB database. See this note for information about Mongo DB instance on IBM Cloud. Get the information about the data connection. Figure 2: Mongo DB on IBM Cloud connection information Then download the TLS certificate as pem file: ibmcloud login -a https://cloud.ibm.com -u passcode -p <somecode-you-get-from-your-login> # Define your resource group ibmcloud target -g gse-eda ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem Back to Cloud pak for Data, an administrator may define connections as a reusable objects by entering the data sources information. The figure below illustrates the connection configuration to the Mongo DB running on IBM Cloud: Figure 3: Define connection in CP4D Add connection in Cloud Pak for Data . Virtualization can help automatically group tables, so it simplifies to group different data elements into a single schema. Once define a Data scientist use this connection in the Collect menu to define a new Data virtualization definition to discover the telemetries data. Create a new project Once logged into Cloud Pak for Data, create a new project. A project is a collection of assets the analytics team work on. It can include data assets and Notebooks, RStudio files Models, scripts... From the main page select the project view: Figure 4: Top level navigation menu and then new project, and select analytics : Figure 5: Add project Select an empty project: Figure 6: Select project type Enter basic information about your project Figure 7: Project metadata The result is the access to the main project page: Figure 8: Project main page Now we need to define data assets into our project... Data Virtualization As introduced in this paragraph , we want to use data virtualization to access the historical telemetry records: The data engineer uses the Data virtualization capability to search for existing tables and add the tables he wants in the cart . For that, he uses the Virtualize menu Figure 9: Data Virtualization menu and then selects Mongo DB in the Filters column and may be apply some search on specific database name. Figure 10: Data Virtualization on Mongo DB Once done, he selects the expected tables and then use Add to cart link. It is important to note that we have two tables to match the telemetry json document and the sensors sub json document. The next step is to assign them to a project: Figure 11: Data Virtualization cart and tables Watson Knowledge Catalog Starting the data governance with IBM Watson\u00ae Knowledge Catalog to help Data scientists and data stewards to find, prepare and understand the data they need. With the Catalog you can define business terms, add data governance rules to enforce protection to data assets: deny, redact, subsitute or mask columns. Then you can see the relationships between a term and assets, policies using it. The figure below illustrates the anonymized column containing sensitive information, by using policies, substitution rules and business terms. For more detail on how to build those rule see this product tour . Create a joined view We need to join the telemetries and the sensors data into the same table, to flatten the records. In the current document there is a 1 to 1 relationship between telemetry and telemetry sensor, so it is easy to flatten the model in one table. In the Data Virtualization, a data steward selects My Virtualized data , and then select TELEMETRICS and TELEMETRICS_SENSORS tables, then the Join view . Within this new panel, he needs to create a join key, by drag the TELEMETRICS_ID and _ID together: Figure 12: Joining tables Once joined, the new view is created, he can then get those new assets as part of his project (Use Add to project ). The figure below show this new asset in the project: Figure 13: Telemetries asset in the project With some data: Figure 14: Telemetry data Next is to start working within a model \u2192 Next \u2192","title":"Collect data with Cloud Pak for Data"},{"location":"collect/cp4d-collect-data/#ibm-cloud-pak-for-data-data-collection","text":"To develop the anomaly predictive service we first need to access the data. We have two datasources in this example: the product information and the telemetries data coming from the different Reefer Containers. With the telemetries we should be able to assess anomaly. The Telemetries are saved to a noSQL database. We are using MongoDB on IBM Cloud. Using Mongo Compass , we can see one of telemetry document as saved into MongoDB. Figure 1: Mongo DB Compass: ibmcloud.telemetries collection It is important to note that the Json document has sensors document embedded. As we will see later they will be mapped to different tables in Cloud Pak Virtualization. As part of the data governance capability, a user with data engineer role can do the following tasks: Define one to many connections to the remote different data sources Create virtual assets to materialize tables and views from the different data sources Assign an asset to an exisint project or a data request (governance object to ask to access data)","title":"IBM Cloud Pak for Data: data collection"},{"location":"collect/cp4d-collect-data/#define-connection","text":"First we need to get the connection information for the MongoDB database. See this note for information about Mongo DB instance on IBM Cloud. Get the information about the data connection. Figure 2: Mongo DB on IBM Cloud connection information Then download the TLS certificate as pem file: ibmcloud login -a https://cloud.ibm.com -u passcode -p <somecode-you-get-from-your-login> # Define your resource group ibmcloud target -g gse-eda ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem Back to Cloud pak for Data, an administrator may define connections as a reusable objects by entering the data sources information. The figure below illustrates the connection configuration to the Mongo DB running on IBM Cloud: Figure 3: Define connection in CP4D Add connection in Cloud Pak for Data . Virtualization can help automatically group tables, so it simplifies to group different data elements into a single schema. Once define a Data scientist use this connection in the Collect menu to define a new Data virtualization definition to discover the telemetries data.","title":"Define connection"},{"location":"collect/cp4d-collect-data/#create-a-new-project","text":"Once logged into Cloud Pak for Data, create a new project. A project is a collection of assets the analytics team work on. It can include data assets and Notebooks, RStudio files Models, scripts... From the main page select the project view: Figure 4: Top level navigation menu and then new project, and select analytics : Figure 5: Add project Select an empty project: Figure 6: Select project type Enter basic information about your project Figure 7: Project metadata The result is the access to the main project page: Figure 8: Project main page Now we need to define data assets into our project...","title":"Create a new project"},{"location":"collect/cp4d-collect-data/#data-virtualization","text":"As introduced in this paragraph , we want to use data virtualization to access the historical telemetry records: The data engineer uses the Data virtualization capability to search for existing tables and add the tables he wants in the cart . For that, he uses the Virtualize menu Figure 9: Data Virtualization menu and then selects Mongo DB in the Filters column and may be apply some search on specific database name. Figure 10: Data Virtualization on Mongo DB Once done, he selects the expected tables and then use Add to cart link. It is important to note that we have two tables to match the telemetry json document and the sensors sub json document. The next step is to assign them to a project: Figure 11: Data Virtualization cart and tables","title":"Data Virtualization"},{"location":"collect/cp4d-collect-data/#watson-knowledge-catalog","text":"Starting the data governance with IBM Watson\u00ae Knowledge Catalog to help Data scientists and data stewards to find, prepare and understand the data they need. With the Catalog you can define business terms, add data governance rules to enforce protection to data assets: deny, redact, subsitute or mask columns. Then you can see the relationships between a term and assets, policies using it. The figure below illustrates the anonymized column containing sensitive information, by using policies, substitution rules and business terms. For more detail on how to build those rule see this product tour .","title":"Watson Knowledge Catalog"},{"location":"collect/cp4d-collect-data/#create-a-joined-view","text":"We need to join the telemetries and the sensors data into the same table, to flatten the records. In the current document there is a 1 to 1 relationship between telemetry and telemetry sensor, so it is easy to flatten the model in one table. In the Data Virtualization, a data steward selects My Virtualized data , and then select TELEMETRICS and TELEMETRICS_SENSORS tables, then the Join view . Within this new panel, he needs to create a join key, by drag the TELEMETRICS_ID and _ID together: Figure 12: Joining tables Once joined, the new view is created, he can then get those new assets as part of his project (Use Add to project ). The figure below show this new asset in the project: Figure 13: Telemetries asset in the project With some data: Figure 14: Telemetry data Next is to start working within a model \u2192 Next \u2192","title":"Create a joined view"},{"location":"collect/cp4i-es/","text":"Data ingestion and in motion with Cloud Pak for Integration","title":"Data ingestion and in motion with Cloud Pak for Integration"},{"location":"collect/cp4i-es/#data-ingestion-and-in-motion-with-cloud-pak-for-integration","text":"","title":"Data ingestion and in motion with Cloud Pak for Integration"},{"location":"collect/generate-telemetry/","text":"Generate telemetry data in MongoDB We are using the simulator to generate data. In the industry, when developing new manufactured product, the engineers do not have a lot of data so they also use a mix of real sensors with simulators to create fake but realistic data to develop and test their models. The historical data need to represent failure and represent the characteristics of a Reefer container. We have defined some sensors to get interesting correlated or independent features. As of now, our telemetry event structure can be seen in this avro schema . For the machine learning environment we can use a csv file or mongodb database or kafka topic as data source. The data generation environment looks like in the figure below: Figure 1: Data collection Simulation The simulator can run as a standalone tool (1) to create training and test data to be saved in a remote mongodb database or can be used to save to csv file. when it runs to simulate reefer container telemetry generation (2), it creates events to Kafka topic, and a stream application can save telemetry records to MongoDB too. We use MongoDB as a Service on IBM Cloud in our reference implementations. Figure 2: IBM Cloud Database We have provided the following documented methods for populating the Product database: Create and save telemetry data with Kubernetes Job running on remote cluster (RECOMMENDED) Create local telemetry data manually Save local telemetry data to MongoDB Create and save telemetry data with Kubernetes Job running on remote cluster In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. The predefined job for this task will create the required telemetry data and save it directly to the configured MongoDB database instance. This is the most direct method for telemetry data generation and what is necessary for the \"happy path\" version of the environment deployment. If you have use case needs that require other locations for data to be saved or transmitted, you can either adapt the Job here or follow the other sections of this document. Utilizing Databases for MongoDB on IBM Cloud, the following Kubernetes Secrets are required to be created from the auto-generated Service credentials in the target namespace: mongodb-url (in the format of hostname-a,hostname-b , as the endpoint is a paired replica set) kubectl create secret generic mongodb-url --from-literal = binding = '1a2...domain.cloud:30796,1a2c....cloud:30796' mongodb-user kubectl create secret generic mongodb-user --from-literal = binding = 'ibm_cloud_...' mongodb-pwd kubectl create secret generic mongodb-pwd --from-literal = binding = '335....223' mongodb-ca-pem (this requires use of the Cloud Databases CLI Plug-in for the IBM Cloud CLI) ibmcloud cdb deployment-cacert [ MongoDB on IBM Cloud service instance name ] > mongodb.crt kubectl create secret generic mongodb-ca-pem --from-literal = binding = \" $( cat mongodb.crt ) \" Review /scripts/createMongoTelemetryData.yaml and update lines 32, 34, 36, or 38 with any additional modifications to the generated telemetry data. Additional rows can be added as needed in the same job execution. The following fields are acceptable inputs: stype can the normal , poweroff , o2sensor , and co2sensor . cid can be C01 , C02 , C03 , or C04 . pid can be P01 , P02 , P03 , or P04 . records can be any positive integer (within reason). Create the create-telemetry-data Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createMongoTelemetryData.yaml You can tail the created pod's output to see the progress of the database initialization: kubectl logs -f --selector = job-name = create-telemetry-data Create local telemetry data manually If not done yet, you can use our docker image to get an isolated python environment. For that do the following preparation steps: # From refarch_reefer_ml folder cd docker docker build -t ibmcase/python -f docker-python-tools.yaml . docker images | grep ibmcase # .... results should include ibmcase/python latest a89153c0e14f The dockerfile installed the needed dependencies and use pipenv. Also we preset the PYTHONPATH environment variable to /home to specify where python should find the application specifics modules. Generate data as csv file Start the python environment From the docker image created before, use the provided script as: # From refarch_reefer_ml folder ./startPythonEnv.sh Generate power off metrics When a reefer container loses power, restart and reloose it, it may become an issue. This is the goal of this simulation. The simulator accepts different arguments as specified below: usage reefer_simulator-tool --stype [ poweroff | co2sensor | o2sensor | normal ] --cid [ C01 | C02 | C03 | C04 | C05 ] --product_id [ P01 | P02 | P03 | P04 | P05 ] --records <the number of records to generate> --file <the filename to create> --append | --db The cid is for the container id. As the simulator is taking some data from internal datasource you can use only one of those values: [C01 | C02 | C03 | C04 | C05] product_id is also one of the value [ P01 | P02 | P03 | P04 | P05] , as the simulator will derive the target temperature and humidity level from its internal datasource: ('P01','Carrots',1,4,0.4), ('P02','Banana',2,6,0.6), ('P03','Salad',1,4,0.4), ('P04','Avocado',2,6,0.4), ('P05','Tomato',1,6,0.4); --db is when you want to save the telemetry into mongodb DB. In this case be sure to have set the credentials and URL in the scripts/setenv.sh script (see the scripts/setenv-tmp.sh template file) --file is to specify a csv file to write the data --append is used to append the output of this run to an existing file: It permits to accumulate different simulation in the same dataset. (Re)create a new file. It is an important step to get the column names as first row. python simulator/reefer_simulator_tool.py --stype poweroff --cid C01 --records 1000 --product_id P02 --file telemetries.csv then new records are added by appending to existing file python simulator/reefer_simulator_tool.py --cid C03 --product_id P03 --records 1000 --file telemetries.csv --stype poweroff --append The results looks like: Generating 1000 poweroff metrics Timestamp ID Temperature ( celsius ) Target_Temperature ( celsius ) Power PowerConsumption ContentType O2 CO2 Time_Door_Open Maintenance_Required Defrost_Cycle 1 .000000 2019 -06-30 T15:43 Z 101 3 .416766 4 17 .698034 6 .662044 1 11 1 8 .735273 0 6 1 .001001 2019 -06-30 T15:43 Z 101 4 .973630 4 3 .701072 8 .457314 1 13 3 5 .699655 0 6 1 .002002 2019 -06-30 T15:43 Z 101 1 .299275 4 7 .629094 From the two previous commands you should have 2001 rows (one for the header which will be used in the model creation): wc -l telemetries.csv 2001 telemetries.csv Generate Co2 sensor malfunction in same file In the same way as above the simulator can generate data for Co2 sensor malfunction using the command: python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype co2sensor --append Generate O2 sensor malfunction in same file python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype o2sensor --append Save local telemetry data to MongoDB MongoDB is a popular document-based database that allows developers to quickly build projects without worrying about schema. Mongo components include: mongod - the core process that runs the actual database mongos - controls routing to the databases in case of sharding config servers (CSRS) - stores the metadata in a sharded environment. We propose to persist telemetry for a long time period. For example we can configure Kafka topic to persist telemetries over a period of 20 days, but have another component to continuously move events as JSON documents inside Mongodb. Using Mongodb as service on IBM Cloud Create the MongoDB service on IBM cloud using default configuration and add a service credentials to get the mongodb.composed url: (something starting as mongodb://ibm_cloud_e154ff52_ed ) username and password. Set those environment variables in scripts/setenv.sh the export MONGO_DB_URL=\"mongodb://ibm_c...\" Get the TLS certificate as pem file: ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem Start python environment Use IBMCLOUD if you use mongodb, postgresql and kafka on cloud, or LOCAL for kafka and postgresql running via docker compose. ./startPythonEnv IBMCLOUD root@03721594782f: cd /home If you are using your own environment, to access mongodb we use the pymongo driver ( pip install pymongo ) The code below is a simple example of how to access mongodb. URL = os . getenv ( 'MONGO_DB_URL' ) client = MongoClient ( URL , ssl = True , ssl_ca_certs = '/home/certs/mongodbca.pem' ) db = client [ 'ibmclouddb' ] # insert a record result = db . telemetries . insert_one ( telemetry ) telemetry = db . telemetries . find_one ({ \"_id\" : ObjectId ( result . inserted_id )}) # get all the records telemetries = db . telemetries . find () for t in telemetries : See the rest of the code in ml/data/ToMongo.py to load records from CSV file, or the simulator/infrastructure/ReeferRepository.py for the one generating metrics and uploading them directly to MongoDB. We propose two approaches to load data to MongoDB: use created csv file, or run the simulator tool connected to MongoDB. Add data from csv file Using the ToMongo.py script we can load the data from the ml/data/telemetries.csv file to mongodb. In a Terminal window uses the following commmand: ./startPythonEnv.sh IBMCLOUD cd ml/data python ToMongo.py Add data using the telemetry repository of the simulator Verify your MONGO* environment variables are set according to your created service in the scriptssetenv.sh file. ./startPythonEnv.sh IBMCLOUD cd simulation python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype poweroff --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype co2sensor --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype o2sensor --db Verify data with mongo CLI To verify the data loaded into the database we use mongo CLI with the following command: # to connect mongo -u $USERNAME -p $PASSWORD --tls --tlsCAFile mongodb.pem --authenticationDatabase admin --host replset/1a2ce8ca-<>.bn<>c0.databases.appdomain.cloud:30796 --tlsAllowInvalidCertificates The full host name is masked . The USERNAME and PASSWORD are environment variables you set from the IBM Cloud service credentials. Something like: { \"connection\": { \"cli\": { \"arguments\": [ [ \"-u\", \"ibm_cloud_<>_48b1_b899\", \"-p\", \"3359192a<>e5fbdf51fd573676e58aff4f9e223\", The mongodb.pem is the certificate in text save in a file. To get this certificate you can use the following command: ibmcloud cdb deployment-cacert gse-eda-mongodb > mongodb.pem or by using the IBM Cloud mongodb service user interface (Manage menu): Once connected use the ibmclouddb database with mongo CLI: MongoDB server version: 4.2.0 replset:PRIMARY> use ibmclouddb replset:PRIMARY> db.getCollection(\"telemetries\").find() Using MongoDB on Openshift 4.2 on-premise We assume you have provisioned an Openshift 4.2 cluster and logged in. We use the following container image: centos/mongodb-36-centos7 . So to install it, we use the following command: oc new-app -e \\ MONGODB_USER=mongo,MONGODB_PASSWORD=<password>,MONGODB_DATABASE=reeferdb,MONGODB_ADMIN_PASSWORD=<admin_password> \\ centos/mongodb-36-centos7 Connect to the pod and then use the mongo CLI $ oc get pods NAME READY STATUS RESTARTS AGE mongodb-36-centos7-1-wcn7h 1 /1 Running 0 4d $ oc rsh mongodb-36-centos7-1-wcn7h bash-4.2$ mongo -u $MONGODB_USER -p $MONGODB_PASSWORD $MONGODB_DATABASE MongoDB shell version: 2 .4.9 connecting to: reeferdb > show collections To remove the db on openshift: oc delete dc The document saved in mongo will not be a flat record as produced by the simulator when it creates records to Kafka, but a Json document which matches the following format, where sensors are in their own object: { \"timestamp\" : \"2019-09-04 T15:31 Z\" , \"containerID\" : \"C100\" , \"product_id\" : \"P02\" , \"content_type\" : 2 , \"sensors\" : { \"temperature\" : 5.49647 , \"oxygen_level\" : 20.4543 , \"nitrogen_level\" : 79.4046 , \"carbon_dioxide_level\" : 4.42579 , \"humidity_level\" : 60.3148 , \"fan_1\" : \"True\" , \"fan_2\" : \"True\" , \"fan_3\" : \"True\" , \"ambiant_temperature\" : 19.8447 }, \"target_temperature\" : 6.0 , \"kilowatts\" : 3.44686 , \"latitude\" : \"37.8226902168957,\" , \"longitude\" : \"-122.3248956640928\" , \"time_door_open\" : 0 , \"defrost_cycle\" : 6 } First be sure to set at least the following environment variables in the setenv.sh file MONGO_DB_URL, MONGO_SSL_PEM If not done before or to ensure the mongo connection works fine, run the ReeferRepository.py tool to create the telemetries collection using the following command: ./startPythonEnv.sh IBMCLOUD > python simulator/infrastructure/ReeferRepository.py Delete records in database In mongo CLI do: db.telemetries.deleteMany ({})","title":"Generate telemetry data"},{"location":"collect/generate-telemetry/#generate-telemetry-data-in-mongodb","text":"We are using the simulator to generate data. In the industry, when developing new manufactured product, the engineers do not have a lot of data so they also use a mix of real sensors with simulators to create fake but realistic data to develop and test their models. The historical data need to represent failure and represent the characteristics of a Reefer container. We have defined some sensors to get interesting correlated or independent features. As of now, our telemetry event structure can be seen in this avro schema . For the machine learning environment we can use a csv file or mongodb database or kafka topic as data source. The data generation environment looks like in the figure below: Figure 1: Data collection Simulation The simulator can run as a standalone tool (1) to create training and test data to be saved in a remote mongodb database or can be used to save to csv file. when it runs to simulate reefer container telemetry generation (2), it creates events to Kafka topic, and a stream application can save telemetry records to MongoDB too. We use MongoDB as a Service on IBM Cloud in our reference implementations. Figure 2: IBM Cloud Database We have provided the following documented methods for populating the Product database: Create and save telemetry data with Kubernetes Job running on remote cluster (RECOMMENDED) Create local telemetry data manually Save local telemetry data to MongoDB","title":"Generate telemetry data in MongoDB"},{"location":"collect/generate-telemetry/#create-and-save-telemetry-data-with-kubernetes-job-running-on-remote-cluster","text":"In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. The predefined job for this task will create the required telemetry data and save it directly to the configured MongoDB database instance. This is the most direct method for telemetry data generation and what is necessary for the \"happy path\" version of the environment deployment. If you have use case needs that require other locations for data to be saved or transmitted, you can either adapt the Job here or follow the other sections of this document. Utilizing Databases for MongoDB on IBM Cloud, the following Kubernetes Secrets are required to be created from the auto-generated Service credentials in the target namespace: mongodb-url (in the format of hostname-a,hostname-b , as the endpoint is a paired replica set) kubectl create secret generic mongodb-url --from-literal = binding = '1a2...domain.cloud:30796,1a2c....cloud:30796' mongodb-user kubectl create secret generic mongodb-user --from-literal = binding = 'ibm_cloud_...' mongodb-pwd kubectl create secret generic mongodb-pwd --from-literal = binding = '335....223' mongodb-ca-pem (this requires use of the Cloud Databases CLI Plug-in for the IBM Cloud CLI) ibmcloud cdb deployment-cacert [ MongoDB on IBM Cloud service instance name ] > mongodb.crt kubectl create secret generic mongodb-ca-pem --from-literal = binding = \" $( cat mongodb.crt ) \" Review /scripts/createMongoTelemetryData.yaml and update lines 32, 34, 36, or 38 with any additional modifications to the generated telemetry data. Additional rows can be added as needed in the same job execution. The following fields are acceptable inputs: stype can the normal , poweroff , o2sensor , and co2sensor . cid can be C01 , C02 , C03 , or C04 . pid can be P01 , P02 , P03 , or P04 . records can be any positive integer (within reason). Create the create-telemetry-data Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createMongoTelemetryData.yaml You can tail the created pod's output to see the progress of the database initialization: kubectl logs -f --selector = job-name = create-telemetry-data","title":"Create and save telemetry data with Kubernetes Job running on remote cluster"},{"location":"collect/generate-telemetry/#create-local-telemetry-data-manually","text":"If not done yet, you can use our docker image to get an isolated python environment. For that do the following preparation steps: # From refarch_reefer_ml folder cd docker docker build -t ibmcase/python -f docker-python-tools.yaml . docker images | grep ibmcase # .... results should include ibmcase/python latest a89153c0e14f The dockerfile installed the needed dependencies and use pipenv. Also we preset the PYTHONPATH environment variable to /home to specify where python should find the application specifics modules.","title":"Create local telemetry data manually"},{"location":"collect/generate-telemetry/#generate-data-as-csv-file","text":"","title":"Generate data as csv file"},{"location":"collect/generate-telemetry/#start-the-python-environment","text":"From the docker image created before, use the provided script as: # From refarch_reefer_ml folder ./startPythonEnv.sh","title":"Start the python environment"},{"location":"collect/generate-telemetry/#generate-power-off-metrics","text":"When a reefer container loses power, restart and reloose it, it may become an issue. This is the goal of this simulation. The simulator accepts different arguments as specified below: usage reefer_simulator-tool --stype [ poweroff | co2sensor | o2sensor | normal ] --cid [ C01 | C02 | C03 | C04 | C05 ] --product_id [ P01 | P02 | P03 | P04 | P05 ] --records <the number of records to generate> --file <the filename to create> --append | --db The cid is for the container id. As the simulator is taking some data from internal datasource you can use only one of those values: [C01 | C02 | C03 | C04 | C05] product_id is also one of the value [ P01 | P02 | P03 | P04 | P05] , as the simulator will derive the target temperature and humidity level from its internal datasource: ('P01','Carrots',1,4,0.4), ('P02','Banana',2,6,0.6), ('P03','Salad',1,4,0.4), ('P04','Avocado',2,6,0.4), ('P05','Tomato',1,6,0.4); --db is when you want to save the telemetry into mongodb DB. In this case be sure to have set the credentials and URL in the scripts/setenv.sh script (see the scripts/setenv-tmp.sh template file) --file is to specify a csv file to write the data --append is used to append the output of this run to an existing file: It permits to accumulate different simulation in the same dataset. (Re)create a new file. It is an important step to get the column names as first row. python simulator/reefer_simulator_tool.py --stype poweroff --cid C01 --records 1000 --product_id P02 --file telemetries.csv then new records are added by appending to existing file python simulator/reefer_simulator_tool.py --cid C03 --product_id P03 --records 1000 --file telemetries.csv --stype poweroff --append The results looks like: Generating 1000 poweroff metrics Timestamp ID Temperature ( celsius ) Target_Temperature ( celsius ) Power PowerConsumption ContentType O2 CO2 Time_Door_Open Maintenance_Required Defrost_Cycle 1 .000000 2019 -06-30 T15:43 Z 101 3 .416766 4 17 .698034 6 .662044 1 11 1 8 .735273 0 6 1 .001001 2019 -06-30 T15:43 Z 101 4 .973630 4 3 .701072 8 .457314 1 13 3 5 .699655 0 6 1 .002002 2019 -06-30 T15:43 Z 101 1 .299275 4 7 .629094 From the two previous commands you should have 2001 rows (one for the header which will be used in the model creation): wc -l telemetries.csv 2001 telemetries.csv","title":"Generate power off metrics"},{"location":"collect/generate-telemetry/#generate-co2-sensor-malfunction-in-same-file","text":"In the same way as above the simulator can generate data for Co2 sensor malfunction using the command: python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype co2sensor --append","title":"Generate Co2 sensor malfunction in same file"},{"location":"collect/generate-telemetry/#generate-o2-sensor-malfunction-in-same-file","text":"python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype o2sensor --append","title":"Generate O2 sensor malfunction in same file"},{"location":"collect/generate-telemetry/#save-local-telemetry-data-to-mongodb","text":"MongoDB is a popular document-based database that allows developers to quickly build projects without worrying about schema. Mongo components include: mongod - the core process that runs the actual database mongos - controls routing to the databases in case of sharding config servers (CSRS) - stores the metadata in a sharded environment. We propose to persist telemetry for a long time period. For example we can configure Kafka topic to persist telemetries over a period of 20 days, but have another component to continuously move events as JSON documents inside Mongodb.","title":"Save local telemetry data to MongoDB"},{"location":"collect/generate-telemetry/#using-mongodb-as-service-on-ibm-cloud","text":"Create the MongoDB service on IBM cloud using default configuration and add a service credentials to get the mongodb.composed url: (something starting as mongodb://ibm_cloud_e154ff52_ed ) username and password. Set those environment variables in scripts/setenv.sh the export MONGO_DB_URL=\"mongodb://ibm_c...\" Get the TLS certificate as pem file: ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem","title":"Using Mongodb as service on IBM Cloud"},{"location":"collect/generate-telemetry/#start-python-environment","text":"Use IBMCLOUD if you use mongodb, postgresql and kafka on cloud, or LOCAL for kafka and postgresql running via docker compose. ./startPythonEnv IBMCLOUD root@03721594782f: cd /home If you are using your own environment, to access mongodb we use the pymongo driver ( pip install pymongo ) The code below is a simple example of how to access mongodb. URL = os . getenv ( 'MONGO_DB_URL' ) client = MongoClient ( URL , ssl = True , ssl_ca_certs = '/home/certs/mongodbca.pem' ) db = client [ 'ibmclouddb' ] # insert a record result = db . telemetries . insert_one ( telemetry ) telemetry = db . telemetries . find_one ({ \"_id\" : ObjectId ( result . inserted_id )}) # get all the records telemetries = db . telemetries . find () for t in telemetries : See the rest of the code in ml/data/ToMongo.py to load records from CSV file, or the simulator/infrastructure/ReeferRepository.py for the one generating metrics and uploading them directly to MongoDB. We propose two approaches to load data to MongoDB: use created csv file, or run the simulator tool connected to MongoDB.","title":"Start python environment"},{"location":"collect/generate-telemetry/#add-data-from-csv-file","text":"Using the ToMongo.py script we can load the data from the ml/data/telemetries.csv file to mongodb. In a Terminal window uses the following commmand: ./startPythonEnv.sh IBMCLOUD cd ml/data python ToMongo.py","title":"Add data from csv file"},{"location":"collect/generate-telemetry/#add-data-using-the-telemetry-repository-of-the-simulator","text":"Verify your MONGO* environment variables are set according to your created service in the scriptssetenv.sh file. ./startPythonEnv.sh IBMCLOUD cd simulation python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype poweroff --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype co2sensor --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype o2sensor --db","title":"Add data using the telemetry repository of the simulator"},{"location":"collect/generate-telemetry/#verify-data-with-mongo-cli","text":"To verify the data loaded into the database we use mongo CLI with the following command: # to connect mongo -u $USERNAME -p $PASSWORD --tls --tlsCAFile mongodb.pem --authenticationDatabase admin --host replset/1a2ce8ca-<>.bn<>c0.databases.appdomain.cloud:30796 --tlsAllowInvalidCertificates The full host name is masked . The USERNAME and PASSWORD are environment variables you set from the IBM Cloud service credentials. Something like: { \"connection\": { \"cli\": { \"arguments\": [ [ \"-u\", \"ibm_cloud_<>_48b1_b899\", \"-p\", \"3359192a<>e5fbdf51fd573676e58aff4f9e223\", The mongodb.pem is the certificate in text save in a file. To get this certificate you can use the following command: ibmcloud cdb deployment-cacert gse-eda-mongodb > mongodb.pem or by using the IBM Cloud mongodb service user interface (Manage menu): Once connected use the ibmclouddb database with mongo CLI: MongoDB server version: 4.2.0 replset:PRIMARY> use ibmclouddb replset:PRIMARY> db.getCollection(\"telemetries\").find()","title":"Verify data with mongo CLI"},{"location":"collect/generate-telemetry/#using-mongodb-on-openshift-42-on-premise","text":"We assume you have provisioned an Openshift 4.2 cluster and logged in. We use the following container image: centos/mongodb-36-centos7 . So to install it, we use the following command: oc new-app -e \\ MONGODB_USER=mongo,MONGODB_PASSWORD=<password>,MONGODB_DATABASE=reeferdb,MONGODB_ADMIN_PASSWORD=<admin_password> \\ centos/mongodb-36-centos7 Connect to the pod and then use the mongo CLI $ oc get pods NAME READY STATUS RESTARTS AGE mongodb-36-centos7-1-wcn7h 1 /1 Running 0 4d $ oc rsh mongodb-36-centos7-1-wcn7h bash-4.2$ mongo -u $MONGODB_USER -p $MONGODB_PASSWORD $MONGODB_DATABASE MongoDB shell version: 2 .4.9 connecting to: reeferdb > show collections To remove the db on openshift: oc delete dc The document saved in mongo will not be a flat record as produced by the simulator when it creates records to Kafka, but a Json document which matches the following format, where sensors are in their own object: { \"timestamp\" : \"2019-09-04 T15:31 Z\" , \"containerID\" : \"C100\" , \"product_id\" : \"P02\" , \"content_type\" : 2 , \"sensors\" : { \"temperature\" : 5.49647 , \"oxygen_level\" : 20.4543 , \"nitrogen_level\" : 79.4046 , \"carbon_dioxide_level\" : 4.42579 , \"humidity_level\" : 60.3148 , \"fan_1\" : \"True\" , \"fan_2\" : \"True\" , \"fan_3\" : \"True\" , \"ambiant_temperature\" : 19.8447 }, \"target_temperature\" : 6.0 , \"kilowatts\" : 3.44686 , \"latitude\" : \"37.8226902168957,\" , \"longitude\" : \"-122.3248956640928\" , \"time_door_open\" : 0 , \"defrost_cycle\" : 6 } First be sure to set at least the following environment variables in the setenv.sh file MONGO_DB_URL, MONGO_SSL_PEM If not done before or to ensure the mongo connection works fine, run the ReeferRepository.py tool to create the telemetries collection using the following command: ./startPythonEnv.sh IBMCLOUD > python simulator/infrastructure/ReeferRepository.py","title":"Using MongoDB on Openshift 4.2 on-premise"},{"location":"collect/generate-telemetry/#delete-records-in-database","text":"In mongo CLI do: db.telemetries.deleteMany ({})","title":"Delete records in database"},{"location":"collect/oss-collect-data/","text":"As part of the IBM AI ladder practice introduced in the Data AI reference architecture and specially the collect step, we need to get a data topology in place to get data at rest so data scientist can do their data analysis, and feature preparation. In this solution, there are two datasources: the events in the kafka topic, using the event sourcing design pattern . the database about the Reefer, the fresh products and reefer telemetries As we do not have Reefer telemetry public data available, we are using our simulator to develop such data. The figure below illustrates this data injection simulation. Also you can use the simulator to create data in csv file, so there is no need to use postgresql to develop the model.","title":"Collect data with open source solution"},{"location":"collect/products-postgres/","text":"Define the products data into postgresql The Simulator references product data stored in a Postgresql database. There are multiple ways to populate this database depending on your level of experience with Postgresql, database services, and your local development environment. We have provided the following documented methods for populating the Product database: Kubernetes Job running on remote cluster (RECOMMENDED) Docker image running on local machine Postgresql CLI (psql) running on local machine Kubernetes Job running on remote cluster In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. Following the configuration prerequisistes defined in the Backing Services documentation for using Databases for PostgreSQL on IBM Cloud, you should already have the following Kubernetes Secrets defined in your target namespace: postgresql-url (in the format of jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=... ) kubectl create secret generic postgresql-url --from-literal = binding = 'jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=...' postgresql-user kubectl create secret generic postgresql-user --from-literal = binding = 'ibm_cloud_...' postgresql-pwd kubectl create secret generic postgresql-pwd --from-literal = binding = '1a2...9z0' postgresql-ca-pem (this requires use of the Cloud Databases CLI Plug-in for the IBM Cloud CLI) ibmcloud cdb deployment-cacert [ PostgreSQL on IBM Cloud service instance name ] > postgres.crt kubectl create secret generic postgresql-ca-pem --from-literal = binding = \" $( cat postgres.crt ) \" Create the create-postgres-tables Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createPGtables.yaml You can tail the created pod's output to see the progress of the database initialization: kubectl logs -f --selector = job-name = create-postgres-tables Docker image running on local machine The simulator code includes the infrastructure/ProductRepository.py that creates tables and adds some product definitions inside the table. Uncomment line 101 from /simulator/infrastructure/ProductRepository.py : # repo.populateProductsReferenceData() The following command is using our python environment docker image and the python code: ./scripts/createPGTables.sh IBMCLOUD Postgresql CLI (psql) running on local machine An alternate techniques is to use psql as described in this section. Previous experience with PSQL is recommended. We use a docker image to run psql: $ cd scripts $ ./startPsql.sh IBMCLOUD $ PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = > List relations... ibmclouddb => \\d Then create table if not done before: ibmclouddb => CREATE TABLE products ( product_id varchar ( 64 ) NOT NULL PRIMARY KEY , description varchar ( 100 ), target_temperature REAL , target_humidity_level REAL ); Populate the data: ibmclouddb => INSERT INTO products ( product_id , description , target_temperature , target_humidity_level ) VALUES ( 'P01' , 'Carrots' , 4 , 0.4 ), ( 'P02' , 'Banana' , 6 , 0.6 ), ( 'P03' , 'Salad' , 4 , 0.4 ), ( 'P04' , 'Avocado' , 6 , 0.4 ), ( 'P05' , 'Tomato' , 4 , 0.4 ); List the products SELECT * FROM products ; You should see: product_id | description | target_temperature | target_humidity_level | content_type ------------+-------------+--------------------+-----------------------+-------------- P01 | Carrots | 4 | 0.4 | 1 P02 | Banana | 6 | 0.6 | 2 P03 | Salad | 4 | 0.4 | 1 P04 | Avocado | 6 | 0.4 | 2 P05 | Tomato | 6 | 0.3 | 6 Exit the PSQL environment ibmclouddb => \\q","title":"Define products reference data"},{"location":"collect/products-postgres/#define-the-products-data-into-postgresql","text":"The Simulator references product data stored in a Postgresql database. There are multiple ways to populate this database depending on your level of experience with Postgresql, database services, and your local development environment. We have provided the following documented methods for populating the Product database: Kubernetes Job running on remote cluster (RECOMMENDED) Docker image running on local machine Postgresql CLI (psql) running on local machine","title":"Define the products data into postgresql"},{"location":"collect/products-postgres/#kubernetes-job-running-on-remote-cluster","text":"In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. Following the configuration prerequisistes defined in the Backing Services documentation for using Databases for PostgreSQL on IBM Cloud, you should already have the following Kubernetes Secrets defined in your target namespace: postgresql-url (in the format of jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=... ) kubectl create secret generic postgresql-url --from-literal = binding = 'jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=...' postgresql-user kubectl create secret generic postgresql-user --from-literal = binding = 'ibm_cloud_...' postgresql-pwd kubectl create secret generic postgresql-pwd --from-literal = binding = '1a2...9z0' postgresql-ca-pem (this requires use of the Cloud Databases CLI Plug-in for the IBM Cloud CLI) ibmcloud cdb deployment-cacert [ PostgreSQL on IBM Cloud service instance name ] > postgres.crt kubectl create secret generic postgresql-ca-pem --from-literal = binding = \" $( cat postgres.crt ) \" Create the create-postgres-tables Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createPGtables.yaml You can tail the created pod's output to see the progress of the database initialization: kubectl logs -f --selector = job-name = create-postgres-tables","title":"Kubernetes Job running on remote cluster"},{"location":"collect/products-postgres/#docker-image-running-on-local-machine","text":"The simulator code includes the infrastructure/ProductRepository.py that creates tables and adds some product definitions inside the table. Uncomment line 101 from /simulator/infrastructure/ProductRepository.py : # repo.populateProductsReferenceData() The following command is using our python environment docker image and the python code: ./scripts/createPGTables.sh IBMCLOUD","title":"Docker image running on local machine"},{"location":"collect/products-postgres/#postgresql-cli-psql-running-on-local-machine","text":"An alternate techniques is to use psql as described in this section. Previous experience with PSQL is recommended. We use a docker image to run psql: $ cd scripts $ ./startPsql.sh IBMCLOUD $ PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = > List relations... ibmclouddb => \\d Then create table if not done before: ibmclouddb => CREATE TABLE products ( product_id varchar ( 64 ) NOT NULL PRIMARY KEY , description varchar ( 100 ), target_temperature REAL , target_humidity_level REAL ); Populate the data: ibmclouddb => INSERT INTO products ( product_id , description , target_temperature , target_humidity_level ) VALUES ( 'P01' , 'Carrots' , 4 , 0.4 ), ( 'P02' , 'Banana' , 6 , 0.6 ), ( 'P03' , 'Salad' , 4 , 0.4 ), ( 'P04' , 'Avocado' , 6 , 0.4 ), ( 'P05' , 'Tomato' , 4 , 0.4 ); List the products SELECT * FROM products ; You should see: product_id | description | target_temperature | target_humidity_level | content_type ------------+-------------+--------------------+-----------------------+-------------- P01 | Carrots | 4 | 0.4 | 1 P02 | Banana | 6 | 0.6 | 2 P03 | Salad | 4 | 0.4 | 1 P04 | Avocado | 6 | 0.4 | 2 P05 | Tomato | 6 | 0.3 | 6 Exit the PSQL environment ibmclouddb => \\q","title":"Postgresql CLI (psql) running on local machine"},{"location":"devops/cd/","text":"Continuous Deployment Our Continuous Deployment (CD) approach focuses on a GitOps-based deployment model, using Git as a single source of truth for the deployment, management, and operations of our running application components. In this model, we have the flexibility to use multiple open-source to apply the single source of truth from a given Git repository onto a desired cluster environment. More detail around the background of GitOps and how it differs from traditional deployment models can be found in this blog post from WeaveWorks. One of the main tools that we use in this space is a GitOps-focused continuous deployment project named ArgoCD . As documented by the IBM Garage for Cloud team, ArgoCD can monitor GitHub-based projects and apply changes stored in that repository's YAML files to a running Kubernetes-based cluster. We have documented our general ArgoCD Continuous Delivery workflow in the ibm-cloud-architecture/refarch-kc-gitops repository. The details of our ArgoCD-based GitOps deployments are covered in the ArgoCD deployments section below. Another DevOps tool which provides the opportunity to deployment applications via the GitOps methodology is Tekton . The Tekton Pipelines project provides a declarative language for defining and executing both CI and CD-style pipelines, all defined with common Kubernetes-like nomenclature. It even has the capability to kick off pipeline runs based off of GitHub webhooks. The details of our Tekton & Appsody deployments are covered in the Tekton & Appsody deployments section below. ArgoCD deployments Our main continuous deployment pattern operates on the same principle of \"zero-infrastructure overhead\" as our continuous integration implementations. This allows us to be agile, adaptable, and efficient in what we deploy where. ArgoCD is a perfect companion to this principle, as we do not need additional long-running CD infrastructure to monitor either a source environment or a target deployment environment. Our CI process sits with our code (on the same hosted infrastructure), while our CD process sits with the target deployment environment (on the same Kubernetes-based cluster). To utilize ArgoCD in this manner, we define a set of Kubernetes YAMLs generated from helm template commands, with environment, namespace, and cluster-specific parameters provided as needed. The details of generating those helm template YAMLs can be found in our main Application Components documentation. The templated YAMLs are generated with the names of the required ConfigMaps and Secrets specific to a namespace on the eventual target deployment cluster. This allows us to create a deployment artifact programmatically without exposing confidential and secret credentials via source code. Once these YAMLs are generated, they are checked in to the main GitOps repository for the project, under a new branch with a branch name in the format of <namespace>/<cluster> for ease of identification. These will then have a folder structure of /<component-name>/templates/<artifact-type>.yaml , with most components providing Deployment, Service, and Route artifact YAMLs. An ArgoCD application is then created on the ArgoCD deployment inside the target environment that can read from the GitOps repository. ArgoCD can also deploy between clusters, which does come in handy in certain use cases, but remember our squads main goal of \"zero-infrastructure overhead\" , so we deploy from ArgoCD into the same cluster it is deployed on the majority of the time. The ArgoCD application is a Custom-Resource Definition, comprising of the details necessary to determine the remote code repository URL, the branch of the code to use, the target namespace, and any formatting capabilities that are necessary. ArgoCD then handles automatically (or manually) syncing the deployments in the target namespace with the state that is described in the YAMLs on the specific branch in the GitOps repository. To keep in sync with the continuous integration implementation we have defined in Continuous integration , we have an additional GitHub Actions workflow defined in this repository that will update the YAML files contained in the repository with the latest microservice container images as they are modified, thus enabling a completely automated build-to-deployment lifecycle. Defined in the .github/workflows/update-gitops-deployments-(eda-integration).yaml workflow file, the workflow will scan the repository for all templated use of container images in Kubernetes Deployment files (and recently updated to be extensible to any YAML-based file!), search Docker Hub for the latest version of that container image, update the YAML file in-place, and check in the YAML updates back to the same repository and branch. This process is kicked off by the webhook Jobs mentioned in our CI process, as well as on a regularly-scheduled cron-like timer. Tekton & Appsody deployments We have also implemented some facets of the project deployment workflows using the Tekton Pipelines project and its inherent ease of support of the Appsody open-source developer experience project through the standard integration between the two built into the Kabanero open-source project, or more formally, the IBM Cloud Pak for Applications . Defined in the /scripts/tekton directory, we have a simple pipeline that will utilize the appsody deploy command to deploy the generated AppsodyApplication CRD YAML to the target environment. Similar to our ArgoCD-based deployments of Helm-generated, standard Kubernetes YAMLs, AppsodyApplication YAMLs can also be deployed through ArgoCD in a GitOps manner. However, for demonstration inside this project, additional capabilities are provided to showcase how we can utilize different pieces of the platform to deploy similar applications when different requirements are presented. Similar to ArgoCD, Tekton Pipelines run on the same cluster (and often in the same namespace!) as your running application code, thus allowing for more programmatic control over the deployment, management, operations, and existence of your application components. The key artifact that enables Tekton to deploy our Appsody-based refarch-reefer-ml/simulator microservice is the generated app-deploy.yaml file. The refarch-reefer-ml/simulator/app-deploy.yaml file was generated according to the appsody build command and then annotated with the required environment variables and metadata for successful operation in a given namespace, very similar to the pattern required for generating our Helm-templated YAMLs in the ArgoCD deployments section above. We then make use of the Appsody Operator to apply the AppsodyApplication to the target environment through the appsody deploy --no-build command. As documented in the Appsody Docs , we are able to take advantage of the pre-built container images available on Docker Hub and the annotated app-deploy.yaml file that is now a synonymous GitOps-like deployment artifact to quickly apply the change to the target namespace in the same cluster. Once the appsody deploy command is succesful, the Appsody Operator and Kubernetes takes care of the rest and reconciles the necessary underlying Kubernetes artifacts that are required to fulfill the requirements of serving up the application code in real-time!","title":"Continuous Deployment"},{"location":"devops/cd/#continuous-deployment","text":"Our Continuous Deployment (CD) approach focuses on a GitOps-based deployment model, using Git as a single source of truth for the deployment, management, and operations of our running application components. In this model, we have the flexibility to use multiple open-source to apply the single source of truth from a given Git repository onto a desired cluster environment. More detail around the background of GitOps and how it differs from traditional deployment models can be found in this blog post from WeaveWorks. One of the main tools that we use in this space is a GitOps-focused continuous deployment project named ArgoCD . As documented by the IBM Garage for Cloud team, ArgoCD can monitor GitHub-based projects and apply changes stored in that repository's YAML files to a running Kubernetes-based cluster. We have documented our general ArgoCD Continuous Delivery workflow in the ibm-cloud-architecture/refarch-kc-gitops repository. The details of our ArgoCD-based GitOps deployments are covered in the ArgoCD deployments section below. Another DevOps tool which provides the opportunity to deployment applications via the GitOps methodology is Tekton . The Tekton Pipelines project provides a declarative language for defining and executing both CI and CD-style pipelines, all defined with common Kubernetes-like nomenclature. It even has the capability to kick off pipeline runs based off of GitHub webhooks. The details of our Tekton & Appsody deployments are covered in the Tekton & Appsody deployments section below.","title":"Continuous Deployment"},{"location":"devops/cd/#argocd-deployments","text":"Our main continuous deployment pattern operates on the same principle of \"zero-infrastructure overhead\" as our continuous integration implementations. This allows us to be agile, adaptable, and efficient in what we deploy where. ArgoCD is a perfect companion to this principle, as we do not need additional long-running CD infrastructure to monitor either a source environment or a target deployment environment. Our CI process sits with our code (on the same hosted infrastructure), while our CD process sits with the target deployment environment (on the same Kubernetes-based cluster). To utilize ArgoCD in this manner, we define a set of Kubernetes YAMLs generated from helm template commands, with environment, namespace, and cluster-specific parameters provided as needed. The details of generating those helm template YAMLs can be found in our main Application Components documentation. The templated YAMLs are generated with the names of the required ConfigMaps and Secrets specific to a namespace on the eventual target deployment cluster. This allows us to create a deployment artifact programmatically without exposing confidential and secret credentials via source code. Once these YAMLs are generated, they are checked in to the main GitOps repository for the project, under a new branch with a branch name in the format of <namespace>/<cluster> for ease of identification. These will then have a folder structure of /<component-name>/templates/<artifact-type>.yaml , with most components providing Deployment, Service, and Route artifact YAMLs. An ArgoCD application is then created on the ArgoCD deployment inside the target environment that can read from the GitOps repository. ArgoCD can also deploy between clusters, which does come in handy in certain use cases, but remember our squads main goal of \"zero-infrastructure overhead\" , so we deploy from ArgoCD into the same cluster it is deployed on the majority of the time. The ArgoCD application is a Custom-Resource Definition, comprising of the details necessary to determine the remote code repository URL, the branch of the code to use, the target namespace, and any formatting capabilities that are necessary. ArgoCD then handles automatically (or manually) syncing the deployments in the target namespace with the state that is described in the YAMLs on the specific branch in the GitOps repository. To keep in sync with the continuous integration implementation we have defined in Continuous integration , we have an additional GitHub Actions workflow defined in this repository that will update the YAML files contained in the repository with the latest microservice container images as they are modified, thus enabling a completely automated build-to-deployment lifecycle. Defined in the .github/workflows/update-gitops-deployments-(eda-integration).yaml workflow file, the workflow will scan the repository for all templated use of container images in Kubernetes Deployment files (and recently updated to be extensible to any YAML-based file!), search Docker Hub for the latest version of that container image, update the YAML file in-place, and check in the YAML updates back to the same repository and branch. This process is kicked off by the webhook Jobs mentioned in our CI process, as well as on a regularly-scheduled cron-like timer.","title":"ArgoCD deployments"},{"location":"devops/cd/#tekton-appsody-deployments","text":"We have also implemented some facets of the project deployment workflows using the Tekton Pipelines project and its inherent ease of support of the Appsody open-source developer experience project through the standard integration between the two built into the Kabanero open-source project, or more formally, the IBM Cloud Pak for Applications . Defined in the /scripts/tekton directory, we have a simple pipeline that will utilize the appsody deploy command to deploy the generated AppsodyApplication CRD YAML to the target environment. Similar to our ArgoCD-based deployments of Helm-generated, standard Kubernetes YAMLs, AppsodyApplication YAMLs can also be deployed through ArgoCD in a GitOps manner. However, for demonstration inside this project, additional capabilities are provided to showcase how we can utilize different pieces of the platform to deploy similar applications when different requirements are presented. Similar to ArgoCD, Tekton Pipelines run on the same cluster (and often in the same namespace!) as your running application code, thus allowing for more programmatic control over the deployment, management, operations, and existence of your application components. The key artifact that enables Tekton to deploy our Appsody-based refarch-reefer-ml/simulator microservice is the generated app-deploy.yaml file. The refarch-reefer-ml/simulator/app-deploy.yaml file was generated according to the appsody build command and then annotated with the required environment variables and metadata for successful operation in a given namespace, very similar to the pattern required for generating our Helm-templated YAMLs in the ArgoCD deployments section above. We then make use of the Appsody Operator to apply the AppsodyApplication to the target environment through the appsody deploy --no-build command. As documented in the Appsody Docs , we are able to take advantage of the pre-built container images available on Docker Hub and the annotated app-deploy.yaml file that is now a synonymous GitOps-like deployment artifact to quickly apply the change to the target namespace in the same cluster. Once the appsody deploy command is succesful, the Appsody Operator and Kubernetes takes care of the rest and reconciles the necessary underlying Kubernetes artifacts that are required to fulfill the requirements of serving up the application code in real-time!","title":"Tekton &amp; Appsody deployments"},{"location":"devops/ci/","text":"Continuous Integration Our Continuous Integration (CI) approach is one of \"zero-infrastructure overhead\" . As such, we utilize GitHub Actions to build and push the microservice's associated container image to Docker Hub for public consumption. The GitHub Actions workflows are defined in the owning repository's .github/workflows/dockerbuild.yaml file. The Reefer Simulator microservice's CI implementation can be found via /.github/workflows/dockerbuild.yaml in the refarch-reefer-ml repository, while the SpringContainerMS microservice's CI implementation can be found via .github/workflows/dockerbuild.yaml in the refarch-kc-container-ms repository. For results of individual completed CI workflow actions, you can view the results via the Actions tab of a given repository. Overview of Continuous Integration workflows for this project The continuous integration workflow for our project looks in the figure below, which are detailing in next sections: On the left side, the developer uses his environment with Appsody CLI to build, and test his code, once tests run successfully, he commits and pushes the code to the master branch, then the github workflow triggers... 1 - Validate Docker Secrets The first job in each GitHub Actions workflow, validate-docker-secrets , ensures that all the necessary Secrets are defined on the repository under which the build action is running. Similar to Kubernetes Secrets, GitHub Repository Secrets allow you to store encrypted, sensitive information in a programmatically accessible way. Here is an example of such secret definitions: 2 - Build Component Images Appsody build for the Simulator microservice The simulator microservice is built using the Appsody open-source project, while leveraging the Python Flask Appsody Stack for its underlying framework. The project can be easily built in a local environment by issuing the appsody build commands, further documented on the Appsody site under Building and deploying . The second job in the Simulator microservice workflow, build-simulator-image , runs on a base Ubuntu container image, creates a new semantically-versioned tag (in the form of 0.1.2 ) for the repository, installs the latest Appsody CLI tools, performs the appsody build command with the appropriate parameters , tags the newly created version-specific image with latest as well, and pushes the image with both tags to the public Docker Hub repository defined by the aforementioned repository secrets. Container build for the Spring Container microservice The SpringContainerMS microservice is developed using the Spring Boot framework, compiled via maven and an associated pom.xml file, and packaged via a traditional Dockerfile. This project can be built locally via Maven and/or Docker, however it is recommended to consume the published container images that are a result of this continuous integration. The second job in the SpringContainerMS microservice github action workflow , build-springcontainer-image , runs on a base Ubuntu container image, creates a new semantically-versioned tag (in the form of 0.1.2 ) for the repository, performs a traditional docker build using the SpringContainerMS/Dockerfile , tags the newly created version-specific image with latest as well, and pushes the image with both tags to the public Docker Hub repository defined by the aforementioned repository secrets. 3 - GitOps Updates The final job, gitops-repo-webhook , is a linkage to our general continuous deployment process, which is GitOps-based and available via ibm-cloud-architecture/refarch-kc-gitops . This step performs a webhook call to our GitOps repository and notifies that repository's GitHub Actions that an update to one of its component's container images has been made and it should scan for the latest version of all the known container images and update the associated YAML files for environment updates. The repository action dispatcher triggers the git action workflow as defined here so the corresponding yaml files (appsody and helm configurations) can be updated (See this repository to understand the action update gitops). Here is an example of appsody.yaml automatically modified in the github repository: ... labels : image.opencontainers.org/title : reefer-simulator stack.appsody.dev/version : 0.1.6 name : reefer-simulator spec : applicationImage : ibmcase/kcontainer-reefer-simulator-appsody:0.1.26 Further description of this continuous deployment process is covered in Continuous Deployment note .","title":"Continuous Integration"},{"location":"devops/ci/#continuous-integration","text":"Our Continuous Integration (CI) approach is one of \"zero-infrastructure overhead\" . As such, we utilize GitHub Actions to build and push the microservice's associated container image to Docker Hub for public consumption. The GitHub Actions workflows are defined in the owning repository's .github/workflows/dockerbuild.yaml file. The Reefer Simulator microservice's CI implementation can be found via /.github/workflows/dockerbuild.yaml in the refarch-reefer-ml repository, while the SpringContainerMS microservice's CI implementation can be found via .github/workflows/dockerbuild.yaml in the refarch-kc-container-ms repository. For results of individual completed CI workflow actions, you can view the results via the Actions tab of a given repository.","title":"Continuous Integration"},{"location":"devops/ci/#overview-of-continuous-integration-workflows-for-this-project","text":"The continuous integration workflow for our project looks in the figure below, which are detailing in next sections: On the left side, the developer uses his environment with Appsody CLI to build, and test his code, once tests run successfully, he commits and pushes the code to the master branch, then the github workflow triggers...","title":"Overview of Continuous Integration workflows for this project"},{"location":"devops/ci/#1-validate-docker-secrets","text":"The first job in each GitHub Actions workflow, validate-docker-secrets , ensures that all the necessary Secrets are defined on the repository under which the build action is running. Similar to Kubernetes Secrets, GitHub Repository Secrets allow you to store encrypted, sensitive information in a programmatically accessible way. Here is an example of such secret definitions:","title":"1 - Validate Docker Secrets"},{"location":"devops/ci/#2-build-component-images","text":"","title":"2 - Build Component Images"},{"location":"devops/ci/#appsody-build-for-the-simulator-microservice","text":"The simulator microservice is built using the Appsody open-source project, while leveraging the Python Flask Appsody Stack for its underlying framework. The project can be easily built in a local environment by issuing the appsody build commands, further documented on the Appsody site under Building and deploying . The second job in the Simulator microservice workflow, build-simulator-image , runs on a base Ubuntu container image, creates a new semantically-versioned tag (in the form of 0.1.2 ) for the repository, installs the latest Appsody CLI tools, performs the appsody build command with the appropriate parameters , tags the newly created version-specific image with latest as well, and pushes the image with both tags to the public Docker Hub repository defined by the aforementioned repository secrets.","title":"Appsody build for the Simulator microservice"},{"location":"devops/ci/#container-build-for-the-spring-container-microservice","text":"The SpringContainerMS microservice is developed using the Spring Boot framework, compiled via maven and an associated pom.xml file, and packaged via a traditional Dockerfile. This project can be built locally via Maven and/or Docker, however it is recommended to consume the published container images that are a result of this continuous integration. The second job in the SpringContainerMS microservice github action workflow , build-springcontainer-image , runs on a base Ubuntu container image, creates a new semantically-versioned tag (in the form of 0.1.2 ) for the repository, performs a traditional docker build using the SpringContainerMS/Dockerfile , tags the newly created version-specific image with latest as well, and pushes the image with both tags to the public Docker Hub repository defined by the aforementioned repository secrets.","title":"Container build for the Spring Container microservice"},{"location":"devops/ci/#3-gitops-updates","text":"The final job, gitops-repo-webhook , is a linkage to our general continuous deployment process, which is GitOps-based and available via ibm-cloud-architecture/refarch-kc-gitops . This step performs a webhook call to our GitOps repository and notifies that repository's GitHub Actions that an update to one of its component's container images has been made and it should scan for the latest version of all the known container images and update the associated YAML files for environment updates. The repository action dispatcher triggers the git action workflow as defined here so the corresponding yaml files (appsody and helm configurations) can be updated (See this repository to understand the action update gitops). Here is an example of appsody.yaml automatically modified in the github repository: ... labels : image.opencontainers.org/title : reefer-simulator stack.appsody.dev/version : 0.1.6 name : reefer-simulator spec : applicationImage : ibmcase/kcontainer-reefer-simulator-appsody:0.1.26 Further description of this continuous deployment process is covered in Continuous Deployment note .","title":"3 - GitOps Updates"},{"location":"environments/cp4d/","text":"Cloud Pak For Data This repository does not address how to install and configure Cloud Pak for Data. The following source of information can be used to get a cluster up and running: Installing Cloud Pak for Data on Red Hat OpenShift Cloud pak playbook","title":"Cloud Pak for Data"},{"location":"environments/cp4d/#cloud-pak-for-data","text":"This repository does not address how to install and configure Cloud Pak for Data. The following source of information can be used to get a cluster up and running: Installing Cloud Pak for Data on Red Hat OpenShift Cloud pak playbook","title":"Cloud Pak For Data"},{"location":"environments/event-streams/","text":"Event Streams provisioning and configuration We cover a lot about Kafka or IBM Event Streams installation and configuration in the EDA reference architecture repository . In this short note we just highlight the steps to be done for deploying Event Streams on premise using Cloud Pak for integration. Deploying Event Streams from Cloud Pak for Integration The cloud pack for integration includes IBM Event Streams, the Kafka solution for on premise deployment. Once you have an Openshift cluster, you can install cloud pak for integration as presented in this tutorial . Then you can deploy Event streams with the default configuration of three broker cluster from the CP4I home page: For your own deployment you can follow the steps described in this tutorial and the Event Streams product documentation . Once you have your instance up and running, you need to get the URL for the brokers, the API key to access topics and the TLS certificate. Define the API key: The copy the broker URL and api key in the scripts/setenv.sh file under the OCP choice: OCP) export KAFKA_BROKERS=eventstream140-ibm-es-proxy-route-bootstrap-eventstreams.apps.green.ocp.csplab.local:443 export KAFKA_APIKEY=\"zb5Rv-81m11A0_\" export KAFKA_CERT=\"/project/useapp/simulator/certs/ocp/es-cert.pem\" And then download the pem and java key. We keep those files in the certs/ocp folder. As an alternate you can use Event Streams on Public Cloud. Event Streams on IBM Cloud Public We recommend creating the Event Stream service using the IBM Cloud catalog , you can also read our quick article on how to deploy Event Streams. With IBM Cloud deployment use the service credentials to create new credentials to get the Kafka brokers list, the admin URL and the api key needed to authenticate the consumers and the producers. For Event Streams on Openshift deployment, click to the connect to the cluster button to get the broker URL and to generate the API key: select the option to generate the key for all topics. Defines topics Create Kafka topics through Kubernetes Job automation In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. Following the configuration prerequisistes defined in the Backing Services documentation for using Kafka via IBM Event Streams on IBM Cloud or IBM Event Streams on OpenShift , you should already have the following Kubernetes ConfigMap & Secrets defined in your target namespace with the information available from the Connect to this service tab on the respective Event Streams service console: ConfigMap: kafka-brokers (in a comma-separated list) kubectl create configmap kafka-brokers --from-literal = brokers = 'host1.appdomain.cloud.com,host2.appdomain.cloud.com,...' Secret: eventstreams-apikey kubectl create secret generic eventstreams-apikey --from-literal = binding = '1a2...9z0' Secret: eventstreams-truststore-jks (this is only required when connecting to IBM Event Streams on OpenShift) kubectl create secret generic eventstreams-truststore-jks --from-file = ~/Downloads/es-cert.jks Event Streams Truststore password - this value is not contained in a Kubernetes Secret, but if using non-default settings in the Event Streams deployment, it should be verified that the password for the generated truststore file is still the default value of password . Review the /scripts/createKafkaTopics.yaml and the fields contained in the env section for optional parameters that can be modified when running the Job for non-default tasks. Create the create-kafka-topics Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createKafkaTopics.yaml You can tail the created pod's output to see the progress of the Kafka topic creation: kubectl logs -f --selector = job-name = create-kafka-topics Create Kafka topics manually through offering UI The following diagram illustrates the needed Kafka topics configured in IBM Cloud Event Stream service: For the telemetries we are now using 3 replicas. This is an example of configuration for Event Streams on openshift on premise:","title":"Event Streams environment"},{"location":"environments/event-streams/#event-streams-provisioning-and-configuration","text":"We cover a lot about Kafka or IBM Event Streams installation and configuration in the EDA reference architecture repository . In this short note we just highlight the steps to be done for deploying Event Streams on premise using Cloud Pak for integration.","title":"Event Streams provisioning and configuration"},{"location":"environments/event-streams/#deploying-event-streams-from-cloud-pak-for-integration","text":"The cloud pack for integration includes IBM Event Streams, the Kafka solution for on premise deployment. Once you have an Openshift cluster, you can install cloud pak for integration as presented in this tutorial . Then you can deploy Event streams with the default configuration of three broker cluster from the CP4I home page: For your own deployment you can follow the steps described in this tutorial and the Event Streams product documentation . Once you have your instance up and running, you need to get the URL for the brokers, the API key to access topics and the TLS certificate. Define the API key: The copy the broker URL and api key in the scripts/setenv.sh file under the OCP choice: OCP) export KAFKA_BROKERS=eventstream140-ibm-es-proxy-route-bootstrap-eventstreams.apps.green.ocp.csplab.local:443 export KAFKA_APIKEY=\"zb5Rv-81m11A0_\" export KAFKA_CERT=\"/project/useapp/simulator/certs/ocp/es-cert.pem\" And then download the pem and java key. We keep those files in the certs/ocp folder. As an alternate you can use Event Streams on Public Cloud.","title":"Deploying Event Streams from Cloud Pak for Integration"},{"location":"environments/event-streams/#event-streams-on-ibm-cloud-public","text":"We recommend creating the Event Stream service using the IBM Cloud catalog , you can also read our quick article on how to deploy Event Streams. With IBM Cloud deployment use the service credentials to create new credentials to get the Kafka brokers list, the admin URL and the api key needed to authenticate the consumers and the producers. For Event Streams on Openshift deployment, click to the connect to the cluster button to get the broker URL and to generate the API key: select the option to generate the key for all topics.","title":"Event Streams on IBM Cloud Public"},{"location":"environments/event-streams/#defines-topics","text":"","title":"Defines topics"},{"location":"environments/event-streams/#create-kafka-topics-through-kubernetes-job-automation","text":"In an effort to keep development systems as clean as possible and speed up deployment of various scenarios, our deployment tasks have been encapsulated in Kubernetes Jobs . These are runnable on any Kubernetes platform, including OpenShift. Following the configuration prerequisistes defined in the Backing Services documentation for using Kafka via IBM Event Streams on IBM Cloud or IBM Event Streams on OpenShift , you should already have the following Kubernetes ConfigMap & Secrets defined in your target namespace with the information available from the Connect to this service tab on the respective Event Streams service console: ConfigMap: kafka-brokers (in a comma-separated list) kubectl create configmap kafka-brokers --from-literal = brokers = 'host1.appdomain.cloud.com,host2.appdomain.cloud.com,...' Secret: eventstreams-apikey kubectl create secret generic eventstreams-apikey --from-literal = binding = '1a2...9z0' Secret: eventstreams-truststore-jks (this is only required when connecting to IBM Event Streams on OpenShift) kubectl create secret generic eventstreams-truststore-jks --from-file = ~/Downloads/es-cert.jks Event Streams Truststore password - this value is not contained in a Kubernetes Secret, but if using non-default settings in the Event Streams deployment, it should be verified that the password for the generated truststore file is still the default value of password . Review the /scripts/createKafkaTopics.yaml and the fields contained in the env section for optional parameters that can be modified when running the Job for non-default tasks. Create the create-kafka-topics Job from the root of the refarch-reefer-ml repository: kubectl apply -f scripts/createKafkaTopics.yaml You can tail the created pod's output to see the progress of the Kafka topic creation: kubectl logs -f --selector = job-name = create-kafka-topics","title":"Create Kafka topics through Kubernetes Job automation"},{"location":"environments/event-streams/#create-kafka-topics-manually-through-offering-ui","text":"The following diagram illustrates the needed Kafka topics configured in IBM Cloud Event Stream service: For the telemetries we are now using 3 replicas. This is an example of configuration for Event Streams on openshift on premise:","title":"Create Kafka topics manually through offering UI"},{"location":"environments/mongo/","text":"Preparing MongoDB on IBM Cloud Create the MongoDB service on IBM cloud using default configuration Figure 1: IBM Cloud Database Figure 2: Mongo DB default configuration Once created the Mongo instance can be found under the services in your resource list: Figure 3: Mongo DB service in resource list Add a Service credentials to get the mongodb.composed url: (something starting as mongodb://ibm_cloud_e154ff52_ed) the username and password. Figure 4: Mongo DB credentials Get the TLS certificate as pem file: ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem If you never used Mongo Compass tool to access a Mongo DB, you can get started with this note.","title":"MongoDB on IBM Cloud"},{"location":"environments/mongo/#preparing-mongodb-on-ibm-cloud","text":"Create the MongoDB service on IBM cloud using default configuration Figure 1: IBM Cloud Database Figure 2: Mongo DB default configuration Once created the Mongo instance can be found under the services in your resource list: Figure 3: Mongo DB service in resource list Add a Service credentials to get the mongodb.composed url: (something starting as mongodb://ibm_cloud_e154ff52_ed) the username and password. Figure 4: Mongo DB credentials Get the TLS certificate as pem file: ibmcloud cdb deployment-cacert gse-eda-mongodb > certs/mongodbca.pem If you never used Mongo Compass tool to access a Mongo DB, you can get started with this note.","title":"Preparing MongoDB on IBM Cloud"},{"location":"environments/mongodb-compass/","text":"Use MongoDB Compass user interface First download from MongoDB download center Once started define a new connection to the IBM Cloud mongodb instance, by using the detail view so we can access the TLS settings. Once the connection is valid you can access the collection and browse or edit the data.","title":"Use MongoDB Compass user interface"},{"location":"environments/mongodb-compass/#use-mongodb-compass-user-interface","text":"First download from MongoDB download center Once started define a new connection to the IBM Cloud mongodb instance, by using the detail view so we can access the TLS settings. Once the connection is valid you can access the collection and browse or edit the data.","title":"Use MongoDB Compass user interface"},{"location":"environments/postgresql/","text":"PostgreSQL on IBM Cloud We are using Postgresql as a data source to persist Product and Container information. The Produts table is used in the predictive model construction. So we need to provision a Postgresql service in IBM Cloud. Use the product documentation to provision your own service. Here is a figure of the databases services in IBM Cloud as of December 2019. Using the default configuration the service is created in a minutes. Once done, we need to define the service credentials for the host, user, password and the certificate to be used by client appliations. Something like that: \"host=bd2d0216-0b7d-4575-8c0b-d2e934843e41.6131b73286f34215871dfad7254b4f7d.databases.appdomain.cloud port=31384 dbname=ibmclouddb user=ibm_cloud_c958... \"PGPASSWORD\": \"2d1c526.....3\" From these informations we need to define the POSTGRES environment variables in the scripts/setenv.sh file. (if not done before rename the scripts/setenv-tmpl.sh to scripts/setenv.sh ) POSTGRES_URL, POSTGRES_DBNAME, You also need to get the SSL certificate as a postgres.pem file, using the following ibmcloud CLI commands: ibmcloud login ibmcloud cdb deployment-cacert <database deployment name> Then in setenv.sh set POSTGRES_SSL_PEM variable to the path where to find this file ( export POSTGRES_SSL_PEM=\"./certs/postgres.pem\" ). Connect with psql To validate the access, we use a public postgres docker image to run psql using the defined environment variables. We have a script to start it: $ cd scripts $ ./startPsql.sh IBMCLOUD root@452074de376a:/# PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = > List relations... ibmclouddb => \\d Next... You can use our tool to add some product definitions, for that see this note...","title":"Postgresql on IBM Cloud"},{"location":"environments/postgresql/#postgresql-on-ibm-cloud","text":"We are using Postgresql as a data source to persist Product and Container information. The Produts table is used in the predictive model construction. So we need to provision a Postgresql service in IBM Cloud. Use the product documentation to provision your own service. Here is a figure of the databases services in IBM Cloud as of December 2019. Using the default configuration the service is created in a minutes. Once done, we need to define the service credentials for the host, user, password and the certificate to be used by client appliations. Something like that: \"host=bd2d0216-0b7d-4575-8c0b-d2e934843e41.6131b73286f34215871dfad7254b4f7d.databases.appdomain.cloud port=31384 dbname=ibmclouddb user=ibm_cloud_c958... \"PGPASSWORD\": \"2d1c526.....3\" From these informations we need to define the POSTGRES environment variables in the scripts/setenv.sh file. (if not done before rename the scripts/setenv-tmpl.sh to scripts/setenv.sh ) POSTGRES_URL, POSTGRES_DBNAME, You also need to get the SSL certificate as a postgres.pem file, using the following ibmcloud CLI commands: ibmcloud login ibmcloud cdb deployment-cacert <database deployment name> Then in setenv.sh set POSTGRES_SSL_PEM variable to the path where to find this file ( export POSTGRES_SSL_PEM=\"./certs/postgres.pem\" ).","title":"PostgreSQL on IBM Cloud"},{"location":"environments/postgresql/#connect-with-psql","text":"To validate the access, we use a public postgres docker image to run psql using the defined environment variables. We have a script to start it: $ cd scripts $ ./startPsql.sh IBMCLOUD root@452074de376a:/# PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = >","title":"Connect with psql"},{"location":"environments/postgresql/#list-relations","text":"ibmclouddb => \\d","title":"List relations..."},{"location":"environments/postgresql/#next","text":"You can use our tool to add some product definitions, for that see this note...","title":"Next..."},{"location":"infuse/build-run/","text":"An alternate approach is to setup a CI/CD pipeline We have adopted the Git Action to manage the continuous integration , and ArgoCD for the continuous deployment. The build process will build the following images: [https://hub.docker.com/repository/docker/ibmcase/kcontainer-reefer-simulator] Helm charts are added for the simulator and the scoring agent, using helm create command, and then the values.yaml and deployment.yaml files were updated to set environment variables and other parameters. Test sending a simulation control to the POST api The script sendSimulControl.sh is used for that. The usage looks like: sendSimulControl.sh hostname simultype (co2sensor | o2sensor | poweroff) containerID nb_of_records pwd refarch-reefer-ml ./scripts/sendSimulControl.sh reefersimulatorroute-reefershipmentsolution.apps.green-with-envy.ocp.csplab.local co2sensor C01 2000 If you use no argument for this script, it will send co2sensor control to the service running on our openshift cluster on IBM Cloud. Looking at the logs from the pod using oc logs reefersimulator-3-jdh2v you can see something like: \"POST /order HTTP/1.1\" 404 232 \"-\" \"curl/7.54.0\" {'containerID': 'c100', 'simulation': 'co2sensor', 'nb_of_records': 10, 'good_temperature': 4.4} Generating 10 Co2 metrics We will see how those events are processed in the next section. The predictive scoring agent Applying the same pattern as the simulation webapp, we implement a kafka consumer and producer in python that calls the serialized analytical model. The code in the scoring\\eventConsumer folder. Applying a TDD approach we start by a TestScoring.py class. import unittest from domain.predictservice import PredictService class TestScoreMetric ( unittest . TestCase ): def testCreation ( self ): serv = PredictService if __name__ == '__main__' : unittest . main () Use the same python environment with docker: . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / scoring / eventConsumer root @1 de81b16f940 : / # cd / home / scoring / eventConsumer root @1 de81b16f940 : / home / scoring / eventConsumer # python tests / TestScoring . py Test fails, so let add the scoring service with a constructor, and load the serialized pickle model (which was copied from the ml folder). import pickle class PredictService : def __init__ ( self , filename = \"domain/model_logistic_regression.pkl\" ): self . model = pickle . load ( open ( filename , \"rb\" ), encoding = 'latin1' ) def predict ( self , metricEvent ): TESTDATA = StringIO ( metricEvent ) data = pd . read_csv ( TESTDATA , sep = \",\" ) data . columns = data . columns . to_series () . apply ( lambda x : x . strip ()) X = data [ X = data [ FEATURES_NAMES ]] return self . model . predict ( X ) Next we need to test a predict on an event formated as a csv string. The test looks like: serv = PredictService() header=\"\"\"Timestamp, ID, Temperature(celsius), Target_Temperature(celsius), Power, PowerConsumption, ContentType, O2, CO2, Time_Door_Open, Maintenance_Required, Defrost_Cycle\"\"\" event=\"2019-04-01 T16:29 Z,1813, 101, 4.291843460900875,4.4,0,10.273342381017777,3,4334.920958996634,4.9631508046318755,1,0,6\"\"\" record=header+\"\\n\"+event print(serv.predict(record)) So the scoring works, now we need to code the scoring application that will be deployed to Openshift cluster, and which acts as a consumer of container metrics events and a producer container events. The Scoring Agent code of this app is ScoringAgent.py module. It starts a consumer to get messages from Kafka. And when a message is received, it needs to do some data extraction and transformation and then use the predictive service. During the tests we have issue in the data quality, so it is always a good practice to add a validation function to assess if all the records are good. For production, this code needs to be enhanced for better error handling an reporting. Run locally Under scoring\\eventConsumer folder, set the environment variables for KAFKA using the commands below: (It uses event streams on IBM Cloud) export KAFKA_BROKERS=broker-3.eventstreams.cloud.ibm.com:9093,broker-1.eventstreams.cloud.ibm.com:9093,broker-0.eventstreams.cloud.ibm.com:9093,broker-5.eventstreams.cloud.ibm.com:9093,broker-2.eventstreams.cloud.ibm.com:9093,broker-4.eventstreams.cloud.ibm.com:9093 export KAFKA_APIKEY=\"set-api-key-for-eventstreams-on-cloud\" docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -e KAFKA_APIKEY=$KAFKA_APIKEY -v $(pwd)/..:/home -ti ibmcase/python bash -c \"cd /home/scoring && export PYTHONPATH=/home && python ScoringAgent.py\" Scoring: Build and run on Openshift The first time we need to add the application to the existing project, run the following command: oc new-app python:latest~https://github.com/ibm-cloud-architecture/refarch-reefer-ml.git --context-dir=scoring/eventConsumer --name reeferpredictivescoring This command will run a source to image, build all the needed yaml files for the kubernetes deployment and start the application in a pod. It use the --context flag to define what to build and run. With this capability we can use the same github repository for different sub component. As done for simulator, the scoring service needs environment variables. We can set them using the commands oc set env dc/reeferpredictivescoring KAFKA_BROKERS=$KAFKA_BROKERS oc set env dc/reeferpredictivescoring KAFKA_APIKEY=$KAFKA_APIKEY oc set env dc/reeferpredictivescoring KAFKA_CERT=/opt/app-root/src/es-cert.pem but we have added a script for you to do so. This script needs only to be run at the first deployment. It leverage the common setenv scripts: ../scripts/defEnvVarInOpenShift.sh The list of running pods should show the build pods for this application: oc get pods reeferpredictivescoring-1-build 1/1 Running 0 24s To run the build again after commit code to github: oc start-build reeferpredictivescoring # or from local file system oc start-build reeferpredictivescoring --from-file=. To see the log: oc logs reeferpredictivescoring-2-rxr6j To be able to run on Openshift, the APP_FILE environment variable has to be set to ScoringApp.py. This can be done in the environment file under the .s2i folder. The scoring service has no API exposed to the external world, so we do not need to create a Route or ingress. See the integration test section to see a demonstration of the solution end to end. Build docker images For the scoring agent: # scoring folder Run kafka on your laptop For development purpose, you can also run kafka, zookeeper and postgresql and the solution on your laptop. For that read this readme for details.","title":"Build and run the solution on Openshift"},{"location":"infuse/build-run/#an-alternate-approach-is-to-setup-a-cicd-pipeline","text":"We have adopted the Git Action to manage the continuous integration , and ArgoCD for the continuous deployment. The build process will build the following images: [https://hub.docker.com/repository/docker/ibmcase/kcontainer-reefer-simulator] Helm charts are added for the simulator and the scoring agent, using helm create command, and then the values.yaml and deployment.yaml files were updated to set environment variables and other parameters.","title":"An alternate approach is to setup a CI/CD pipeline"},{"location":"infuse/build-run/#test-sending-a-simulation-control-to-the-post-api","text":"The script sendSimulControl.sh is used for that. The usage looks like: sendSimulControl.sh hostname simultype (co2sensor | o2sensor | poweroff) containerID nb_of_records pwd refarch-reefer-ml ./scripts/sendSimulControl.sh reefersimulatorroute-reefershipmentsolution.apps.green-with-envy.ocp.csplab.local co2sensor C01 2000 If you use no argument for this script, it will send co2sensor control to the service running on our openshift cluster on IBM Cloud. Looking at the logs from the pod using oc logs reefersimulator-3-jdh2v you can see something like: \"POST /order HTTP/1.1\" 404 232 \"-\" \"curl/7.54.0\" {'containerID': 'c100', 'simulation': 'co2sensor', 'nb_of_records': 10, 'good_temperature': 4.4} Generating 10 Co2 metrics We will see how those events are processed in the next section.","title":"Test sending a simulation control to the POST api"},{"location":"infuse/build-run/#the-predictive-scoring-agent","text":"Applying the same pattern as the simulation webapp, we implement a kafka consumer and producer in python that calls the serialized analytical model. The code in the scoring\\eventConsumer folder. Applying a TDD approach we start by a TestScoring.py class. import unittest from domain.predictservice import PredictService class TestScoreMetric ( unittest . TestCase ): def testCreation ( self ): serv = PredictService if __name__ == '__main__' : unittest . main () Use the same python environment with docker: . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / scoring / eventConsumer root @1 de81b16f940 : / # cd / home / scoring / eventConsumer root @1 de81b16f940 : / home / scoring / eventConsumer # python tests / TestScoring . py Test fails, so let add the scoring service with a constructor, and load the serialized pickle model (which was copied from the ml folder). import pickle class PredictService : def __init__ ( self , filename = \"domain/model_logistic_regression.pkl\" ): self . model = pickle . load ( open ( filename , \"rb\" ), encoding = 'latin1' ) def predict ( self , metricEvent ): TESTDATA = StringIO ( metricEvent ) data = pd . read_csv ( TESTDATA , sep = \",\" ) data . columns = data . columns . to_series () . apply ( lambda x : x . strip ()) X = data [ X = data [ FEATURES_NAMES ]] return self . model . predict ( X ) Next we need to test a predict on an event formated as a csv string. The test looks like: serv = PredictService() header=\"\"\"Timestamp, ID, Temperature(celsius), Target_Temperature(celsius), Power, PowerConsumption, ContentType, O2, CO2, Time_Door_Open, Maintenance_Required, Defrost_Cycle\"\"\" event=\"2019-04-01 T16:29 Z,1813, 101, 4.291843460900875,4.4,0,10.273342381017777,3,4334.920958996634,4.9631508046318755,1,0,6\"\"\" record=header+\"\\n\"+event print(serv.predict(record)) So the scoring works, now we need to code the scoring application that will be deployed to Openshift cluster, and which acts as a consumer of container metrics events and a producer container events. The Scoring Agent code of this app is ScoringAgent.py module. It starts a consumer to get messages from Kafka. And when a message is received, it needs to do some data extraction and transformation and then use the predictive service. During the tests we have issue in the data quality, so it is always a good practice to add a validation function to assess if all the records are good. For production, this code needs to be enhanced for better error handling an reporting.","title":"The predictive scoring agent"},{"location":"infuse/build-run/#run-locally","text":"Under scoring\\eventConsumer folder, set the environment variables for KAFKA using the commands below: (It uses event streams on IBM Cloud) export KAFKA_BROKERS=broker-3.eventstreams.cloud.ibm.com:9093,broker-1.eventstreams.cloud.ibm.com:9093,broker-0.eventstreams.cloud.ibm.com:9093,broker-5.eventstreams.cloud.ibm.com:9093,broker-2.eventstreams.cloud.ibm.com:9093,broker-4.eventstreams.cloud.ibm.com:9093 export KAFKA_APIKEY=\"set-api-key-for-eventstreams-on-cloud\" docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -e KAFKA_APIKEY=$KAFKA_APIKEY -v $(pwd)/..:/home -ti ibmcase/python bash -c \"cd /home/scoring && export PYTHONPATH=/home && python ScoringAgent.py\"","title":"Run locally"},{"location":"infuse/build-run/#scoring-build-and-run-on-openshift","text":"The first time we need to add the application to the existing project, run the following command: oc new-app python:latest~https://github.com/ibm-cloud-architecture/refarch-reefer-ml.git --context-dir=scoring/eventConsumer --name reeferpredictivescoring This command will run a source to image, build all the needed yaml files for the kubernetes deployment and start the application in a pod. It use the --context flag to define what to build and run. With this capability we can use the same github repository for different sub component. As done for simulator, the scoring service needs environment variables. We can set them using the commands oc set env dc/reeferpredictivescoring KAFKA_BROKERS=$KAFKA_BROKERS oc set env dc/reeferpredictivescoring KAFKA_APIKEY=$KAFKA_APIKEY oc set env dc/reeferpredictivescoring KAFKA_CERT=/opt/app-root/src/es-cert.pem but we have added a script for you to do so. This script needs only to be run at the first deployment. It leverage the common setenv scripts: ../scripts/defEnvVarInOpenShift.sh The list of running pods should show the build pods for this application: oc get pods reeferpredictivescoring-1-build 1/1 Running 0 24s To run the build again after commit code to github: oc start-build reeferpredictivescoring # or from local file system oc start-build reeferpredictivescoring --from-file=. To see the log: oc logs reeferpredictivescoring-2-rxr6j To be able to run on Openshift, the APP_FILE environment variable has to be set to ScoringApp.py. This can be done in the environment file under the .s2i folder. The scoring service has no API exposed to the external world, so we do not need to create a Route or ingress. See the integration test section to see a demonstration of the solution end to end.","title":"Scoring: Build and run on Openshift"},{"location":"infuse/build-run/#build-docker-images","text":"For the scoring agent: # scoring folder","title":"Build docker images"},{"location":"infuse/build-run/#run-kafka-on-your-laptop","text":"For development purpose, you can also run kafka, zookeeper and postgresql and the solution on your laptop. For that read this readme for details.","title":"Run kafka on your laptop"},{"location":"infuse/dev-scoring/","text":"Develop the scoring agent with Cloud Pak for Application We have two approaches to build the predictive scoring agent: A python Flask app listening to telemetry events coming from Kafka, run the predictive scoring within the same app. The model was developed with Jupiter notebook and serialized with pickle , so it responds in microseconds and generates anomaly to a second kafka topic. The code is under scoring folder, and uses essentially open source components. See this article to understand how it is built. A Java app built on MicroProfile 3.0 with the reactive messaging annotations, to consume telemetry events and call a remote predictive scoring service, developped and deployed within Cloud Pak for Data. The code is under scoping-mp folder. The scoring service needs to use an analytics scoring model built using machine learning techniques, and serialized so that it can be loaded in memory. Java Micro Profile 3.0 with Cloud Pak for Application As the scoring-mp component is consuming message from Kafka and produce messages, we want to use the Reactive messaging extension to microprofile. We are reusing the approach presented in this repository based on SmallRye Reactive Messaging . Deploying the model using Watson Machine Learning TODO Cloud Pak model deployment Further Readings Appsody for cloud native development Appsody microprofile stack Cloud Pak for Application demo video Use Codewind for VScode","title":"Develop the scoring app with event messaging and microprofile"},{"location":"infuse/dev-scoring/#develop-the-scoring-agent-with-cloud-pak-for-application","text":"We have two approaches to build the predictive scoring agent: A python Flask app listening to telemetry events coming from Kafka, run the predictive scoring within the same app. The model was developed with Jupiter notebook and serialized with pickle , so it responds in microseconds and generates anomaly to a second kafka topic. The code is under scoring folder, and uses essentially open source components. See this article to understand how it is built. A Java app built on MicroProfile 3.0 with the reactive messaging annotations, to consume telemetry events and call a remote predictive scoring service, developped and deployed within Cloud Pak for Data. The code is under scoping-mp folder. The scoring service needs to use an analytics scoring model built using machine learning techniques, and serialized so that it can be loaded in memory.","title":"Develop the scoring agent with Cloud Pak for Application"},{"location":"infuse/dev-scoring/#java-micro-profile-30-with-cloud-pak-for-application","text":"As the scoring-mp component is consuming message from Kafka and produce messages, we want to use the Reactive messaging extension to microprofile. We are reusing the approach presented in this repository based on SmallRye Reactive Messaging .","title":"Java Micro Profile 3.0 with Cloud Pak for Application"},{"location":"infuse/dev-scoring/#deploying-the-model-using-watson-machine-learning","text":"TODO Cloud Pak model deployment","title":"Deploying the model using Watson Machine Learning"},{"location":"infuse/dev-scoring/#further-readings","text":"Appsody for cloud native development Appsody microprofile stack Cloud Pak for Application demo video Use Codewind for VScode","title":"Further Readings"},{"location":"infuse/integration-tests/","text":"Integration tests to proof the solution Recall that the architecture of the deployed components look like in the figure below: So the first component to start is the container consumer which consumes events from the kafka containers topic. This topic is where the microservices will post messages about a Reefer container. It is used by this microservice already: Reefer container manager . Pre-requisites Be sure to have set the environment variables in the ./scripts/setenv.sh to point to your Event Stream or Kafka deployment. You need to start four terminal windows if you run the solution locally on you laptop, and only 2 terminals if you run the solution on our deployed cluster. Note Our deployed cluster in on IBM Cloud Openshift 3.11 cluster. Start Reefer container events consumer In the consumer folder use the command: ./runContainerConsumer.sh This script starts the docker python image, we built earlier and use the ConsumeContainers.py module. Start the predictive scoring service We can run it locally or on kubernetes cluster like Openshift. Under scoring folder, use the command: ./runScoringApp.sh In the beginning of the trace log you should see the bootstrap.servers brokers list, the group.id , and api key as sasl.password . Recalls the scoring is a producer and a consumer. See the build and run on Openshift section for running on kubernetes cluster. Start the simulator web app Under the simulator folder ./runReeferSimulator.sh To build and run it on Openshift review this section . Start a simulation Under the scripts folder ./sendSimulControl.sh Validate integration tests To test your local deployment ./sendSimulControl.sh localhost:8080 poweroff or to test on our cloud based deployed solution ./sendSimulControl.sh The traces will look like these: Simulator trace The trace from the pod demonstrate the configuration and the control message received at the POST operation, and then the event generated. {'bootstrap.servers': 'broker-3-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-1-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-0-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-5-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-2-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-4-\"<hidden-part>.eventstreams.cloud.ibm.com:9093', 'group.id': 'ReeferMetricsSimulator', 'security.protocol': 'sasl_ssl', 'sasl.mechanisms': 'PLAIN', 'ssl.ca.location': '/etc/pki/tls/cert.pem', 'sasl.username': 'token', 'sasl.password': '<hidden-part>'} ... {'containerID': 'c101', 'simulation': 'poweroff', 'nb_of_records': 50, 'good_temperature': 4.4} Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859800, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:50 Z', 'c101', 2.0905792037649547, 4.4, 16.282392569138707, 6.603341673152029, 2, 16, 8.827184272293419, 6.33603138958275, 0, 5)\"} Message delivered to containerMetrics [0] Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859860, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:51 Z', 'c101', 2.0905792037649547, 4.4, 0, -0.04371530981778182, 2, 3, 6.295683442800409, 5.36863196753292, 0, 1)\"} Message delivered to containerMetrics [0] ... Scoring trace Container consumer trace @@@ poll next container from containers partition: [0] at offset 3 with key b'c100': value: {\"timestamp\": 1566854815, \"type\": \"ContainerMaintenance\", \"version\": \"1\", \"containerID\": \"c100\", \"payload\": {\"containerID\": \"c100\", \"type\": \"Reefer\", \"status\": \"MaintenanceNeeded\", \"Reason\": \"Predictive maintenance scoring found a risk of failure\"}}","title":"Run integration tests"},{"location":"infuse/integration-tests/#integration-tests-to-proof-the-solution","text":"Recall that the architecture of the deployed components look like in the figure below: So the first component to start is the container consumer which consumes events from the kafka containers topic. This topic is where the microservices will post messages about a Reefer container. It is used by this microservice already: Reefer container manager .","title":"Integration tests to proof the solution"},{"location":"infuse/integration-tests/#pre-requisites","text":"Be sure to have set the environment variables in the ./scripts/setenv.sh to point to your Event Stream or Kafka deployment. You need to start four terminal windows if you run the solution locally on you laptop, and only 2 terminals if you run the solution on our deployed cluster. Note Our deployed cluster in on IBM Cloud Openshift 3.11 cluster.","title":"Pre-requisites"},{"location":"infuse/integration-tests/#start-reefer-container-events-consumer","text":"In the consumer folder use the command: ./runContainerConsumer.sh This script starts the docker python image, we built earlier and use the ConsumeContainers.py module.","title":"Start Reefer container events consumer"},{"location":"infuse/integration-tests/#start-the-predictive-scoring-service","text":"We can run it locally or on kubernetes cluster like Openshift. Under scoring folder, use the command: ./runScoringApp.sh In the beginning of the trace log you should see the bootstrap.servers brokers list, the group.id , and api key as sasl.password . Recalls the scoring is a producer and a consumer. See the build and run on Openshift section for running on kubernetes cluster.","title":"Start the predictive scoring service"},{"location":"infuse/integration-tests/#start-the-simulator-web-app","text":"Under the simulator folder ./runReeferSimulator.sh To build and run it on Openshift review this section .","title":"Start the simulator web app"},{"location":"infuse/integration-tests/#start-a-simulation","text":"Under the scripts folder ./sendSimulControl.sh","title":"Start a simulation"},{"location":"infuse/integration-tests/#validate-integration-tests","text":"To test your local deployment ./sendSimulControl.sh localhost:8080 poweroff or to test on our cloud based deployed solution ./sendSimulControl.sh The traces will look like these:","title":"Validate integration tests"},{"location":"infuse/integration-tests/#simulator-trace","text":"The trace from the pod demonstrate the configuration and the control message received at the POST operation, and then the event generated. {'bootstrap.servers': 'broker-3-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-1-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-0-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-5-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-2-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-4-\"<hidden-part>.eventstreams.cloud.ibm.com:9093', 'group.id': 'ReeferMetricsSimulator', 'security.protocol': 'sasl_ssl', 'sasl.mechanisms': 'PLAIN', 'ssl.ca.location': '/etc/pki/tls/cert.pem', 'sasl.username': 'token', 'sasl.password': '<hidden-part>'} ... {'containerID': 'c101', 'simulation': 'poweroff', 'nb_of_records': 50, 'good_temperature': 4.4} Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859800, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:50 Z', 'c101', 2.0905792037649547, 4.4, 16.282392569138707, 6.603341673152029, 2, 16, 8.827184272293419, 6.33603138958275, 0, 5)\"} Message delivered to containerMetrics [0] Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859860, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:51 Z', 'c101', 2.0905792037649547, 4.4, 0, -0.04371530981778182, 2, 3, 6.295683442800409, 5.36863196753292, 0, 1)\"} Message delivered to containerMetrics [0] ...","title":"Simulator trace"},{"location":"infuse/integration-tests/#scoring-trace","text":"","title":"Scoring trace"},{"location":"infuse/integration-tests/#container-consumer-trace","text":"@@@ poll next container from containers partition: [0] at offset 3 with key b'c100': value: {\"timestamp\": 1566854815, \"type\": \"ContainerMaintenance\", \"version\": \"1\", \"containerID\": \"c100\", \"payload\": {\"containerID\": \"c100\", \"type\": \"Reefer\", \"status\": \"MaintenanceNeeded\", \"Reason\": \"Predictive maintenance scoring found a risk of failure\"}}","title":"Container consumer trace"},{"location":"infuse/oss-scoring-app/","text":"Scoring app using Python In this article we are presenting how to develop the scoring agent using python and serialized model, developed with Jupiter notebook. Requirements We are using the job stories to define the requirements: when a Reefer container telemetry event arrives, I want the scoring app to compute an anomaly detection predictive score so that it can create a reefer container maintance command event. when a Reefer container telemetry event arrives, I want the scoring app I want the data to be transform so the scoring can be done using expected structure. when the scoring app is deploy to kubernetes, I want to be sure it is healthy so that the kubernetes scheduler does not kill it Create the project with Appsody For this code we are using the same approach as for the simulator app development The application is built using Appsody as the developer experience tooling. The Appsody CLI is required locally to build and deploy the application properly. Code approach Flask blueprint swagger Integrate test driven development pyttest Coverage Deployent to openshift appsody deploy Code explanation","title":"Develop the scoring app using open source stack"},{"location":"infuse/oss-scoring-app/#scoring-app-using-python","text":"In this article we are presenting how to develop the scoring agent using python and serialized model, developed with Jupiter notebook.","title":"Scoring app using Python"},{"location":"infuse/oss-scoring-app/#requirements","text":"We are using the job stories to define the requirements: when a Reefer container telemetry event arrives, I want the scoring app to compute an anomaly detection predictive score so that it can create a reefer container maintance command event. when a Reefer container telemetry event arrives, I want the scoring app I want the data to be transform so the scoring can be done using expected structure. when the scoring app is deploy to kubernetes, I want to be sure it is healthy so that the kubernetes scheduler does not kill it","title":"Requirements"},{"location":"infuse/oss-scoring-app/#create-the-project-with-appsody","text":"For this code we are using the same approach as for the simulator app development The application is built using Appsody as the developer experience tooling. The Appsody CLI is required locally to build and deploy the application properly.","title":"Create the project with Appsody"},{"location":"infuse/oss-scoring-app/#code-approach","text":"Flask blueprint swagger","title":"Code approach"},{"location":"infuse/oss-scoring-app/#integrate-test-driven-development","text":"pyttest Coverage","title":"Integrate test driven development"},{"location":"infuse/oss-scoring-app/#deployent-to-openshift","text":"appsody deploy","title":"Deployent to openshift"},{"location":"infuse/oss-scoring-app/#code-explanation","text":"","title":"Code explanation"},{"location":"infuse/simul-app/","text":"The Simulator as web app The Simulator webapp is a simple python (3.7) Flask web app exposing a REST POST end point to control the type of simulation to run and to produce Reefer telemetry events to kafka reeferTelemetry topic. What_to_learn In this article we shortly present the design and implementation approaches used for this application, as well as how to use Appsody to jumpstart the implementation, and continously run and debug the application. We are presenting some best practice on TDD with python. Requirements: Job Stories The simulator is not in the critical path for production like component. It is here to help us develop the other components of the solution as we do not have real life Reefer container. To try something different, we are not using user stories to present what the simulator should do but we are using job stories . when I want to generate mockup telemetries data for my data scientist friend, I want to start the simulator tool from command line so I can get a csv file with data when I want to generate mockup telemetries data for my data scientist friend, I want to be able to simulate co2 sensor, o2 sensor and power sensor issue so I can get relevant data for the machine learning model to make sense when I want to generate mockup telemetries data for my data scientist friend, I want to start the simulator tool from command line using parameter so I can get save data to a remote document oriented database: mongodb on IBM cloud. when I want to demonstrate the solution, I want to call a REST api to control the generation of faulty sensor data so I can get the scoring service returning maintenance needed. The simulator needs to integrate with kafka / event stream deployed as service on the cloud or on-premise on openshift. Design approach To support remote control of the simulator while running as webapp, we define a POST operation on the /control URL: with a json control object to define the number records to simulate, the sensor to impact (co2sensor, o2sensor, power) , the container ID, (one of C01, C02, C03, C04) which carries the product referenced by product_id (one of P01, P02, P03, P04, P05): { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } We have tried to support a domain driven design approach to structure the code, with domain, infrastructure and app modules. The domain module has a unique class for the simulator which main goals is to generate tuples or records for the different simulation types. It is reused for the standalone simulation tools to generate data at rest. As the simulator is also a webapp we need to package it with Flask and run it using one of the Web Server Gateway Interface (WSGI) implementation with Gunicorn . We recommend to follow Flask tutorial if you do not know this python library to develop web app or REST service. Flask is a simple library to implement REST based microservice and web application in python. It has other related projects to add interesting features to develop production application. The standard development includes defining routes, function to support handling the request and generating HTTP response, but also defining APIs... Read more with the explore Flask book online . Flask is mono threaded so it fits well in a simple web application for development purpose, but for production it is recommended to add a web server like Gunicorn to handle multiple concurrent requests. Code approach The application is built using Appsody as the developer experience tooling. The Appsody CLI is required locally to build and deploy the application properly. The app is done using Flask, and the code is generated using appsody init python-flask command with the Python Flask appsody stack and template. Appsody helps developer to do not worry about the details of k8s deployment and build. During a Appsody run, debug or test step (2), Appsody creates a Docker container based on the parent stack Dockerfile, and combines application code with the source code in the template. We recommend reading the Python Flask Appsody Stack git hub repo to get familiar with appsody python stack. This stack is defining the Flask application, and import the 'userapp' where the application code resides, then use blueprints to define health and metrics APIs: from flask import Flask app = Flask ( __name__ ) from userapp import * from server.routes.health import health_bp app . register_blueprint ( health_bp ) from server.routes.prometheus import metrics_bp app . register_blueprint ( metrics_bp ) This code is not updatable as it is part of the image. But we can add our business logic as part of the simulator/__init__.py code using another Flask blueprints module api/controller.py . The userapp module is defined when appsody integrates our code with the stack base image using Docker. Below is an extract of the docker file managing module installation and defining what appsody does during build, run and test: ENV APPSODY_MOUNTS=/:/project/userapp ENV APPSODY_DEPS=/project/deps WORKDIR /project RUN python -m pip install -r requirements.txt -t /project/deps ENV FLASK_APP=server/__init__.py Looking at the content of the final docker container running the application we can see this structure: /project |-- Dockerfile Pipfile Pipfile.lock constraints.txt requirements.txt deps/ server/ test/ userapp/ The basic concept of blueprints is that they record operations to execute when registered on an application. So to add the operation to support the control we add a blueprint, and then register it in the main application: __init__py . from userapp.api.controller import control_blueprint app . register_blueprint ( control_blueprint ) To define the API, we use Flasgger as an extension to Flask to extract Open API specification from the code. It comes with Swagger UI, so we can see the API documentation of the microservice at the URL /apidocs . It can also validate the data according to the schema defined. For the POST /control we define the using Swagger 2.0 the API in a separate file: api/controlapi.yml and import it at the method level to support the POSt operation. This method is defined in its blueprint as a REST resource. The code controller.py is under api folder. Below is a code extract to illustrate the use of Flask-RESTful and blueprint and the swagger annotation: from flasgger import swag_from from flask_restful import Resource , Api control_blueprint = Blueprint ( \"control\" , __name__ ) api = Api ( control_blueprint ) class SimulationController ( Resource ): @swag_from ( 'controlapi.yml' ) def post ( self ): # .. api . add_resource ( SimulationController , \"/control\" ) The Pipfile defines the dependencies for this component, and is used by pipenv during the automatic build process within appsody build . To launch the web application in development mode, using an IBM Event Streams remote use the following commands: # set environment variables - from simulator folder $ source ../scripts/setenv.sh OCP # Start appsody with the environment variables: in simulator folder $ appsody run --docker-options = \"-e KAFKA_BROKERS= $KAFKA_BROKERS -e KAFKA_APIKEY= $KAFKA_APIKEY -e KAFKA_CERT= $KAFKA_CERT -e TELEMETRY_TOPIC= $TELEMETRY_TOPIC -e CONTAINER_TOPIC= $CONTAINER_TOPIC \" The trace shows the Kafka configuration options: Kafka options are: [Container] {'bootstrap.servers': 'eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443', 'group.id': 'ReeferTelemetryProducers', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'xCByo4478xQH...EVdcbGNCtLLuItKgVDc', 'ssl.ca.location': '/project/userapp/certs/ocp/es-cert.pem'} Testing Unit test the Simulator The test coverage is not yet great. To run the test use appsody test . cd simulator . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / simulator root @1 de81b16f940 : / # cd / home / simulator root @1 de81b16f940 : / # python tests / unit / TestSimulator . py Functional testing Prepare for kubernetes deployment There are three required configuration elements for connectivity to IBM Event Streams (Kafka) prior to deployment as we already presented in the following notes. A ConfigMap named kafka-brokers Reference Link A Secret named eventstreams-api-key Reference Link A Secret named eventstreams-cert-pem (if connecting to an on-premise version of IBM Event Streams) Reference Link Once those elements are defined it is important to configure the app so it can retrieve those information via environment variables. With Appsody the file appsody-config.yaml is supporting these configurations. See the lines 24 to 40 to get the settings to map environment variables to Config Maps or Secrets. When using TLS connection, the TLS certificate is mounted inside the container via mounted file system. Inside the container the directory will be /certs and the volume is in fact a Secret containing the pem certificate as string. volumeMounts : - mountPath : /certs name : eventstreams-cert-pem volumes : - name : eventstreams-cert-pem secret : optional : true secretName : eventstreams-cert-pem Build The Docker image can be built from this directory by using the appsody build command: Ensure you are logged in to the desired remote Docker registry through your local Docker client. The appsody build -t ibmcase/kcontainer-reefer-simulator:appsody-v1 --push command will build and push the application's Docker image to the specified remote image registry (here this is the public docker hub repository). Application deployment The application can be deployed to a remote OpenShift cluster by using the appsody deploy command (We recommend reading Appsody build and deploy product documentation ): Deploy using the docker image on public docker hub repository: # login to the openshift cluster if not done already oc login --token = rR.... --server = https://api.green.ocp.csplab.local:6443 # Deploy to the eda-sandbox project appsody deploy -t ibmcase/kcontainer-reefer-simulator:appsody-v1 --namespace eda-sandbox --no-build You can verify the deployment with the CLI oc get pods or the via the Openshift console: To make the webapp visible externally to the cluster, you need to add a route for this deployment. Login to the admin console and use Create Route button on top right of the screen, Then enter a name and select the existing service Once created, the URL of the app is visible in the route list panel: Add the host name in your local /etc/hosts or be sure the hostname is defined in DNS server. Map to the IP address of the kubernetes proxy server end point. Continuous deployment with Tekton The general approach to use Tekton to deploy the components of the solution is defined in this note . Usage Once deployed, you can access the Swagger-based REST API via the defined route and trigger the simulation controls. To determine the route, use the oc get route reefer-simulator command and go to the URL specified in the HOST/PORT field in your browser. From there, drill down into the POST /control section and click Try it out! . Enter any of the following options for the fields prepopulated in the control body: Container: C01, C02, C03, C04 Product: P01, P02, P03, P04 Simulation: poweroff, co2sensor, o2sensor, normal Number of records: A positive integer Click Execute To generate data at rest The same simulator can be run as a standalone tool to create csv files or to write to a remote mongoDB database. The explanation on how to use this tool is done in a separate note here . More readings Flask Restful Appsody build and deploy product documentation","title":"Develop the simulation app"},{"location":"infuse/simul-app/#the-simulator-as-web-app","text":"The Simulator webapp is a simple python (3.7) Flask web app exposing a REST POST end point to control the type of simulation to run and to produce Reefer telemetry events to kafka reeferTelemetry topic. What_to_learn In this article we shortly present the design and implementation approaches used for this application, as well as how to use Appsody to jumpstart the implementation, and continously run and debug the application. We are presenting some best practice on TDD with python.","title":"The Simulator as web app"},{"location":"infuse/simul-app/#requirements-job-stories","text":"The simulator is not in the critical path for production like component. It is here to help us develop the other components of the solution as we do not have real life Reefer container. To try something different, we are not using user stories to present what the simulator should do but we are using job stories . when I want to generate mockup telemetries data for my data scientist friend, I want to start the simulator tool from command line so I can get a csv file with data when I want to generate mockup telemetries data for my data scientist friend, I want to be able to simulate co2 sensor, o2 sensor and power sensor issue so I can get relevant data for the machine learning model to make sense when I want to generate mockup telemetries data for my data scientist friend, I want to start the simulator tool from command line using parameter so I can get save data to a remote document oriented database: mongodb on IBM cloud. when I want to demonstrate the solution, I want to call a REST api to control the generation of faulty sensor data so I can get the scoring service returning maintenance needed. The simulator needs to integrate with kafka / event stream deployed as service on the cloud or on-premise on openshift.","title":"Requirements: Job Stories"},{"location":"infuse/simul-app/#design-approach","text":"To support remote control of the simulator while running as webapp, we define a POST operation on the /control URL: with a json control object to define the number records to simulate, the sensor to impact (co2sensor, o2sensor, power) , the container ID, (one of C01, C02, C03, C04) which carries the product referenced by product_id (one of P01, P02, P03, P04, P05): { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } We have tried to support a domain driven design approach to structure the code, with domain, infrastructure and app modules. The domain module has a unique class for the simulator which main goals is to generate tuples or records for the different simulation types. It is reused for the standalone simulation tools to generate data at rest. As the simulator is also a webapp we need to package it with Flask and run it using one of the Web Server Gateway Interface (WSGI) implementation with Gunicorn . We recommend to follow Flask tutorial if you do not know this python library to develop web app or REST service. Flask is a simple library to implement REST based microservice and web application in python. It has other related projects to add interesting features to develop production application. The standard development includes defining routes, function to support handling the request and generating HTTP response, but also defining APIs... Read more with the explore Flask book online . Flask is mono threaded so it fits well in a simple web application for development purpose, but for production it is recommended to add a web server like Gunicorn to handle multiple concurrent requests.","title":"Design approach"},{"location":"infuse/simul-app/#code-approach","text":"The application is built using Appsody as the developer experience tooling. The Appsody CLI is required locally to build and deploy the application properly. The app is done using Flask, and the code is generated using appsody init python-flask command with the Python Flask appsody stack and template. Appsody helps developer to do not worry about the details of k8s deployment and build. During a Appsody run, debug or test step (2), Appsody creates a Docker container based on the parent stack Dockerfile, and combines application code with the source code in the template. We recommend reading the Python Flask Appsody Stack git hub repo to get familiar with appsody python stack. This stack is defining the Flask application, and import the 'userapp' where the application code resides, then use blueprints to define health and metrics APIs: from flask import Flask app = Flask ( __name__ ) from userapp import * from server.routes.health import health_bp app . register_blueprint ( health_bp ) from server.routes.prometheus import metrics_bp app . register_blueprint ( metrics_bp ) This code is not updatable as it is part of the image. But we can add our business logic as part of the simulator/__init__.py code using another Flask blueprints module api/controller.py . The userapp module is defined when appsody integrates our code with the stack base image using Docker. Below is an extract of the docker file managing module installation and defining what appsody does during build, run and test: ENV APPSODY_MOUNTS=/:/project/userapp ENV APPSODY_DEPS=/project/deps WORKDIR /project RUN python -m pip install -r requirements.txt -t /project/deps ENV FLASK_APP=server/__init__.py Looking at the content of the final docker container running the application we can see this structure: /project |-- Dockerfile Pipfile Pipfile.lock constraints.txt requirements.txt deps/ server/ test/ userapp/ The basic concept of blueprints is that they record operations to execute when registered on an application. So to add the operation to support the control we add a blueprint, and then register it in the main application: __init__py . from userapp.api.controller import control_blueprint app . register_blueprint ( control_blueprint ) To define the API, we use Flasgger as an extension to Flask to extract Open API specification from the code. It comes with Swagger UI, so we can see the API documentation of the microservice at the URL /apidocs . It can also validate the data according to the schema defined. For the POST /control we define the using Swagger 2.0 the API in a separate file: api/controlapi.yml and import it at the method level to support the POSt operation. This method is defined in its blueprint as a REST resource. The code controller.py is under api folder. Below is a code extract to illustrate the use of Flask-RESTful and blueprint and the swagger annotation: from flasgger import swag_from from flask_restful import Resource , Api control_blueprint = Blueprint ( \"control\" , __name__ ) api = Api ( control_blueprint ) class SimulationController ( Resource ): @swag_from ( 'controlapi.yml' ) def post ( self ): # .. api . add_resource ( SimulationController , \"/control\" ) The Pipfile defines the dependencies for this component, and is used by pipenv during the automatic build process within appsody build . To launch the web application in development mode, using an IBM Event Streams remote use the following commands: # set environment variables - from simulator folder $ source ../scripts/setenv.sh OCP # Start appsody with the environment variables: in simulator folder $ appsody run --docker-options = \"-e KAFKA_BROKERS= $KAFKA_BROKERS -e KAFKA_APIKEY= $KAFKA_APIKEY -e KAFKA_CERT= $KAFKA_CERT -e TELEMETRY_TOPIC= $TELEMETRY_TOPIC -e CONTAINER_TOPIC= $CONTAINER_TOPIC \" The trace shows the Kafka configuration options: Kafka options are: [Container] {'bootstrap.servers': 'eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443', 'group.id': 'ReeferTelemetryProducers', 'security.protocol': 'SASL_SSL', 'sasl.mechanisms': 'PLAIN', 'sasl.username': 'token', 'sasl.password': 'xCByo4478xQH...EVdcbGNCtLLuItKgVDc', 'ssl.ca.location': '/project/userapp/certs/ocp/es-cert.pem'}","title":"Code approach"},{"location":"infuse/simul-app/#testing","text":"","title":"Testing"},{"location":"infuse/simul-app/#unit-test-the-simulator","text":"The test coverage is not yet great. To run the test use appsody test . cd simulator . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / simulator root @1 de81b16f940 : / # cd / home / simulator root @1 de81b16f940 : / # python tests / unit / TestSimulator . py","title":"Unit test the Simulator"},{"location":"infuse/simul-app/#functional-testing","text":"","title":"Functional testing"},{"location":"infuse/simul-app/#prepare-for-kubernetes-deployment","text":"There are three required configuration elements for connectivity to IBM Event Streams (Kafka) prior to deployment as we already presented in the following notes. A ConfigMap named kafka-brokers Reference Link A Secret named eventstreams-api-key Reference Link A Secret named eventstreams-cert-pem (if connecting to an on-premise version of IBM Event Streams) Reference Link Once those elements are defined it is important to configure the app so it can retrieve those information via environment variables. With Appsody the file appsody-config.yaml is supporting these configurations. See the lines 24 to 40 to get the settings to map environment variables to Config Maps or Secrets. When using TLS connection, the TLS certificate is mounted inside the container via mounted file system. Inside the container the directory will be /certs and the volume is in fact a Secret containing the pem certificate as string. volumeMounts : - mountPath : /certs name : eventstreams-cert-pem volumes : - name : eventstreams-cert-pem secret : optional : true secretName : eventstreams-cert-pem","title":"Prepare for kubernetes deployment"},{"location":"infuse/simul-app/#build","text":"The Docker image can be built from this directory by using the appsody build command: Ensure you are logged in to the desired remote Docker registry through your local Docker client. The appsody build -t ibmcase/kcontainer-reefer-simulator:appsody-v1 --push command will build and push the application's Docker image to the specified remote image registry (here this is the public docker hub repository).","title":"Build"},{"location":"infuse/simul-app/#application-deployment","text":"The application can be deployed to a remote OpenShift cluster by using the appsody deploy command (We recommend reading Appsody build and deploy product documentation ): Deploy using the docker image on public docker hub repository: # login to the openshift cluster if not done already oc login --token = rR.... --server = https://api.green.ocp.csplab.local:6443 # Deploy to the eda-sandbox project appsody deploy -t ibmcase/kcontainer-reefer-simulator:appsody-v1 --namespace eda-sandbox --no-build You can verify the deployment with the CLI oc get pods or the via the Openshift console: To make the webapp visible externally to the cluster, you need to add a route for this deployment. Login to the admin console and use Create Route button on top right of the screen, Then enter a name and select the existing service Once created, the URL of the app is visible in the route list panel: Add the host name in your local /etc/hosts or be sure the hostname is defined in DNS server. Map to the IP address of the kubernetes proxy server end point.","title":"Application deployment"},{"location":"infuse/simul-app/#continuous-deployment-with-tekton","text":"The general approach to use Tekton to deploy the components of the solution is defined in this note .","title":"Continuous deployment with Tekton"},{"location":"infuse/simul-app/#usage","text":"Once deployed, you can access the Swagger-based REST API via the defined route and trigger the simulation controls. To determine the route, use the oc get route reefer-simulator command and go to the URL specified in the HOST/PORT field in your browser. From there, drill down into the POST /control section and click Try it out! . Enter any of the following options for the fields prepopulated in the control body: Container: C01, C02, C03, C04 Product: P01, P02, P03, P04 Simulation: poweroff, co2sensor, o2sensor, normal Number of records: A positive integer Click Execute","title":"Usage"},{"location":"infuse/simul-app/#to-generate-data-at-rest","text":"The same simulator can be run as a standalone tool to create csv files or to write to a remote mongoDB database. The explanation on how to use this tool is done in a separate note here .","title":"To generate data at rest"},{"location":"infuse/simul-app/#more-readings","text":"Flask Restful Appsody build and deploy product documentation","title":"More readings"},{"location":"organize/readme/","text":"Organize data Under construction","title":"Organize"},{"location":"organize/readme/#organize-data","text":"Under construction","title":"Organize data"}]}