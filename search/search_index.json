{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reefer Anomaly Detection Solution This project is to demonstrate how to perform real time analytics, like anomaly detection for Reefer container in the shipping industry, using Reefer container telemetry event stream. Note This project is part of the reference implementation solution to demonstrate the IBM event driven reference architecture but it also presents how to combine different IBM cloud paks to build the solution: Cloud pak for data, for automation, for integration and for application. As we will detail in next section there are four components in this solution that make the anomaly detection: a Reefer simulator (we do not have such Reefer container in our stock), a container microservice, an analytics scoring agent and a business process. The diagram below is a very high level view of the components in a three layer architecture with Openshift for the deployment infrastructure. We also look at an open source version of this solution using an approach close to opendatahub.io proposed architecture, as illustrated in the following diagram: The Reefer container is an IoT device, which emits container telemetries every 10 minutes via the MQTT protocol. The first component receiving those messages is Apache Nifi to transform the telemetry message to a kafka event. Kafka is used as the event backbone and event sourcing so microservices, deployed on Openshift, can consume and publish messages. For persistence reason, we may leverage big data type of storage like Postgresql or Cassandra to persist the container's telemetry over a longer time period. This datasource is used by the Data Scientists to do its data preparation and build training and test sets and select the best model. We also illustrate how to connect to Kafka topic as data source, from a Jupyter notebook. Data scientists can run Jupyter lab on OpenShift and build a model to be deployed as python microservice, consumer of Reefer telemetry events. When anomaly is detected, a new event is posted to the containers topic so the Reefer container manager microservice can apply the expected business logic. MVP component view For a minimum viable demonstration the runtime components looks like in the figure below: A web app, deployed on Openshift, is running a simulator to simulate the generation of Reefer container telemetries while the container is at sea or during end to end transportation. The app exposes a simple POST operation with a control object to control the simulation. Here is an example of such control.json object { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } The simulation can be done on o2sensor, co2sensor or power. A curl script does the HTTP POST request of this json object. See this paragraph. The telemetry events are sent to the reeferTelemetries topic in Kafka. The predictive scoring is a consumer of such events, read one event at a time and call the model internally, then sends a new event when maintenance is required. See the note for details. The maintenance requirement is an event in the containers topic. The 6 th component of the solution, is the container microservice which was defined in the EDA reference implementation. The maintenance engineer intervention process is modeled in BPM, deploy on public cloud and the process application is exposed as API. The container identifier and the telemetry record is sent as input to the process. Pre-requisites to build and run this solution Start by cloning this project using the command: git clone https://github.com/ibm-cloud-architecture/refarch-reefer-ml Be sure to have Event Stream or a Kafka cluster running somewhere We recommend creating the Event Stream service using the IBM Cloud catalog , you can also read our quick article on how to deploy event stream. As an alternate approach, we have deployed Event Stream on Openshift running on-premise servers following the product documentation here . The following diagram illustrates the needed Kafka topics configured in IBM Cloud Event Stream service: With IBM Cloud deployment use the service credentials to create new credentials to get the Kafka brokers list, the admin URL and the api key needed to authenticate the consumers and the producers. For Event Streams on Openshift deployment, click to the connect to the cluster button to get the broker URL and to generate the API key: select the option to generate the key for all topics. Provision a Postgresql service If you plan to use Postgresql as a data source instead of using csv file, then you need to provision a Postgresql service in IBM Cloud. Use the product documentation to provision your own service. Define service credential and use the composed url, the database name and the SSL certificate. Use the following commands to get the certificate: ibmcloud login ibmcloud cdb cacert <database deployment name> Save this file as postgres.pem under the simulator folder. Set environment variables As part of the 12 factors practice , we externalize the end points configuration in environment variables. We are providing a script template ( scripts/setenv-tmp.sh ) to set those variables for your local development. Rename this file as setenv.sh . This file is git ignored, to do not share keys and passwords in public domain. The variables help the different code in the solution to access the Event Stream broker cluster and the Postgresql service running on IBM Cloud. Building a python development environment as docker image To avoid impacting our laptop environment (specially macbook which use python), we use a dockerfile to get the basic of python 3.7.x and the python modules like kafka, http requests, pandas, sklearn, pytest... we need to develop and test the different python code of this solution. To build your python image with all the needed libraries, use the following commands: cd docker docker build -f docker-python-tools -t ibmcase/python . To use this python environment you can use the script: startPythonEnv . When running with Event Stream and Postgres on the cloud use IBMCLOUD argument: # refarch-reefer-ml project folder ./startPythonEnv.sh IBMCLOUD Build the docker image for Jupyter notebook We are using a special version of conda to add the postgresql and kafka libraries for python so we can access postgresql or kafka from notebook. The Dockerfile may use a cert.pem file, which contains the postgres certificate so the notebook can connect to postgresql service with SSL connection. cd docker docker build -f docker-jupyter-tool -t ibmcase/jupyter . To run this jupyter server use the startJupyterServer.sh script: # refarch-reefer-ml project folder ./startJupyterServer.sh IBMCLOUD Create the postgresql database If you use POSTGRESQL on IBM Cloud, you need to get the SSL certificate and put it as postgres.pem under the simulator folder, or set POSTGRES_SSL_PEM to the path where to find this file. The postgres.pem file needs to be in the simulator folder. Run the ReeferRepository.py tool to create the database schema and to add some reference data like the product catalog: ./startPythonEnv.sh IBMCLOUD > python simulator/infrastructure/ReeferRepository.py You should see the following trace: Connect remote with ssl ('PostgreSQL 10.10 on x86_64-pc-linux-gnu, compiled by gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516, 64-bit',) [ { \"container_id\": \"C01\", \"last_maintenance_date\": null, \"reefer_model\": \"20RF\" }, { \"container_id\": \"C02\", \"last_maintenance_date\": null, \"reefer_model\": \"20RF\" }, { \"container_id\": \"C03\", \"last_maintenance_date\": null, \"reefer_model\": \"40RH\" }, { \"container_id\": \"C04\", \"last_maintenance_date\": null, \"reefer_model\": \"45RW\" } ] [ { \"content_type\": 1, \"description\": \"Carrots\", \"product_id\": \"P01\", \"target_humidity_level\": 0.4, \"target_temperature\": 4.0 }, { \"content_type\": 2, \"description\": \"Banana\", \"product_id\": \"P02\", \"target_humidity_level\": 0.6, \"target_temperature\": 6.0 }, { \"content_type\": 1, \"description\": \"Salad\", \"product_id\": \"P03\", \"target_humidity_level\": 0.4, \"target_temperature\": 4.0 }, { \"content_type\": 2, \"description\": \"Avocado\", \"product_id\": \"P04\", \"target_humidity_level\": 0.4, \"target_temperature\": 6.0 } ] ('public', 'reefers', 'ibm-cloud-base-user', None, True, False, True, False) ('public', 'products', 'ibm-cloud-base-user', None, True, False, True, False) ('public', 'reefer_telemetries', 'ibm-cloud-base-user', None, True, False, True, False) Project approach The solution covers different development areas and practices: event driven microservice, business process, data ops and machine learning model development. The software life cycle integrate the three main tracks of DevOps, DataOps and MLOps as presented in this methodology article . DevOps As all the services developed in this solution are event-driven we do not need to rephrase how we used event storming and applied the different patterns for each microservices. You can read the following articles to learn more: Event storming workshop From analysis to microservice using domain driven design Apply DDD to order microservice Apply DDD to the container management microservice For the adoption of Appsody and CI/CD based on tektron, we present how we use Cloud Pak for application in the development of the scoring microservice in this note . To understand how to deploy each service on openshift see this detailed note . DataOps Collect data with open source approach . Collect data with cloud pak for data . MLOps To use Jupyter, Sparks and kubeflow see this note Further Readings Romeo Kienzler anomaly detection article 1 Romeo Kienzler anomaly detection article 2 Romeo Kienzler anomaly detection article 3","title":"Introduction"},{"location":"#reefer-anomaly-detection-solution","text":"This project is to demonstrate how to perform real time analytics, like anomaly detection for Reefer container in the shipping industry, using Reefer container telemetry event stream. Note This project is part of the reference implementation solution to demonstrate the IBM event driven reference architecture but it also presents how to combine different IBM cloud paks to build the solution: Cloud pak for data, for automation, for integration and for application. As we will detail in next section there are four components in this solution that make the anomaly detection: a Reefer simulator (we do not have such Reefer container in our stock), a container microservice, an analytics scoring agent and a business process. The diagram below is a very high level view of the components in a three layer architecture with Openshift for the deployment infrastructure. We also look at an open source version of this solution using an approach close to opendatahub.io proposed architecture, as illustrated in the following diagram: The Reefer container is an IoT device, which emits container telemetries every 10 minutes via the MQTT protocol. The first component receiving those messages is Apache Nifi to transform the telemetry message to a kafka event. Kafka is used as the event backbone and event sourcing so microservices, deployed on Openshift, can consume and publish messages. For persistence reason, we may leverage big data type of storage like Postgresql or Cassandra to persist the container's telemetry over a longer time period. This datasource is used by the Data Scientists to do its data preparation and build training and test sets and select the best model. We also illustrate how to connect to Kafka topic as data source, from a Jupyter notebook. Data scientists can run Jupyter lab on OpenShift and build a model to be deployed as python microservice, consumer of Reefer telemetry events. When anomaly is detected, a new event is posted to the containers topic so the Reefer container manager microservice can apply the expected business logic.","title":"Reefer Anomaly Detection Solution"},{"location":"#mvp-component-view","text":"For a minimum viable demonstration the runtime components looks like in the figure below: A web app, deployed on Openshift, is running a simulator to simulate the generation of Reefer container telemetries while the container is at sea or during end to end transportation. The app exposes a simple POST operation with a control object to control the simulation. Here is an example of such control.json object { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } The simulation can be done on o2sensor, co2sensor or power. A curl script does the HTTP POST request of this json object. See this paragraph. The telemetry events are sent to the reeferTelemetries topic in Kafka. The predictive scoring is a consumer of such events, read one event at a time and call the model internally, then sends a new event when maintenance is required. See the note for details. The maintenance requirement is an event in the containers topic. The 6 th component of the solution, is the container microservice which was defined in the EDA reference implementation. The maintenance engineer intervention process is modeled in BPM, deploy on public cloud and the process application is exposed as API. The container identifier and the telemetry record is sent as input to the process.","title":"MVP component view"},{"location":"#pre-requisites-to-build-and-run-this-solution","text":"Start by cloning this project using the command: git clone https://github.com/ibm-cloud-architecture/refarch-reefer-ml","title":"Pre-requisites to build and run this solution"},{"location":"#be-sure-to-have-event-stream-or-a-kafka-cluster-running-somewhere","text":"We recommend creating the Event Stream service using the IBM Cloud catalog , you can also read our quick article on how to deploy event stream. As an alternate approach, we have deployed Event Stream on Openshift running on-premise servers following the product documentation here . The following diagram illustrates the needed Kafka topics configured in IBM Cloud Event Stream service: With IBM Cloud deployment use the service credentials to create new credentials to get the Kafka brokers list, the admin URL and the api key needed to authenticate the consumers and the producers. For Event Streams on Openshift deployment, click to the connect to the cluster button to get the broker URL and to generate the API key: select the option to generate the key for all topics.","title":"Be sure to have Event Stream or a Kafka cluster running somewhere"},{"location":"#provision-a-postgresql-service","text":"If you plan to use Postgresql as a data source instead of using csv file, then you need to provision a Postgresql service in IBM Cloud. Use the product documentation to provision your own service. Define service credential and use the composed url, the database name and the SSL certificate. Use the following commands to get the certificate: ibmcloud login ibmcloud cdb cacert <database deployment name> Save this file as postgres.pem under the simulator folder.","title":"Provision a Postgresql service"},{"location":"#set-environment-variables","text":"As part of the 12 factors practice , we externalize the end points configuration in environment variables. We are providing a script template ( scripts/setenv-tmp.sh ) to set those variables for your local development. Rename this file as setenv.sh . This file is git ignored, to do not share keys and passwords in public domain. The variables help the different code in the solution to access the Event Stream broker cluster and the Postgresql service running on IBM Cloud.","title":"Set environment variables"},{"location":"#building-a-python-development-environment-as-docker-image","text":"To avoid impacting our laptop environment (specially macbook which use python), we use a dockerfile to get the basic of python 3.7.x and the python modules like kafka, http requests, pandas, sklearn, pytest... we need to develop and test the different python code of this solution. To build your python image with all the needed libraries, use the following commands: cd docker docker build -f docker-python-tools -t ibmcase/python . To use this python environment you can use the script: startPythonEnv . When running with Event Stream and Postgres on the cloud use IBMCLOUD argument: # refarch-reefer-ml project folder ./startPythonEnv.sh IBMCLOUD","title":"Building a python development environment as docker image"},{"location":"#build-the-docker-image-for-jupyter-notebook","text":"We are using a special version of conda to add the postgresql and kafka libraries for python so we can access postgresql or kafka from notebook. The Dockerfile may use a cert.pem file, which contains the postgres certificate so the notebook can connect to postgresql service with SSL connection. cd docker docker build -f docker-jupyter-tool -t ibmcase/jupyter . To run this jupyter server use the startJupyterServer.sh script: # refarch-reefer-ml project folder ./startJupyterServer.sh IBMCLOUD","title":"Build the docker image for Jupyter notebook"},{"location":"#create-the-postgresql-database","text":"If you use POSTGRESQL on IBM Cloud, you need to get the SSL certificate and put it as postgres.pem under the simulator folder, or set POSTGRES_SSL_PEM to the path where to find this file. The postgres.pem file needs to be in the simulator folder. Run the ReeferRepository.py tool to create the database schema and to add some reference data like the product catalog: ./startPythonEnv.sh IBMCLOUD > python simulator/infrastructure/ReeferRepository.py You should see the following trace: Connect remote with ssl ('PostgreSQL 10.10 on x86_64-pc-linux-gnu, compiled by gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516, 64-bit',) [ { \"container_id\": \"C01\", \"last_maintenance_date\": null, \"reefer_model\": \"20RF\" }, { \"container_id\": \"C02\", \"last_maintenance_date\": null, \"reefer_model\": \"20RF\" }, { \"container_id\": \"C03\", \"last_maintenance_date\": null, \"reefer_model\": \"40RH\" }, { \"container_id\": \"C04\", \"last_maintenance_date\": null, \"reefer_model\": \"45RW\" } ] [ { \"content_type\": 1, \"description\": \"Carrots\", \"product_id\": \"P01\", \"target_humidity_level\": 0.4, \"target_temperature\": 4.0 }, { \"content_type\": 2, \"description\": \"Banana\", \"product_id\": \"P02\", \"target_humidity_level\": 0.6, \"target_temperature\": 6.0 }, { \"content_type\": 1, \"description\": \"Salad\", \"product_id\": \"P03\", \"target_humidity_level\": 0.4, \"target_temperature\": 4.0 }, { \"content_type\": 2, \"description\": \"Avocado\", \"product_id\": \"P04\", \"target_humidity_level\": 0.4, \"target_temperature\": 6.0 } ] ('public', 'reefers', 'ibm-cloud-base-user', None, True, False, True, False) ('public', 'products', 'ibm-cloud-base-user', None, True, False, True, False) ('public', 'reefer_telemetries', 'ibm-cloud-base-user', None, True, False, True, False)","title":"Create the postgresql database"},{"location":"#project-approach","text":"The solution covers different development areas and practices: event driven microservice, business process, data ops and machine learning model development. The software life cycle integrate the three main tracks of DevOps, DataOps and MLOps as presented in this methodology article .","title":"Project approach"},{"location":"#devops","text":"As all the services developed in this solution are event-driven we do not need to rephrase how we used event storming and applied the different patterns for each microservices. You can read the following articles to learn more: Event storming workshop From analysis to microservice using domain driven design Apply DDD to order microservice Apply DDD to the container management microservice For the adoption of Appsody and CI/CD based on tektron, we present how we use Cloud Pak for application in the development of the scoring microservice in this note . To understand how to deploy each service on openshift see this detailed note .","title":"DevOps"},{"location":"#dataops","text":"Collect data with open source approach . Collect data with cloud pak for data .","title":"DataOps"},{"location":"#mlops","text":"To use Jupyter, Sparks and kubeflow see this note","title":"MLOps"},{"location":"#further-readings","text":"Romeo Kienzler anomaly detection article 1 Romeo Kienzler anomaly detection article 2 Romeo Kienzler anomaly detection article 3","title":"Further Readings"},{"location":"performance-tests/","text":"Performance tests We have to develop some performance tests for some client engagements. So reusing the reefer telemetry generator can help us to run performance test. The simulator is configured to run 2 Gunicorn processes with 4 threads each. So potentially running 8 requests in parallel. (See the code simulator/infrastructure/webappconfig.py )","title":"Performance tests"},{"location":"performance-tests/#performance-tests","text":"We have to develop some performance tests for some client engagements. So reusing the reefer telemetry generator can help us to run performance test. The simulator is configured to run 2 Gunicorn processes with 4 threads each. So potentially running 8 requests in parallel. (See the code simulator/infrastructure/webappconfig.py )","title":"Performance tests"},{"location":"analyze/oss-ml-dev/","text":"Defining the anomaly detection scoring model Predictive maintenance and anomaly detection are complex problems to address. We do not pretend to address those complex problems in this repository, as we focus in putting in place the end to end creation and deployment of the model. To review the problem of predictive maintenance read this article. If you want to contribute to build a better model, we are looking for contributors . To build the model and work on the data, we will use a local version of Jupyter notebook to load the logistic regression nodebook from the ml folder. We have two types of notebook Start a jupyter server using our docker image and a postgresql in IBM cloud. pwd ./startJupyterServer IBMCLOUD or LOCAL Then open a web browser to http://localhost:8888?token=<sometoken> go under work/ml folder. Open one of the model: the model_logistic_regression.ipynb to work on data set saved in the ml/data/telemetries.csv file. the model_logistic_regression-pg.ipynb to work on data saved in postgresql running on IBM Cloud. The notebooks include comments to explain how the model is done. We use logistic regression to build a binary classification (maintenance required or not), as the data are simulated, and the focus is not in the model building, but more on the end to end process. The notebook persists the trained model as a pickle file so it can be loaded by a python module or another notebook. For more information on using the Jupyter notebook, here is a product documentation . The co2 sensor plot over time shows the training data with some sporadic behavior: The confusion matrix shows very little false positive and false negative: Use the model in another notebook: We can use a second notebook to test the model with one telemetry record using the pickle serialized model. The notebook is named predictMaintenance.ipynb .","title":"Define the anomaly detection scoring model with OSS"},{"location":"analyze/oss-ml-dev/#defining-the-anomaly-detection-scoring-model","text":"Predictive maintenance and anomaly detection are complex problems to address. We do not pretend to address those complex problems in this repository, as we focus in putting in place the end to end creation and deployment of the model. To review the problem of predictive maintenance read this article. If you want to contribute to build a better model, we are looking for contributors . To build the model and work on the data, we will use a local version of Jupyter notebook to load the logistic regression nodebook from the ml folder. We have two types of notebook Start a jupyter server using our docker image and a postgresql in IBM cloud. pwd ./startJupyterServer IBMCLOUD or LOCAL Then open a web browser to http://localhost:8888?token=<sometoken> go under work/ml folder. Open one of the model: the model_logistic_regression.ipynb to work on data set saved in the ml/data/telemetries.csv file. the model_logistic_regression-pg.ipynb to work on data saved in postgresql running on IBM Cloud. The notebooks include comments to explain how the model is done. We use logistic regression to build a binary classification (maintenance required or not), as the data are simulated, and the focus is not in the model building, but more on the end to end process. The notebook persists the trained model as a pickle file so it can be loaded by a python module or another notebook. For more information on using the Jupyter notebook, here is a product documentation . The co2 sensor plot over time shows the training data with some sporadic behavior: The confusion matrix shows very little false positive and false negative: Use the model in another notebook: We can use a second notebook to test the model with one telemetry record using the pickle serialized model. The notebook is named predictMaintenance.ipynb .","title":"Defining the anomaly detection scoring model"},{"location":"analyze/predictive-maintenance/","text":"Reefer Container Predictive Maintenance In this section, we discuss how to build an analytic model using machine learning techniques from data coming from event store like kafka. We train the model with the help of historical data to predict whether maintenance is required for the reefer container at a certain point in time. You will learn how to simulate date for reefer, develop the predictive maintenance model, and integrate the model into an application. Introduction A reefer container is a refrigerated shipping container used to store or transport frozen or cold goods perishable items or goods that require temperature control. Reefers make an excellent, portable solution for short or long term storage and can be used to ship or truck goods over long distances as they can be plugged into the power station on ships or have it clipped on generators attached. Perishable products must be kept at a controlled temperature, from point of origin to delivery to retailer or pharmacy. The logistics industry refers to this as the \u201ccold chain\u201d and it encompasses both \u201creefers\u201d (refrigerated containers) as well as warehouses, distribution centers and the final storage or holding areas. Throughout this chain the risk of failure is ever-present, meaning there is always a possibility of cargo exceeding permissible or safe temperature levels, even if only briefly. For example, a truck might be stopped without power in desert heat, allowing temperatures in the reefer to rise. Then power is restored and the temperature in the container comes back down, but the product is damaged. When cargo with such as any of those items listed above are exposed to temperatures outside of prescribed limits it can be damaged. In some cases this is evident, such as with bananas, but in other situations, like the transport of vaccines, it may not be apparent that damage has occurred and the vaccine becomes ineffective. For some products, going over temperature, even only briefly, can reduce shelf life dramatically, incurring substantial costs when it cannot be sold. Organizations contracting to ship perishable products often specify the permissible temperature range. However, even if it is possible to show that product was exposed to conditions outside of those contracted, proving where it happened, and thus responsibility, can be much harder. Predictive maintenance problem statement If you want a good understanding of the problem space for predictive maintenance read, Yana Ageeva's article in toward data science . The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly From a methodology point of view the Data Scientist needs to address the following questions: What type of failure to consider and which one to predict? What kind of failure is happening? slow degradation or instantaneous failure? What could be the relation between a product characteristics and the failure? What kind of measure exist to assess the given characteristic? Which measurements correspond to good functioning and which ones correspond to failure? How often metrics are reported? What question the model should answer? What kind of output should the model give? How long in advance should the model be able to indicate that a failure will occur? What are the business impact to do not predict the failure? and predicting false negative failure? What is the expected accuracy? Reefer problem types There are multiple different potential issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions\" issue: Sensors in the refrigeration unit need to be calibrated and be continuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which leads to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be difficult to know if there is an issue or not. The other common potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. Blocked air passage: to keep the temperature homogenous inside the reefer. So the question we want to answer is: does the Reefer keep accurate temperature overtime between what is set versus what is measured? Modeling techniques The model uses the generated data from above scenarios: When the container's door is open for a longer time - this gives a false positive that maintainence is required. When sensors are malfunctioning, it records arbitrary readings. When the readings are normal. We have currently trained our model on 3000 datapoints from the three scenarios above. There are different modeling approach to tackle predictive maintenance: regression model classification to predict failure for a given time period classify anomalous behavior: classes are not known in advance. Normal operation is known. compute probability of failure over time Code execution The simulator continuosly generates container metrics, publishes it to Kafka and run the predictMaintainence.ipynb to predict if maintainence is sought at this point in time. Model description We are using Machine Learning supervised learning here. There are two types of supervised learning - 1) Classification: Predict a categorical response, 2) Regression: Predict a continuous response Linear regression Pros: 1) Fast 2) No tuning required 3) Highly interpretable 4) Well-understood Cons: 1) Unlikely to produce the best predictive accuracy 2) Presumes a linear relationship between the features and response 3) If the relationship is highly non-linear as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate Naive Bayes classification Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent. The classification is conducted by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively. Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator. Model evaluation We are using Root Mean Squared Error (RMSE) for evaluating the model performance. Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors. Classification does better here as the scenarion is more of a classification problem. References Understand Reefer container For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting. PREDICTION OF TEMPERATURE INSIDE A REFRIGERATED CONTAINER IN THE PRESENCE OF PERISHABLE GOODS Temperature Monitoring During Transportation, Storage and Processing of Perishable Products Understanding machine learning classifiers","title":"Reefer anomaly detection problem"},{"location":"analyze/predictive-maintenance/#reefer-container-predictive-maintenance","text":"In this section, we discuss how to build an analytic model using machine learning techniques from data coming from event store like kafka. We train the model with the help of historical data to predict whether maintenance is required for the reefer container at a certain point in time. You will learn how to simulate date for reefer, develop the predictive maintenance model, and integrate the model into an application.","title":"Reefer Container Predictive Maintenance"},{"location":"analyze/predictive-maintenance/#introduction","text":"A reefer container is a refrigerated shipping container used to store or transport frozen or cold goods perishable items or goods that require temperature control. Reefers make an excellent, portable solution for short or long term storage and can be used to ship or truck goods over long distances as they can be plugged into the power station on ships or have it clipped on generators attached. Perishable products must be kept at a controlled temperature, from point of origin to delivery to retailer or pharmacy. The logistics industry refers to this as the \u201ccold chain\u201d and it encompasses both \u201creefers\u201d (refrigerated containers) as well as warehouses, distribution centers and the final storage or holding areas. Throughout this chain the risk of failure is ever-present, meaning there is always a possibility of cargo exceeding permissible or safe temperature levels, even if only briefly. For example, a truck might be stopped without power in desert heat, allowing temperatures in the reefer to rise. Then power is restored and the temperature in the container comes back down, but the product is damaged. When cargo with such as any of those items listed above are exposed to temperatures outside of prescribed limits it can be damaged. In some cases this is evident, such as with bananas, but in other situations, like the transport of vaccines, it may not be apparent that damage has occurred and the vaccine becomes ineffective. For some products, going over temperature, even only briefly, can reduce shelf life dramatically, incurring substantial costs when it cannot be sold. Organizations contracting to ship perishable products often specify the permissible temperature range. However, even if it is possible to show that product was exposed to conditions outside of those contracted, proving where it happened, and thus responsibility, can be much harder.","title":"Introduction"},{"location":"analyze/predictive-maintenance/#predictive-maintenance-problem-statement","text":"If you want a good understanding of the problem space for predictive maintenance read, Yana Ageeva's article in toward data science . The success of predictive maintenance models depend on three main components: having the right data framing the problem appropriately evaluating the predictions properly From a methodology point of view the Data Scientist needs to address the following questions: What type of failure to consider and which one to predict? What kind of failure is happening? slow degradation or instantaneous failure? What could be the relation between a product characteristics and the failure? What kind of measure exist to assess the given characteristic? Which measurements correspond to good functioning and which ones correspond to failure? How often metrics are reported? What question the model should answer? What kind of output should the model give? How long in advance should the model be able to indicate that a failure will occur? What are the business impact to do not predict the failure? and predicting false negative failure? What is the expected accuracy?","title":"Predictive maintenance problem statement"},{"location":"analyze/predictive-maintenance/#reefer-problem-types","text":"There are multiple different potential issues that could happen to a refrigerator container. We are choosing to model the \"Sensor Malfunctions\" issue: Sensors in the refrigeration unit need to be calibrated and be continuously operational. An example of failure may come from the air sensor making inaccurate readings of temperatures, which leads to sploiled content. A potential reason may come from a faulty calibration, which can go unnoticed for a good time period. It may be difficult to know if there is an issue or not. The other common potential issues are: Fluid leaks, like engine oil, coolant liquid. The preassure sensors added to the circuit may help identify preassure lost over time. Faulty belts and hoses. Faulty calibration: A non-calibrated reefer can cool at a slower or faster rate than desired. Damaged Air Chute. Condenser Issues like broken or damaged coils, clamps or bolts missing, and leaks. Door Seals damaged. Blocked air passage: to keep the temperature homogenous inside the reefer. So the question we want to answer is: does the Reefer keep accurate temperature overtime between what is set versus what is measured?","title":"Reefer problem types"},{"location":"analyze/predictive-maintenance/#modeling-techniques","text":"The model uses the generated data from above scenarios: When the container's door is open for a longer time - this gives a false positive that maintainence is required. When sensors are malfunctioning, it records arbitrary readings. When the readings are normal. We have currently trained our model on 3000 datapoints from the three scenarios above. There are different modeling approach to tackle predictive maintenance: regression model classification to predict failure for a given time period classify anomalous behavior: classes are not known in advance. Normal operation is known. compute probability of failure over time","title":"Modeling techniques"},{"location":"analyze/predictive-maintenance/#code-execution","text":"The simulator continuosly generates container metrics, publishes it to Kafka and run the predictMaintainence.ipynb to predict if maintainence is sought at this point in time.","title":"Code execution"},{"location":"analyze/predictive-maintenance/#model-description","text":"We are using Machine Learning supervised learning here. There are two types of supervised learning - 1) Classification: Predict a categorical response, 2) Regression: Predict a continuous response","title":"Model description"},{"location":"analyze/predictive-maintenance/#linear-regression","text":"Pros: 1) Fast 2) No tuning required 3) Highly interpretable 4) Well-understood Cons: 1) Unlikely to produce the best predictive accuracy 2) Presumes a linear relationship between the features and response 3) If the relationship is highly non-linear as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate","title":"Linear regression"},{"location":"analyze/predictive-maintenance/#naive-bayes-classification","text":"Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent. The classification is conducted by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively. Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator.","title":"Naive Bayes classification"},{"location":"analyze/predictive-maintenance/#model-evaluation","text":"We are using Root Mean Squared Error (RMSE) for evaluating the model performance. Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors. Classification does better here as the scenarion is more of a classification problem.","title":"Model evaluation"},{"location":"analyze/predictive-maintenance/#references","text":"Understand Reefer container For modeling predictive maintenance we found this article from BigData Republique, on Medium, very interesting. PREDICTION OF TEMPERATURE INSIDE A REFRIGERATED CONTAINER IN THE PRESENCE OF PERISHABLE GOODS Temperature Monitoring During Transportation, Storage and Processing of Perishable Products Understanding machine learning classifiers","title":"References"},{"location":"collect/cp4d-collect-data/","text":"IBM Cloud Pak for Data: data collection","title":"Collect data with cloud pak for data"},{"location":"collect/cp4d-collect-data/#ibm-cloud-pak-for-data-data-collection","text":"","title":"IBM Cloud Pak for Data: data collection"},{"location":"collect/generate-telemetry/","text":"Generate telemetry data We are using the simulator to generate data. In the industry, when developing new manufactured product, the engineers do not have a lot of data so they also use a mix of real sensors with simulators to create fake but realistic data to develop and test their models. The historical data need to represent failure, and represent the characteristics of a Reefer container. We have define some sensors to get interesting correlated or independant features. As of now our telemetry event structure can be seen in this avro schema . For the machine learning environment we can use csv file as input data or mongodb database or kafka topic. The environment looks like in the figure below: The simulator can run as a standalone tool to create training and test data to be saved in a remote mongodb database or can be used to save to csv file. We use mongodb as a service on IBM cloud. We also use reference data like products and shipping container inventory in Postgresql on IBM Cloud too. The service has credential with URL and SSL certificate. Create reference data in postgresql See this separate note for detail. Start python environment Use IBMCLOUD if you use mongodb, postgresql and kafka on cloud, or LOCAL for kafka and postgresql running via docker compose. . / startPythonEnv IBMCLOUD or LOCAL or CP root @03721594782f : cd / home In the Dockerfile we set the first PYTHONPATH to /home to specify where python should find the application specifics modules. Generate data as csv file Generate power off metrics When a reefer container loses power, restart and reloose it, it may become an issue. This is the goal of this simulation. The simulator accepts different arguments as specified below usage reefer_simulator-tool --stype [poweroff | co2sensor | o2sensor | normal] --cid [C01 | C02 | C03 | C04 | C05] --product_id [ P01 | P02 | P03 | P04 | P05 ] --records <the number of records to generate> --file <the filename to create> --append --db The cid is for the container id. As the simulator is taking some data from internal datasource you can use only one of those values: [C01 | C02 | C03 | C04 | C05] product_id is also one of the value [ P01 | P02 | P03 | P04 | P05] , as the simulator will derive the target temperature and humidity level from its internal datasource: ('P01','Carrots',1,4,0.4), ('P02','Banana',2,6,0.6), ('P03','Salad',1,4,0.4), ('P04','Avocado',2,6,0.4), ('P05','Tomato',1,6,0.4); --db is when you want to save the telemetry into mongodb DB. In this case be sure to have set the credentials and URL in the scripts/setenv.sh script (see the scripts/setenv-tmp.sh template file) --file is to specify a csv file to write the data --append is used to append the output of this run to an existing file: It permits to accumulate different simulation in the same dataset. (Re)create a new file. It is an important step to get the header as first row. python simulator/reefer_simulator_tool.py --stype poweroff --cid C01 --records 1000 --product_id P02 --file telemetries.csv append to existing file python simulator/reefer_simulator_tool.py --cid C03 --product_id P03 --records 1000 --file telemetries.csv --stype poweroff --append The results looks like: Generating 1000 poweroff metrics Timestamp ID Temperature(celsius) Target_Temperature(celsius) Power PowerConsumption ContentType O2 CO2 Time_Door_Open Maintenance_Required Defrost_Cycle 1.000000 2019-06-30 T15:43 Z 101 3.416766 4 17.698034 6.662044 1 11 1 8.735273 0 6 1.001001 2019-06-30 T15:43 Z 101 4.973630 4 3.701072 8.457314 1 13 3 5.699655 0 6 1.002002 2019-06-30 T15:43 Z 101 1.299275 4 7.629094 From the two previous commands you should have 2001 rows (one for the header which will be used in the model creation): wc -l telemetries.csv 2001 telemetries.csv Generate Co2 sensor malfunction in same file In the same way as above the simulator can generate data for Co2 sensor malfunction using the command: python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype co2sensor --append Note The simulator is integrated in the event producer to send real time events to kafka, as if the Reefer container was loaded with fresh goods and is travelling oversea. A consumer code can call the predictive model to assess if maintenance is required and post new event on a containers topic (this consumer code is in the scoring/eventConsumer folder). Generate O2 sensor malfunction in same file python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype o2sensor --append Saving to Mongodb The same tool can be used to save to a mongodb database. The document saved in mongo will not be a flat record as produced by the simulator when it creates records to Kafka, but the Json document matches the following format, where sensors are in their own object: { \"timestamp\" : \"2019-09-04 T15:31 Z\" , \"containerID\" : \"C100\" , \"product_id\" : \"P02\" , \"content_type\" : 2 , \"sensors\" : { \"temperature\" : 5.49647 , \"oxygen_level\" : 20.4543 , \"nitrogen_level\" : 79.4046 , \"carbon_dioxide_level\" : 4.42579 , \"humidity_level\" : 60.3148 , \"fan_1\" : \"True\" , \"fan_2\" : \"True\" , \"fan_3\" : \"True\" , \"ambiant_temperature\" : 19.8447 }, \"target_temperature\" : 6.0 , \"kilowatts\" : 3.44686 , \"latitude\" : \"37.8226902168957,\" , \"longitude\" : \"-122.3248956640928\" , \"time_door_open\" : 0 , \"defrost_cycle\" : 6 } First be sure to set at least the following environment variables in the setenv.sh file MONGO_DB_URL, MONGO_SSL_PEM If not done before or to ensure the mongo connection works fine, run the ReeferRepository.py tool to create the telemetries collection using the following command: ./startPythonEnv.sh IBMCLOUD > python simulator/infrastructure/ReeferRepository.py Generate records in mongodb using SimulationTool Once done run the simulator with the --db argument like below: python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype poweroff --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype co2sensor --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype o2sensor --db Verify data with mongo CLI To verify the data loaded into the database we use mongo CLI with the following command: # to connect mongo -u $USERNAME -p $PASSWORD --tls --tlsCAFile mongodb.pem --authenticationDatabase admin --host replset/1a2ce8ca-<>.bn<>c0.databases.appdomain.cloud:30796 --tlsAllowInvalidCertificates The full host name is masked . The USERNAME and PASSWORD are environment variables you set from the IBM Cloud service credentials. Something like: { \"connection\": { \"cli\": { \"arguments\": [ [ \"-u\", \"ibm_cloud_<>_48b1_b899\", \"-p\", \"3359192a<>e5fbdf51fd573676e58aff4f9e223\", The mongodb.pem is the certificate in text save in a file. To get this certificate you can use the following command: ibmcloud cdb deployment-cacert gse-eda-mongodb > mongodb.pem or by using the IBM Cloud mongodb service user interface (Manage menu): Once connected use the ibmclouddb database with mongo CLI: MongoDB server version: 4.2.0 replset:PRIMARY> use ibmclouddb replset:PRIMARY> db.getCollection(\"telemetries\").find() Delete records in database In mongo CLI do: db.telemetries.deleteMany({}) Next step is to build the model...","title":"Generate telemetry data"},{"location":"collect/generate-telemetry/#generate-telemetry-data","text":"We are using the simulator to generate data. In the industry, when developing new manufactured product, the engineers do not have a lot of data so they also use a mix of real sensors with simulators to create fake but realistic data to develop and test their models. The historical data need to represent failure, and represent the characteristics of a Reefer container. We have define some sensors to get interesting correlated or independant features. As of now our telemetry event structure can be seen in this avro schema . For the machine learning environment we can use csv file as input data or mongodb database or kafka topic. The environment looks like in the figure below: The simulator can run as a standalone tool to create training and test data to be saved in a remote mongodb database or can be used to save to csv file. We use mongodb as a service on IBM cloud. We also use reference data like products and shipping container inventory in Postgresql on IBM Cloud too. The service has credential with URL and SSL certificate.","title":"Generate telemetry data"},{"location":"collect/generate-telemetry/#create-reference-data-in-postgresql","text":"See this separate note for detail.","title":"Create reference data in postgresql"},{"location":"collect/generate-telemetry/#start-python-environment","text":"Use IBMCLOUD if you use mongodb, postgresql and kafka on cloud, or LOCAL for kafka and postgresql running via docker compose. . / startPythonEnv IBMCLOUD or LOCAL or CP root @03721594782f : cd / home In the Dockerfile we set the first PYTHONPATH to /home to specify where python should find the application specifics modules.","title":"Start python environment"},{"location":"collect/generate-telemetry/#generate-data-as-csv-file","text":"","title":"Generate data as csv file"},{"location":"collect/generate-telemetry/#generate-power-off-metrics","text":"When a reefer container loses power, restart and reloose it, it may become an issue. This is the goal of this simulation. The simulator accepts different arguments as specified below usage reefer_simulator-tool --stype [poweroff | co2sensor | o2sensor | normal] --cid [C01 | C02 | C03 | C04 | C05] --product_id [ P01 | P02 | P03 | P04 | P05 ] --records <the number of records to generate> --file <the filename to create> --append --db The cid is for the container id. As the simulator is taking some data from internal datasource you can use only one of those values: [C01 | C02 | C03 | C04 | C05] product_id is also one of the value [ P01 | P02 | P03 | P04 | P05] , as the simulator will derive the target temperature and humidity level from its internal datasource: ('P01','Carrots',1,4,0.4), ('P02','Banana',2,6,0.6), ('P03','Salad',1,4,0.4), ('P04','Avocado',2,6,0.4), ('P05','Tomato',1,6,0.4); --db is when you want to save the telemetry into mongodb DB. In this case be sure to have set the credentials and URL in the scripts/setenv.sh script (see the scripts/setenv-tmp.sh template file) --file is to specify a csv file to write the data --append is used to append the output of this run to an existing file: It permits to accumulate different simulation in the same dataset. (Re)create a new file. It is an important step to get the header as first row. python simulator/reefer_simulator_tool.py --stype poweroff --cid C01 --records 1000 --product_id P02 --file telemetries.csv append to existing file python simulator/reefer_simulator_tool.py --cid C03 --product_id P03 --records 1000 --file telemetries.csv --stype poweroff --append The results looks like: Generating 1000 poweroff metrics Timestamp ID Temperature(celsius) Target_Temperature(celsius) Power PowerConsumption ContentType O2 CO2 Time_Door_Open Maintenance_Required Defrost_Cycle 1.000000 2019-06-30 T15:43 Z 101 3.416766 4 17.698034 6.662044 1 11 1 8.735273 0 6 1.001001 2019-06-30 T15:43 Z 101 4.973630 4 3.701072 8.457314 1 13 3 5.699655 0 6 1.002002 2019-06-30 T15:43 Z 101 1.299275 4 7.629094 From the two previous commands you should have 2001 rows (one for the header which will be used in the model creation): wc -l telemetries.csv 2001 telemetries.csv","title":"Generate power off metrics"},{"location":"collect/generate-telemetry/#generate-co2-sensor-malfunction-in-same-file","text":"In the same way as above the simulator can generate data for Co2 sensor malfunction using the command: python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype co2sensor --append Note The simulator is integrated in the event producer to send real time events to kafka, as if the Reefer container was loaded with fresh goods and is travelling oversea. A consumer code can call the predictive model to assess if maintenance is required and post new event on a containers topic (this consumer code is in the scoring/eventConsumer folder).","title":"Generate Co2 sensor malfunction in same file"},{"location":"collect/generate-telemetry/#generate-o2-sensor-malfunction-in-same-file","text":"python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --file basedata --stype o2sensor --append","title":"Generate O2 sensor malfunction in same file"},{"location":"collect/generate-telemetry/#saving-to-mongodb","text":"The same tool can be used to save to a mongodb database. The document saved in mongo will not be a flat record as produced by the simulator when it creates records to Kafka, but the Json document matches the following format, where sensors are in their own object: { \"timestamp\" : \"2019-09-04 T15:31 Z\" , \"containerID\" : \"C100\" , \"product_id\" : \"P02\" , \"content_type\" : 2 , \"sensors\" : { \"temperature\" : 5.49647 , \"oxygen_level\" : 20.4543 , \"nitrogen_level\" : 79.4046 , \"carbon_dioxide_level\" : 4.42579 , \"humidity_level\" : 60.3148 , \"fan_1\" : \"True\" , \"fan_2\" : \"True\" , \"fan_3\" : \"True\" , \"ambiant_temperature\" : 19.8447 }, \"target_temperature\" : 6.0 , \"kilowatts\" : 3.44686 , \"latitude\" : \"37.8226902168957,\" , \"longitude\" : \"-122.3248956640928\" , \"time_door_open\" : 0 , \"defrost_cycle\" : 6 } First be sure to set at least the following environment variables in the setenv.sh file MONGO_DB_URL, MONGO_SSL_PEM If not done before or to ensure the mongo connection works fine, run the ReeferRepository.py tool to create the telemetries collection using the following command: ./startPythonEnv.sh IBMCLOUD > python simulator/infrastructure/ReeferRepository.py","title":"Saving to Mongodb"},{"location":"collect/generate-telemetry/#generate-records-in-mongodb-using-simulationtool","text":"Once done run the simulator with the --db argument like below: python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype poweroff --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype co2sensor --db python simulator/reefer_simulator_tool.py --cid C03 --product_id P02 --records 1000 --stype o2sensor --db","title":"Generate records in mongodb using SimulationTool"},{"location":"collect/generate-telemetry/#verify-data-with-mongo-cli","text":"To verify the data loaded into the database we use mongo CLI with the following command: # to connect mongo -u $USERNAME -p $PASSWORD --tls --tlsCAFile mongodb.pem --authenticationDatabase admin --host replset/1a2ce8ca-<>.bn<>c0.databases.appdomain.cloud:30796 --tlsAllowInvalidCertificates The full host name is masked . The USERNAME and PASSWORD are environment variables you set from the IBM Cloud service credentials. Something like: { \"connection\": { \"cli\": { \"arguments\": [ [ \"-u\", \"ibm_cloud_<>_48b1_b899\", \"-p\", \"3359192a<>e5fbdf51fd573676e58aff4f9e223\", The mongodb.pem is the certificate in text save in a file. To get this certificate you can use the following command: ibmcloud cdb deployment-cacert gse-eda-mongodb > mongodb.pem or by using the IBM Cloud mongodb service user interface (Manage menu): Once connected use the ibmclouddb database with mongo CLI: MongoDB server version: 4.2.0 replset:PRIMARY> use ibmclouddb replset:PRIMARY> db.getCollection(\"telemetries\").find()","title":"Verify data with mongo CLI"},{"location":"collect/generate-telemetry/#delete-records-in-database","text":"In mongo CLI do: db.telemetries.deleteMany({}) Next step is to build the model...","title":"Delete records in database"},{"location":"collect/mongodb-telemetries/","text":"Work with Mongodb to persist telemetries MongoDB is a popular document-based database that allows developers to quickly build projects without worrying about schema. Mongo components include: mongod - the core process that runs the actual database mongos - controls routing to the databases in case of sharding config servers (CSRS) - stores the metadata in a sharded environment. With mongodb on IBM Cloud Create the service credentials and the mongodb.composed url, something starting as mongodb://ibm_cloud_e154ff52_ed Set this URL in setenv.sh the MONGO_DB_URL=\"mongodb://ibm_c...\" Get the certificate as pem file ibmcloud cdb deployment-cacert gse-eda-mongodb > mongodbca.pem Use the python pymongo driver and open a connection with code like below: URL = os . getenv ( 'MONGO_DB_URL' ) client = MongoClient ( URL , ssl = True , ssl_ca_certs = '/home/mongodb.pem' ) db = client . ibmclouddb # insert a record result = db . telemetries . insert_one ( telemetry ) telemetry = db . telemetries . find_one ({ \"_id\" : ObjectId ( result . inserted_id )}) # get all the records telemetries = db . telemetries . find () for t in telemetries : See the rest of the code in ml/data/ToMongo.py . As a telemetry repository for the simulator The approach is to use the same APIs, but get the pem file in the docker image and set the environment variables accordingly. The ReeferRepository.py implements the mongodb operations to save telemetry. On Openshift 3.11 on premise We use the following image: centos/mongodb-36-centos7 . So to install it, we use the following command: oc new-app -e \\ MONGODB_USER=mongo,MONGODB_PASSWORD=<password>,MONGODB_DATABASE=reeferdb,MONGODB_ADMIN_PASSWORD=<admin_password> \\ centos/mongodb-36-centos7 Connect to the pod and then use the mongo CLI $ oc get pods NAME READY STATUS RESTARTS AGE mongodb-36-centos7-1-wcn7h 1 /1 Running 0 4d $ oc rsh mongodb-36-centos7-1-wcn7h bash-4.2$ mongo -u $MONGODB_USER -p $MONGODB_PASSWORD $MONGODB_DATABASE MongoDB shell version: 2 .4.9 connecting to: reeferdb > show collections To remove the db on openshift: oc delete dc","title":"Work with Mongodb to persist telemetries"},{"location":"collect/mongodb-telemetries/#work-with-mongodb-to-persist-telemetries","text":"MongoDB is a popular document-based database that allows developers to quickly build projects without worrying about schema. Mongo components include: mongod - the core process that runs the actual database mongos - controls routing to the databases in case of sharding config servers (CSRS) - stores the metadata in a sharded environment.","title":"Work with Mongodb to persist telemetries"},{"location":"collect/mongodb-telemetries/#with-mongodb-on-ibm-cloud","text":"Create the service credentials and the mongodb.composed url, something starting as mongodb://ibm_cloud_e154ff52_ed Set this URL in setenv.sh the MONGO_DB_URL=\"mongodb://ibm_c...\" Get the certificate as pem file ibmcloud cdb deployment-cacert gse-eda-mongodb > mongodbca.pem Use the python pymongo driver and open a connection with code like below: URL = os . getenv ( 'MONGO_DB_URL' ) client = MongoClient ( URL , ssl = True , ssl_ca_certs = '/home/mongodb.pem' ) db = client . ibmclouddb # insert a record result = db . telemetries . insert_one ( telemetry ) telemetry = db . telemetries . find_one ({ \"_id\" : ObjectId ( result . inserted_id )}) # get all the records telemetries = db . telemetries . find () for t in telemetries : See the rest of the code in ml/data/ToMongo.py .","title":"With mongodb on IBM Cloud"},{"location":"collect/mongodb-telemetries/#as-a-telemetry-repository-for-the-simulator","text":"The approach is to use the same APIs, but get the pem file in the docker image and set the environment variables accordingly. The ReeferRepository.py implements the mongodb operations to save telemetry.","title":"As a telemetry repository for the simulator"},{"location":"collect/mongodb-telemetries/#on-openshift-311-on-premise","text":"We use the following image: centos/mongodb-36-centos7 . So to install it, we use the following command: oc new-app -e \\ MONGODB_USER=mongo,MONGODB_PASSWORD=<password>,MONGODB_DATABASE=reeferdb,MONGODB_ADMIN_PASSWORD=<admin_password> \\ centos/mongodb-36-centos7 Connect to the pod and then use the mongo CLI $ oc get pods NAME READY STATUS RESTARTS AGE mongodb-36-centos7-1-wcn7h 1 /1 Running 0 4d $ oc rsh mongodb-36-centos7-1-wcn7h bash-4.2$ mongo -u $MONGODB_USER -p $MONGODB_PASSWORD $MONGODB_DATABASE MongoDB shell version: 2 .4.9 connecting to: reeferdb > show collections To remove the db on openshift: oc delete dc","title":"On Openshift 3.11 on premise"},{"location":"collect/oss-collect-data/","text":"As part of the IBM AI ladder practice introduced in the Data AI reference architecture and specially the collect step, we need to get a data topology in place to get data at rest so data scientist can do their data analysis, and feature preparation. In this solution, there are two datasources: the events in the kafka topic, using the event sourcing design pattern . the database about the Reefer, the fresh products and reefer telemetries As we do not have Reefer telemetry public data available, we are using our simulator to develop such data. The figure below illustrates this data injection simulation. Also you can use the simulator to create data in csv file, so there is no need to use postgresql to develop the model.","title":"Collect data with open source solution"},{"location":"collect/products-postgres/","text":"Define the products data into postgresql We are using a Postgresql instance on IBM Cloud. So once you create your own instance get the credential for the host, user, password and the certificate. \"host=bd2d0216-0b7d-4575-8c0b-d2e934843e41.6131b73286f34215871dfad7254b4f7d.databases.appdomain.cloud port=31384 dbname=ibmclouddb user=ibm_cloud_c958... \"PGPASSWORD\": \"2d1c526.....3\" First be sure to set at least the following environment variables in the setenv.sh file POSTGRES_URL, POSTGRES_DBNAME, If you use POSTGRESQL on IBM Cloud or any deployment using SSL, you need to get the SSL certificate as postgres.pem file, or set POSTGRES_SSL_PEM variable to the path where to find this file. Populate the products data with the Simulator repository The simulator code includes the infrastructure/ProductRepository.py that can create tables and add some product definitions inside the table. The following command using docker and python interpreter can create the database: ./scripts/createPGTables.sh IBMCLOUD And alternate techniques is to use psql Connect with psql We use a docker image to run psql: # under the data_schema/rdbms/postgresql folder $ ./startPsql.sh IBMCLOUD $ PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = > List relations... ibmclouddb => \\d Then create table if not done before ibmclouddb => CREATE TABLE products ( product_id varchar(64) NOT NULL PRIMARY KEY, description varchar(100), target_temperature REAL, target_humidity_level REAL ); Populate the data: ibmclouddb => INSERT INTO products(product_id,description,target_temperature,target_humidity_level) VALUES ('P01','Carrots',4,0.4), ('P02','Banana',6,0.6), ('P03','Salad',4,0.4), ('P04','Avocado',6,0.4), ('P05','Tomato',4,0.4); List the products SELECT * FROM products; You should see: product_id | description | target_temperature | target_humidity_level | content_type ------------+-------------+--------------------+-----------------------+-------------- P01 | Carrots | 4 | 0.4 | 1 P02 | Banana | 6 | 0.6 | 2 P03 | Salad | 4 | 0.4 | 1 P04 | Avocado | 6 | 0.4 | 2 P05 | Tomato | 6 | 0.3 | 6 Quit ibmclouddb => \\q","title":"Get products into postgresql"},{"location":"collect/products-postgres/#define-the-products-data-into-postgresql","text":"We are using a Postgresql instance on IBM Cloud. So once you create your own instance get the credential for the host, user, password and the certificate. \"host=bd2d0216-0b7d-4575-8c0b-d2e934843e41.6131b73286f34215871dfad7254b4f7d.databases.appdomain.cloud port=31384 dbname=ibmclouddb user=ibm_cloud_c958... \"PGPASSWORD\": \"2d1c526.....3\" First be sure to set at least the following environment variables in the setenv.sh file POSTGRES_URL, POSTGRES_DBNAME, If you use POSTGRESQL on IBM Cloud or any deployment using SSL, you need to get the SSL certificate as postgres.pem file, or set POSTGRES_SSL_PEM variable to the path where to find this file.","title":"Define the products data into postgresql"},{"location":"collect/products-postgres/#populate-the-products-data-with-the-simulator-repository","text":"The simulator code includes the infrastructure/ProductRepository.py that can create tables and add some product definitions inside the table. The following command using docker and python interpreter can create the database: ./scripts/createPGTables.sh IBMCLOUD And alternate techniques is to use psql","title":"Populate the products data with the Simulator repository"},{"location":"collect/products-postgres/#connect-with-psql","text":"We use a docker image to run psql: # under the data_schema/rdbms/postgresql folder $ ./startPsql.sh IBMCLOUD $ PGPASSWORD = $POSTGRES_PWD psql --host = $HOST --port = $PORT --username = $POSTGRES_USER --dbname = $POSTGRES_DB ibmclouddb = >","title":"Connect with psql"},{"location":"collect/products-postgres/#list-relations","text":"ibmclouddb => \\d Then create table if not done before ibmclouddb => CREATE TABLE products ( product_id varchar(64) NOT NULL PRIMARY KEY, description varchar(100), target_temperature REAL, target_humidity_level REAL ); Populate the data: ibmclouddb => INSERT INTO products(product_id,description,target_temperature,target_humidity_level) VALUES ('P01','Carrots',4,0.4), ('P02','Banana',6,0.6), ('P03','Salad',4,0.4), ('P04','Avocado',6,0.4), ('P05','Tomato',4,0.4); List the products SELECT * FROM products; You should see: product_id | description | target_temperature | target_humidity_level | content_type ------------+-------------+--------------------+-----------------------+-------------- P01 | Carrots | 4 | 0.4 | 1 P02 | Banana | 6 | 0.6 | 2 P03 | Salad | 4 | 0.4 | 1 P04 | Avocado | 6 | 0.4 | 2 P05 | Tomato | 6 | 0.3 | 6 Quit ibmclouddb => \\q","title":"List relations..."},{"location":"infuse/bpm/","text":"Maintenance field engineer dispatching business process Need to cover: problem statement business process model deploy to BPM on cloud service end point explanation specific logic in the container microservice demonstration script for a BPM point of view Problem statement When the anomaly detection scoring service create a maintenance record on the containers topic, the container microservice will trigger a field engineer dispatching case so an engineer can go the reefer container if it was unloaded in the destination harbor. Before creating a ticket or while adding information to the ticket, the process may get the container estimated arrival time and destination harbor to do the dispatching. This mean adding an API on container microservice.","title":"Engineer dispatching with Business process with Cloud Pak for Automation"},{"location":"infuse/bpm/#maintenance-field-engineer-dispatching-business-process","text":"Need to cover: problem statement business process model deploy to BPM on cloud service end point explanation specific logic in the container microservice demonstration script for a BPM point of view","title":"Maintenance field engineer dispatching business process"},{"location":"infuse/bpm/#problem-statement","text":"When the anomaly detection scoring service create a maintenance record on the containers topic, the container microservice will trigger a field engineer dispatching case so an engineer can go the reefer container if it was unloaded in the destination harbor. Before creating a ticket or while adding information to the ticket, the process may get the container estimated arrival time and destination harbor to do the dispatching. This mean adding an API on container microservice.","title":"Problem statement"},{"location":"infuse/build-run/","text":"Development approach The app is done using Flask, and the code is generated using Appsody template and adapted. An alternate approach is to setup a CI/CD pipeline We have adopted the Git Action to manage the continuous integration , and ArgoCD for the continuous deployment. The build process will build the following images: [https://hub.docker.com/repository/docker/ibmcase/kcontainer-reefer-simulator] Helm charts are added for the simulator and the scoring agent, using helm create command, and then the values.yaml and deployment.yaml files were updated to set environment variables and other parameters. Test sending a simulation control to the POST api The script sendSimulControl.sh is used for that. The usage looks like: sendSimulControl.sh hostname simultype (co2sensor | o2sensor | poweroff) containerID nb_of_records pwd refarch-reefer-ml ./scripts/sendSimulControl.sh reefersimulatorroute-reefershipmentsolution.apps.green-with-envy.ocp.csplab.local co2sensor C01 2000 If you use no argument for this script, it will send co2sensor control to the service running on our openshift cluster on IBM Cloud. Looking at the logs from the pod using oc logs reefersimulator-3-jdh2v you can see something like: \"POST /order HTTP/1.1\" 404 232 \"-\" \"curl/7.54.0\" {'containerID': 'c100', 'simulation': 'co2sensor', 'nb_of_records': 10, 'good_temperature': 4.4} Generating 10 Co2 metrics We will see how those events are processed in the next section. The predictive scoring agent Applying the same pattern as the simulation webapp, we implement a kafka consumer and producer in python that calls the serialized analytical model. The code in the scoring\\eventConsumer folder. Applying a TDD approach we start by a TestScoring.py class. import unittest from domain.predictservice import PredictService class TestScoreMetric ( unittest . TestCase ): def testCreation ( self ): serv = PredictService if __name__ == '__main__' : unittest . main () Use the same python environment with docker: . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / scoring / eventConsumer root @1 de81b16f940 : / # cd / home / scoring / eventConsumer root @1 de81b16f940 : / home / scoring / eventConsumer # python tests / TestScoring . py Test fails, so let add the scoring service with a constructor, and load the serialized pickle model (which was copied from the ml folder). import pickle class PredictService : def __init__ ( self , filename = \"domain/model_logistic_regression.pkl\" ): self . model = pickle . load ( open ( filename , \"rb\" ), encoding = 'latin1' ) def predict ( self , metricEvent ): TESTDATA = StringIO ( metricEvent ) data = pd . read_csv ( TESTDATA , sep = \",\" ) data . columns = data . columns . to_series () . apply ( lambda x : x . strip ()) X = data [ X = data [ FEATURES_NAMES ]] return self . model . predict ( X ) Next we need to test a predict on an event formated as a csv string. The test looks like: serv = PredictService() header=\"\"\"Timestamp, ID, Temperature(celsius), Target_Temperature(celsius), Power, PowerConsumption, ContentType, O2, CO2, Time_Door_Open, Maintenance_Required, Defrost_Cycle\"\"\" event=\"2019-04-01 T16:29 Z,1813, 101, 4.291843460900875,4.4,0,10.273342381017777,3,4334.920958996634,4.9631508046318755,1,0,6\"\"\" record=header+\"\\n\"+event print(serv.predict(record)) So the scoring works, now we need to code the scoring application that will be deployed to Openshift cluster, and which acts as a consumer of container metrics events and a producer container events. The Scoring Agent code of this app is ScoringAgent.py module. It starts a consumer to get messages from Kafka. And when a message is received, it needs to do some data extraction and transformation and then use the predictive service. During the tests we have issue in the data quality, so it is always a good practice to add a validation function to assess if all the records are good. For production, this code needs to be enhanced for better error handling an reporting. Run locally Under scoring\\eventConsumer folder, set the environment variables for KAFKA using the commands below: (It uses event streams on IBM Cloud) export KAFKA_BROKERS=broker-3.eventstreams.cloud.ibm.com:9093,broker-1.eventstreams.cloud.ibm.com:9093,broker-0.eventstreams.cloud.ibm.com:9093,broker-5.eventstreams.cloud.ibm.com:9093,broker-2.eventstreams.cloud.ibm.com:9093,broker-4.eventstreams.cloud.ibm.com:9093 export KAFKA_APIKEY=\"set-api-key-for-eventstreams-on-cloud\" docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -e KAFKA_APIKEY=$KAFKA_APIKEY -v $(pwd)/..:/home -ti ibmcase/python bash -c \"cd /home/scoring && export PYTHONPATH=/home && python ScoringAgent.py\" Scoring: Build and run on Openshift The first time we need to add the application to the existing project, run the following command: oc new-app python:latest~https://github.com/ibm-cloud-architecture/refarch-reefer-ml.git --context-dir=scoring/eventConsumer --name reeferpredictivescoring This command will run a source to image, build all the needed yaml files for the kubernetes deployment and start the application in a pod. It use the --context flag to define what to build and run. With this capability we can use the same github repository for different sub component. As done for simulator, the scoring service needs environment variables. We can set them using the commands oc set env dc/reeferpredictivescoring KAFKA_BROKERS=$KAFKA_BROKERS oc set env dc/reeferpredictivescoring KAFKA_APIKEY=$KAFKA_APIKEY oc set env dc/reeferpredictivescoring KAFKA_CERT=/opt/app-root/src/es-cert.pem but we have added a script for you to do so. This script needs only to be run at the first deployment. It leverage the common setenv scripts: ../scripts/defEnvVarInOpenShift.sh The list of running pods should show the build pods for this application: oc get pods reeferpredictivescoring-1-build 1/1 Running 0 24s To run the build again after commit code to github: oc start-build reeferpredictivescoring # or from local file system oc start-build reeferpredictivescoring --from-file=. To see the log: oc logs reeferpredictivescoring-2-rxr6j To be able to run on Openshift, the APP_FILE environment variable has to be set to ScoringApp.py. This can be done in the environment file under the .s2i folder. The scoring service has no API exposed to the external world, so we do not need to create a Route or ingress. See the integration test section to see a demonstration of the solution end to end. Build docker images For the scoring agent: # scoring folder Run kafka on your laptop For development purpose, you can also run kafka, zookeeper and postgresql and the solution on your laptop. For that read this readme for details.","title":"Build and run the solution on Openshift"},{"location":"infuse/build-run/#development-approach","text":"The app is done using Flask, and the code is generated using Appsody template and adapted.","title":"Development approach"},{"location":"infuse/build-run/#an-alternate-approach-is-to-setup-a-cicd-pipeline","text":"We have adopted the Git Action to manage the continuous integration , and ArgoCD for the continuous deployment. The build process will build the following images: [https://hub.docker.com/repository/docker/ibmcase/kcontainer-reefer-simulator] Helm charts are added for the simulator and the scoring agent, using helm create command, and then the values.yaml and deployment.yaml files were updated to set environment variables and other parameters.","title":"An alternate approach is to setup a CI/CD pipeline"},{"location":"infuse/build-run/#test-sending-a-simulation-control-to-the-post-api","text":"The script sendSimulControl.sh is used for that. The usage looks like: sendSimulControl.sh hostname simultype (co2sensor | o2sensor | poweroff) containerID nb_of_records pwd refarch-reefer-ml ./scripts/sendSimulControl.sh reefersimulatorroute-reefershipmentsolution.apps.green-with-envy.ocp.csplab.local co2sensor C01 2000 If you use no argument for this script, it will send co2sensor control to the service running on our openshift cluster on IBM Cloud. Looking at the logs from the pod using oc logs reefersimulator-3-jdh2v you can see something like: \"POST /order HTTP/1.1\" 404 232 \"-\" \"curl/7.54.0\" {'containerID': 'c100', 'simulation': 'co2sensor', 'nb_of_records': 10, 'good_temperature': 4.4} Generating 10 Co2 metrics We will see how those events are processed in the next section.","title":"Test sending a simulation control to the POST api"},{"location":"infuse/build-run/#the-predictive-scoring-agent","text":"Applying the same pattern as the simulation webapp, we implement a kafka consumer and producer in python that calls the serialized analytical model. The code in the scoring\\eventConsumer folder. Applying a TDD approach we start by a TestScoring.py class. import unittest from domain.predictservice import PredictService class TestScoreMetric ( unittest . TestCase ): def testCreation ( self ): serv = PredictService if __name__ == '__main__' : unittest . main () Use the same python environment with docker: . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / scoring / eventConsumer root @1 de81b16f940 : / # cd / home / scoring / eventConsumer root @1 de81b16f940 : / home / scoring / eventConsumer # python tests / TestScoring . py Test fails, so let add the scoring service with a constructor, and load the serialized pickle model (which was copied from the ml folder). import pickle class PredictService : def __init__ ( self , filename = \"domain/model_logistic_regression.pkl\" ): self . model = pickle . load ( open ( filename , \"rb\" ), encoding = 'latin1' ) def predict ( self , metricEvent ): TESTDATA = StringIO ( metricEvent ) data = pd . read_csv ( TESTDATA , sep = \",\" ) data . columns = data . columns . to_series () . apply ( lambda x : x . strip ()) X = data [ X = data [ FEATURES_NAMES ]] return self . model . predict ( X ) Next we need to test a predict on an event formated as a csv string. The test looks like: serv = PredictService() header=\"\"\"Timestamp, ID, Temperature(celsius), Target_Temperature(celsius), Power, PowerConsumption, ContentType, O2, CO2, Time_Door_Open, Maintenance_Required, Defrost_Cycle\"\"\" event=\"2019-04-01 T16:29 Z,1813, 101, 4.291843460900875,4.4,0,10.273342381017777,3,4334.920958996634,4.9631508046318755,1,0,6\"\"\" record=header+\"\\n\"+event print(serv.predict(record)) So the scoring works, now we need to code the scoring application that will be deployed to Openshift cluster, and which acts as a consumer of container metrics events and a producer container events. The Scoring Agent code of this app is ScoringAgent.py module. It starts a consumer to get messages from Kafka. And when a message is received, it needs to do some data extraction and transformation and then use the predictive service. During the tests we have issue in the data quality, so it is always a good practice to add a validation function to assess if all the records are good. For production, this code needs to be enhanced for better error handling an reporting.","title":"The predictive scoring agent"},{"location":"infuse/build-run/#run-locally","text":"Under scoring\\eventConsumer folder, set the environment variables for KAFKA using the commands below: (It uses event streams on IBM Cloud) export KAFKA_BROKERS=broker-3.eventstreams.cloud.ibm.com:9093,broker-1.eventstreams.cloud.ibm.com:9093,broker-0.eventstreams.cloud.ibm.com:9093,broker-5.eventstreams.cloud.ibm.com:9093,broker-2.eventstreams.cloud.ibm.com:9093,broker-4.eventstreams.cloud.ibm.com:9093 export KAFKA_APIKEY=\"set-api-key-for-eventstreams-on-cloud\" docker run -e KAFKA_BROKERS=$KAFKA_BROKERS -e KAFKA_APIKEY=$KAFKA_APIKEY -v $(pwd)/..:/home -ti ibmcase/python bash -c \"cd /home/scoring && export PYTHONPATH=/home && python ScoringAgent.py\"","title":"Run locally"},{"location":"infuse/build-run/#scoring-build-and-run-on-openshift","text":"The first time we need to add the application to the existing project, run the following command: oc new-app python:latest~https://github.com/ibm-cloud-architecture/refarch-reefer-ml.git --context-dir=scoring/eventConsumer --name reeferpredictivescoring This command will run a source to image, build all the needed yaml files for the kubernetes deployment and start the application in a pod. It use the --context flag to define what to build and run. With this capability we can use the same github repository for different sub component. As done for simulator, the scoring service needs environment variables. We can set them using the commands oc set env dc/reeferpredictivescoring KAFKA_BROKERS=$KAFKA_BROKERS oc set env dc/reeferpredictivescoring KAFKA_APIKEY=$KAFKA_APIKEY oc set env dc/reeferpredictivescoring KAFKA_CERT=/opt/app-root/src/es-cert.pem but we have added a script for you to do so. This script needs only to be run at the first deployment. It leverage the common setenv scripts: ../scripts/defEnvVarInOpenShift.sh The list of running pods should show the build pods for this application: oc get pods reeferpredictivescoring-1-build 1/1 Running 0 24s To run the build again after commit code to github: oc start-build reeferpredictivescoring # or from local file system oc start-build reeferpredictivescoring --from-file=. To see the log: oc logs reeferpredictivescoring-2-rxr6j To be able to run on Openshift, the APP_FILE environment variable has to be set to ScoringApp.py. This can be done in the environment file under the .s2i folder. The scoring service has no API exposed to the external world, so we do not need to create a Route or ingress. See the integration test section to see a demonstration of the solution end to end.","title":"Scoring: Build and run on Openshift"},{"location":"infuse/build-run/#build-docker-images","text":"For the scoring agent: # scoring folder","title":"Build docker images"},{"location":"infuse/build-run/#run-kafka-on-your-laptop","text":"For development purpose, you can also run kafka, zookeeper and postgresql and the solution on your laptop. For that read this readme for details.","title":"Run kafka on your laptop"},{"location":"infuse/dev-scoring/","text":"Develop the scoring agent with cloud pak for application The scoring service needs to use an analytics scoring model built using a machine learning techniques, and serialized so that it can be loaded in memory. Using appsody... Further Readings Cloud Pak for Application demo video","title":"Develop scoring app with cloud pak for application"},{"location":"infuse/dev-scoring/#develop-the-scoring-agent-with-cloud-pak-for-application","text":"The scoring service needs to use an analytics scoring model built using a machine learning techniques, and serialized so that it can be loaded in memory. Using appsody...","title":"Develop the scoring agent with cloud pak for application"},{"location":"infuse/dev-scoring/#further-readings","text":"Cloud Pak for Application demo video","title":"Further Readings"},{"location":"infuse/integration-tests/","text":"Integration tests to proof the solution Recall that the architecture of the deployed components look like in the figure below: So the first component to start is the container consumer which consumes events from the kafka containers topic. This topic is where the microservices will post messages about a Reefer container. It is used by this microservice already: Reefer container manager . Pre-requisites Be sure to have set the environment variables in the ./scripts/setenv.sh to point to your Event Stream or Kafka deployment. You need to start four terminal windows if you run the solution locally on you laptop, and only 2 terminals if you run the solution on our deployed cluster. Note Our deployed cluster in on IBM Cloud Openshift 3.11 cluster. Start Reefer container events consumer In the consumer folder use the command: ./runContainerConsumer.sh This script starts the docker python image, we built earlier and use the ConsumeContainers.py module. Start the predictive scoring service We can run it locally or on kubernetes cluster like Openshift. Under scoring folder, use the command: ./runScoringApp.sh In the beginning of the trace log you should see the bootstrap.servers brokers list, the group.id , and api key as sasl.password . Recalls the scoring is a producer and a consumer. See the build and run on Openshift section for running on kubernetes cluster. Start the simulator web app Under the simulator folder ./runReeferSimulator.sh To build and run it on Openshift review this section . Start a simulation Under the scripts folder ./sendSimulControl.sh Validate integration tests To test your local deployment ./sendSimulControl.sh localhost:8080 poweroff or to test on our cloud based deployed solution ./sendSimulControl.sh The traces will look like these: Simulator trace The trace from the pod demonstrate the configuration and the control message received at the POST operation, and then the event generated. {'bootstrap.servers': 'broker-3-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-1-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-0-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-5-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-2-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-4-\"<hidden-part>.eventstreams.cloud.ibm.com:9093', 'group.id': 'ReeferMetricsSimulator', 'security.protocol': 'sasl_ssl', 'sasl.mechanisms': 'PLAIN', 'ssl.ca.location': '/etc/pki/tls/cert.pem', 'sasl.username': 'token', 'sasl.password': '<hidden-part>'} ... {'containerID': 'c101', 'simulation': 'poweroff', 'nb_of_records': 50, 'good_temperature': 4.4} Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859800, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:50 Z', 'c101', 2.0905792037649547, 4.4, 16.282392569138707, 6.603341673152029, 2, 16, 8.827184272293419, 6.33603138958275, 0, 5)\"} Message delivered to containerMetrics [0] Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859860, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:51 Z', 'c101', 2.0905792037649547, 4.4, 0, -0.04371530981778182, 2, 3, 6.295683442800409, 5.36863196753292, 0, 1)\"} Message delivered to containerMetrics [0] ... Scoring trace Container consumer trace @@@ poll next container from containers partition: [0] at offset 3 with key b'c100': value: {\"timestamp\": 1566854815, \"type\": \"ContainerMaintenance\", \"version\": \"1\", \"containerID\": \"c100\", \"payload\": {\"containerID\": \"c100\", \"type\": \"Reefer\", \"status\": \"MaintenanceNeeded\", \"Reason\": \"Predictive maintenance scoring found a risk of failure\"}}","title":"Run integration tests"},{"location":"infuse/integration-tests/#integration-tests-to-proof-the-solution","text":"Recall that the architecture of the deployed components look like in the figure below: So the first component to start is the container consumer which consumes events from the kafka containers topic. This topic is where the microservices will post messages about a Reefer container. It is used by this microservice already: Reefer container manager .","title":"Integration tests to proof the solution"},{"location":"infuse/integration-tests/#pre-requisites","text":"Be sure to have set the environment variables in the ./scripts/setenv.sh to point to your Event Stream or Kafka deployment. You need to start four terminal windows if you run the solution locally on you laptop, and only 2 terminals if you run the solution on our deployed cluster. Note Our deployed cluster in on IBM Cloud Openshift 3.11 cluster.","title":"Pre-requisites"},{"location":"infuse/integration-tests/#start-reefer-container-events-consumer","text":"In the consumer folder use the command: ./runContainerConsumer.sh This script starts the docker python image, we built earlier and use the ConsumeContainers.py module.","title":"Start Reefer container events consumer"},{"location":"infuse/integration-tests/#start-the-predictive-scoring-service","text":"We can run it locally or on kubernetes cluster like Openshift. Under scoring folder, use the command: ./runScoringApp.sh In the beginning of the trace log you should see the bootstrap.servers brokers list, the group.id , and api key as sasl.password . Recalls the scoring is a producer and a consumer. See the build and run on Openshift section for running on kubernetes cluster.","title":"Start the predictive scoring service"},{"location":"infuse/integration-tests/#start-the-simulator-web-app","text":"Under the simulator folder ./runReeferSimulator.sh To build and run it on Openshift review this section .","title":"Start the simulator web app"},{"location":"infuse/integration-tests/#start-a-simulation","text":"Under the scripts folder ./sendSimulControl.sh","title":"Start a simulation"},{"location":"infuse/integration-tests/#validate-integration-tests","text":"To test your local deployment ./sendSimulControl.sh localhost:8080 poweroff or to test on our cloud based deployed solution ./sendSimulControl.sh The traces will look like these:","title":"Validate integration tests"},{"location":"infuse/integration-tests/#simulator-trace","text":"The trace from the pod demonstrate the configuration and the control message received at the POST operation, and then the event generated. {'bootstrap.servers': 'broker-3-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-1-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-0-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-5-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-2-\"<hidden-part>.eventstreams.cloud.ibm.com:9093,broker-4-\"<hidden-part>.eventstreams.cloud.ibm.com:9093', 'group.id': 'ReeferMetricsSimulator', 'security.protocol': 'sasl_ssl', 'sasl.mechanisms': 'PLAIN', 'ssl.ca.location': '/etc/pki/tls/cert.pem', 'sasl.username': 'token', 'sasl.password': '<hidden-part>'} ... {'containerID': 'c101', 'simulation': 'poweroff', 'nb_of_records': 50, 'good_temperature': 4.4} Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859800, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:50 Z', 'c101', 2.0905792037649547, 4.4, 16.282392569138707, 6.603341673152029, 2, 16, 8.827184272293419, 6.33603138958275, 0, 5)\"} Message delivered to containerMetrics [0] Reefer contaimer metric event to send:{\"containerID\": \"c101\", \"timestamp\": 1566859860, \"type\": \"ContainerMetric\", \"payload\": \"('2019-08-26 T22:51 Z', 'c101', 2.0905792037649547, 4.4, 0, -0.04371530981778182, 2, 3, 6.295683442800409, 5.36863196753292, 0, 1)\"} Message delivered to containerMetrics [0] ...","title":"Simulator trace"},{"location":"infuse/integration-tests/#scoring-trace","text":"","title":"Scoring trace"},{"location":"infuse/integration-tests/#container-consumer-trace","text":"@@@ poll next container from containers partition: [0] at offset 3 with key b'c100': value: {\"timestamp\": 1566854815, \"type\": \"ContainerMaintenance\", \"version\": \"1\", \"containerID\": \"c100\", \"payload\": {\"containerID\": \"c100\", \"type\": \"Reefer\", \"status\": \"MaintenanceNeeded\", \"Reason\": \"Predictive maintenance scoring found a risk of failure\"}}","title":"Container consumer trace"},{"location":"infuse/simul-app/","text":"The Simulator as web app The Simulator webapp is a simple python (3.7) Flask web app exposing a REST POST end point to control the type of simulation and as effect it is producing Reefer telemetry events to kafka reeferTelemetry topic. The POST operation in on the /control url and send a jsong control object, like: { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" } Design approach We have tried to support a domain driven design approach to structure the code, with domain, infrastructure and app modules. The domain module has a unique class for the simulator which main goals is to generate tuples or records for the different simulation types. It is reused for the standalone simulation tools to generate data at rest. As the simulator is also a webapp we need to package it with Flask and run it using one of the Web Server Gateway Interface (WSGI) implementation with Gunicorn . We recommend to follow Flask tutorial if you do not know this python library to develop web app or REST service. Flask is a simple library to implement REST based microservice and web application in python. It has other related projects to add interesting features to develop production application. The standard development includes defining routes, function to support handling the request and generating HTTP response, but also defining APIs... Read more with the explore Flask book online . Flask is mono threaded so it fits well in a simple web application for development purpose, but for production it is recommended to add a web server like Gunicorn to handle multiple concurrent requests. explain swaggers explain blueprint The pipfile defines the dependencies for this component. Testing Unit test the Simulator The test coverage is not yet great. To run them cd simulator . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / simulator root @1 de81b16f940 : / # cd / home / simulator root @1 de81b16f940 : / # python tests / unit / TestSimulator . py Build We have multiple build approaches depending of the environment: local, openshift source to image, github actions. Build locally with docker The goal is to build the docker image, publish it to docker hub registry or a private registry, which then can be deployed to any kubernetes environment or run using docker compose: # In simulator folder docker build -t ibmcase/kcontainer-reefer-simulator . docker login docker push ibmcase/kcontainer-reefer-simulator > The push refers to repository [ docker.io/ibmcase/kcontainer-reefer-simulator ] The scripts: scripts/runReeferSimulatorApp.sh IBMCLOUD should run the simulator docker image with Event Streams on IBM Cloud. Openshift source to image To build and deploy the code to an OpenShift cluster, using the source 2 image approach, do the following steps: Login to the OpenShift cluster. oc login -u apikey -p <apikey> --server=https://... To find the API key and server URL go to the openshift console under your account, to access the Copy login command menu. Create a project if you did not create one already: oc new-project reefershipmentsolution --description=\"A Reefer container shipment solution\" Remember the project is mapped to a kubernetes namespace, but includes other components too The first deploy you need to create a new app from the source code, and use source to image build process to deploy the app. You can use a subdirectory of your source code repository by specifying a --context-dir flag. oc new-app python:latest~https://github.com/ibm-cloud-architecture/refarch-reefer-ml.git --context-dir=simulator --name reefersimulator Then to track the build progress, look at the logs of the build pod: oc logs -f bc/reefersimulator The dependencies are loaded, the build is scheduled and executed, the image is uploaded to the registry, and started. To display information about the build configuration for the application: oc describe bc/reefersimulator When you want to redeploy, trigger a remote build (run on Openshift) from local source code do the following command: oc start-build reefersimulator --from-file=. Set environment variables For Broker URLs oc set env dc/reefersimulator KAFKA_BROKERS=kafka03-prod02.messagehub.services.us-south.blu.... For apikey: oc set env dc/reefersimulator KAFKA_APIKEY=\"\" If you connect to event stream or kafka with SSL specify where to find the SSL certificate: oc set env dc/reefersimulator KAFKA_CERT=\"/opt/app-root/src/es-cert.pem\" Get all environment variables set for a given pod: (get the pod id with oc get pod ) oc exec reefersimulator-31-2kdv5 env Once the build is done you should see the container up and running oc get pod reefersimulator-3-build 0/1 Completed 0 15m reefersimulator-3-jdh2v 1/1 Running 0 1m Note The first time the container start, it may crash as the environment variables like KAFKA_APIKEY is not defined. You can use the ./scripts/defEnvVarInOpenShift.sh command to create the needed environment variables. To make the webapp visible externally to the cluster, you need to add a route for this deployment. Login to the admin console and use Create Route button on top right of the screen, Then enter a name and select the existing service Once created, the URL of the app is visible in the route list panel: Add the host name in your local /etc/hosts or be sure the hostname is defined in DNS server. Map to the IP address of the kubernetes proxy server end point.","title":"Develop the simulation app"},{"location":"infuse/simul-app/#the-simulator-as-web-app","text":"The Simulator webapp is a simple python (3.7) Flask web app exposing a REST POST end point to control the type of simulation and as effect it is producing Reefer telemetry events to kafka reeferTelemetry topic. The POST operation in on the /control url and send a jsong control object, like: { 'containerID': 'C02', 'simulation': 'co2sensor', 'nb_of_records': 1000, \"product_id\" : \"P02\" }","title":"The Simulator as web app"},{"location":"infuse/simul-app/#design-approach","text":"We have tried to support a domain driven design approach to structure the code, with domain, infrastructure and app modules. The domain module has a unique class for the simulator which main goals is to generate tuples or records for the different simulation types. It is reused for the standalone simulation tools to generate data at rest. As the simulator is also a webapp we need to package it with Flask and run it using one of the Web Server Gateway Interface (WSGI) implementation with Gunicorn . We recommend to follow Flask tutorial if you do not know this python library to develop web app or REST service. Flask is a simple library to implement REST based microservice and web application in python. It has other related projects to add interesting features to develop production application. The standard development includes defining routes, function to support handling the request and generating HTTP response, but also defining APIs... Read more with the explore Flask book online . Flask is mono threaded so it fits well in a simple web application for development purpose, but for production it is recommended to add a web server like Gunicorn to handle multiple concurrent requests. explain swaggers explain blueprint The pipfile defines the dependencies for this component.","title":"Design approach"},{"location":"infuse/simul-app/#testing","text":"","title":"Testing"},{"location":"infuse/simul-app/#unit-test-the-simulator","text":"The test coverage is not yet great. To run them cd simulator . / startPythonEnv root @1 de81b16f940 : / # export PYTHONPATH =/ home / simulator root @1 de81b16f940 : / # cd / home / simulator root @1 de81b16f940 : / # python tests / unit / TestSimulator . py","title":"Unit test the Simulator"},{"location":"infuse/simul-app/#build","text":"We have multiple build approaches depending of the environment: local, openshift source to image, github actions.","title":"Build"},{"location":"infuse/simul-app/#build-locally-with-docker","text":"The goal is to build the docker image, publish it to docker hub registry or a private registry, which then can be deployed to any kubernetes environment or run using docker compose: # In simulator folder docker build -t ibmcase/kcontainer-reefer-simulator . docker login docker push ibmcase/kcontainer-reefer-simulator > The push refers to repository [ docker.io/ibmcase/kcontainer-reefer-simulator ] The scripts: scripts/runReeferSimulatorApp.sh IBMCLOUD should run the simulator docker image with Event Streams on IBM Cloud.","title":"Build locally with docker"},{"location":"infuse/simul-app/#openshift-source-to-image","text":"To build and deploy the code to an OpenShift cluster, using the source 2 image approach, do the following steps: Login to the OpenShift cluster. oc login -u apikey -p <apikey> --server=https://... To find the API key and server URL go to the openshift console under your account, to access the Copy login command menu. Create a project if you did not create one already: oc new-project reefershipmentsolution --description=\"A Reefer container shipment solution\" Remember the project is mapped to a kubernetes namespace, but includes other components too The first deploy you need to create a new app from the source code, and use source to image build process to deploy the app. You can use a subdirectory of your source code repository by specifying a --context-dir flag. oc new-app python:latest~https://github.com/ibm-cloud-architecture/refarch-reefer-ml.git --context-dir=simulator --name reefersimulator Then to track the build progress, look at the logs of the build pod: oc logs -f bc/reefersimulator The dependencies are loaded, the build is scheduled and executed, the image is uploaded to the registry, and started. To display information about the build configuration for the application: oc describe bc/reefersimulator When you want to redeploy, trigger a remote build (run on Openshift) from local source code do the following command: oc start-build reefersimulator --from-file=. Set environment variables For Broker URLs oc set env dc/reefersimulator KAFKA_BROKERS=kafka03-prod02.messagehub.services.us-south.blu.... For apikey: oc set env dc/reefersimulator KAFKA_APIKEY=\"\" If you connect to event stream or kafka with SSL specify where to find the SSL certificate: oc set env dc/reefersimulator KAFKA_CERT=\"/opt/app-root/src/es-cert.pem\" Get all environment variables set for a given pod: (get the pod id with oc get pod ) oc exec reefersimulator-31-2kdv5 env Once the build is done you should see the container up and running oc get pod reefersimulator-3-build 0/1 Completed 0 15m reefersimulator-3-jdh2v 1/1 Running 0 1m Note The first time the container start, it may crash as the environment variables like KAFKA_APIKEY is not defined. You can use the ./scripts/defEnvVarInOpenShift.sh command to create the needed environment variables. To make the webapp visible externally to the cluster, you need to add a route for this deployment. Login to the admin console and use Create Route button on top right of the screen, Then enter a name and select the existing service Once created, the URL of the app is visible in the route list panel: Add the host name in your local /etc/hosts or be sure the hostname is defined in DNS server. Map to the IP address of the kubernetes proxy server end point.","title":"Openshift  source to image"},{"location":"organize/readme/","text":"Organize data Under construction","title":"Organize"},{"location":"organize/readme/#organize-data","text":"Under construction","title":"Organize data"}]}